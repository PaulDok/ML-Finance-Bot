{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Описание Домашнего Задания"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нейросети – ML - модель\n",
    "\n",
    "<b>Цель:</b>\n",
    "В данном домашнем задании вы потренируетесь в построении модели машинного обучения для формирования вашей торговой стратегии на основе нейросетей.\n",
    "\n",
    "\n",
    "<b>Описание/Пошаговая инструкция выполнения домашнего задания:</b>\n",
    "Уважаемый слушатель!\n",
    "\n",
    "\n",
    "Вы успешно создали полноценную торговую стратегию на основе модели машинного обучения и обеспечили фиксацию и сравнение метрик полученных моделей.\n",
    "\n",
    "\n",
    "Вы решаете построить более сложные модели машинного обучения и хотите использовать нейросетевые модели, в том числе для обработки временных рядов и глубокие нейронные сети. Здесь вам могут помочь методы рекуррентных сетей и современные трансформерные архитектуры.\n",
    "\n",
    "\n",
    "Поговорив с коллегами, вы понимаете, что самостоятельно построить и обучить действительно сложные архитектуры будет сложно и решаете воспользоваться предобученными свободно распространяемыми моделями.\n",
    "\n",
    "\n",
    "На основании вышесказанного вам необходимо построить несколько моделей на основе нейронных сетей, позволяющих прогнозировать оптимальное торговое действие.\n",
    "\n",
    "\n",
    "<b>На основе представленной информации, вам предлагается:</b>\n",
    "\n",
    "\n",
    "1) Создать модель (торговую стратегию) на основе нейронных сетей для прогнозирования оптимального торгового действия. Можно использовать, как самостоятельно обученные архитектуры, так и использовать предобученные сети или фреймворки.\n",
    "2) Провести тестирование разработанной стратегии на валидационном датасете.\n",
    "3) Зафиксировать метрики модели для дальнейшего сравнения экспериментов.\n",
    "4) Сформировать дашборд, показывающий эффективность различных торговых\n",
    "стратегий."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подход к реализации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В рамках ДЗ №4 был подготовлен фреймворк для проведения экспериментов, логгирования их результатов и отображения в UI\n",
    "\n",
    "Следовательно, задача на данном этапе сводится к:\n",
    "- созданию нескольких классов нейросетевых моделей\n",
    "- добавлению их в пул исследуемых (landing.py -> ml_model_strategy_training_loop_callback -> model_options)\n",
    "- проведению тестирования с использованием имеющегося функционала\n",
    "\n",
    "Реализацию проведу с использованием фреймворка pytorch (он мне ближе из альтернатив, а написание нейросети на чистом numpy не рассматриваю т.к. цель задания - не демонстрация понимания низкоуровневой логики)\n",
    "\n",
    "Из архитектур - реализую CNN, LSTM, GRU как наиболее подходящие к домену прогнозирования временных рядов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Создадим класс-обёртку для PyTorch моделей в нашем проекте"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Для облегчения разработки достанем данные в том виде, в котором они используются в training_loop.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\miniconda3\\envs\\hw_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\ProgramData\\miniconda3\\envs\\hw_env\\lib\\site-packages\\backtesting\\_plotting.py:55: UserWarning: Jupyter Notebook detected. Setting Bokeh output to notebook. This may not work in Jupyter clients without JavaScript support, such as old IDEs. Reset with `backtesting.set_bokeh_output(notebook=False)`.\n",
      "  warnings.warn('Jupyter Notebook detected. '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "    <style>\n",
       "        .bk-notebook-logo {\n",
       "            display: block;\n",
       "            width: 20px;\n",
       "            height: 20px;\n",
       "            background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAABx0RVh0U29mdHdhcmUAQWRvYmUgRmlyZXdvcmtzIENTNui8sowAAAOkSURBVDiNjZRtaJVlGMd/1/08zzln5zjP1LWcU9N0NkN8m2CYjpgQYQXqSs0I84OLIC0hkEKoPtiH3gmKoiJDU7QpLgoLjLIQCpEsNJ1vqUOdO7ppbuec5+V+rj4ctwzd8IIbbi6u+8f1539dt3A78eXC7QizUF7gyV1fD1Yqg4JWz84yffhm0qkFqBogB9rM8tZdtwVsPUhWhGcFJngGeWrPzHm5oaMmkfEg1usvLFyc8jLRqDOMru7AyC8saQr7GG7f5fvDeH7Ej8CM66nIF+8yngt6HWaKh7k49Soy9nXurCi1o3qUbS3zWfrYeQDTB/Qj6kX6Ybhw4B+bOYoLKCC9H3Nu/leUTZ1JdRWkkn2ldcCamzrcf47KKXdAJllSlxAOkRgyHsGC/zRday5Qld9DyoM4/q/rUoy/CXh3jzOu3bHUVZeU+DEn8FInkPBFlu3+nW3Nw0mk6vCDiWg8CeJaxEwuHS3+z5RgY+YBR6V1Z1nxSOfoaPa4LASWxxdNp+VWTk7+4vzaou8v8PN+xo+KY2xsw6une2frhw05CTYOmQvsEhjhWjn0bmXPjpE1+kplmmkP3suftwTubK9Vq22qKmrBhpY4jvd5afdRA3wGjFAgcnTK2s4hY0/GPNIb0nErGMCRxWOOX64Z8RAC4oCXdklmEvcL8o0BfkNK4lUg9HTl+oPlQxdNo3Mg4Nv175e/1LDGzZen30MEjRUtmXSfiTVu1kK8W4txyV6BMKlbgk3lMwYCiusNy9fVfvvwMxv8Ynl6vxoByANLTWplvuj/nF9m2+PDtt1eiHPBr1oIfhCChQMBw6Aw0UulqTKZdfVvfG7VcfIqLG9bcldL/+pdWTLxLUy8Qq38heUIjh4XlzZxzQm19lLFlr8vdQ97rjZVOLf8nclzckbcD4wxXMidpX30sFd37Fv/GtwwhzhxGVAprjbg0gCAEeIgwCZyTV2Z1REEW8O4py0wsjeloKoMr6iCY6dP92H6Vw/oTyICIthibxjm/DfN9lVz8IqtqKYLUXfoKVMVQVVJOElGjrnnUt9T9wbgp8AyYKaGlqingHZU/uG2NTZSVqwHQTWkx9hxjkpWDaCg6Ckj5qebgBVbT3V3NNXMSiWSDdGV3hrtzla7J+duwPOToIg42ChPQOQjspnSlp1V+Gjdged7+8UN5CRAV7a5EdFNwCjEaBR27b3W890TE7g24NAP/mMDXRWrGoFPQI9ls/MWO2dWFAar/xcOIImbbpA3zgAAAABJRU5ErkJggg==);\n",
       "        }\n",
       "    </style>\n",
       "    <div>\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-notebook-logo\"></a>\n",
       "        <span id=\"aa102799-d3d0-4033-91ab-327aa1f9f44d\">Loading BokehJS ...</span>\n",
       "    </div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "'use strict';\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\nconst JS_MIME_TYPE = 'application/javascript';\n  const HTML_MIME_TYPE = 'text/html';\n  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n  const CLASS_NAME = 'output_bokeh rendered_html';\n\n  /**\n   * Render data to the DOM node\n   */\n  function render(props, node) {\n    const script = document.createElement(\"script\");\n    node.appendChild(script);\n  }\n\n  /**\n   * Handle when an output is cleared or removed\n   */\n  function handleClearOutput(event, handle) {\n    function drop(id) {\n      const view = Bokeh.index.get_by_id(id)\n      if (view != null) {\n        view.model.document.clear()\n        Bokeh.index.delete(view)\n      }\n    }\n\n    const cell = handle.cell;\n\n    const id = cell.output_area._bokeh_element_id;\n    const server_id = cell.output_area._bokeh_server_id;\n\n    // Clean up Bokeh references\n    if (id != null) {\n      drop(id)\n    }\n\n    if (server_id !== undefined) {\n      // Clean up Bokeh references\n      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n      cell.notebook.kernel.execute(cmd_clean, {\n        iopub: {\n          output: function(msg) {\n            const id = msg.content.text.trim()\n            drop(id)\n          }\n        }\n      });\n      // Destroy server and session\n      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n      cell.notebook.kernel.execute(cmd_destroy);\n    }\n  }\n\n  /**\n   * Handle when a new output is added\n   */\n  function handleAddOutput(event, handle) {\n    const output_area = handle.output_area;\n    const output = handle.output;\n\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n      return\n    }\n\n    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n      // store reference to embed id on output_area\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n    }\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n      const bk_div = document.createElement(\"div\");\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n      const script_attrs = bk_div.children[0].attributes;\n      for (let i = 0; i < script_attrs.length; i++) {\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n      }\n      // store reference to server id on output_area\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n    }\n  }\n\n  function register_renderer(events, OutputArea) {\n\n    function append_mime(data, metadata, element) {\n      // create a DOM node to render to\n      const toinsert = this.create_output_subarea(\n        metadata,\n        CLASS_NAME,\n        EXEC_MIME_TYPE\n      );\n      this.keyboard_manager.register_events(toinsert);\n      // Render to node\n      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n      render(props, toinsert[toinsert.length - 1]);\n      element.append(toinsert);\n      return toinsert\n    }\n\n    /* Handle when an output is cleared or removed */\n    events.on('clear_output.CodeCell', handleClearOutput);\n    events.on('delete.Cell', handleClearOutput);\n\n    /* Handle when a new output is added */\n    events.on('output_added.OutputArea', handleAddOutput);\n\n    /**\n     * Register the mime type and append_mime function with output_area\n     */\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n      /* Is output safe? */\n      safe: true,\n      /* Index of renderer in `output_area.display_order` */\n      index: 0\n    });\n  }\n\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\n  if (root.Jupyter !== undefined) {\n    const events = require('base/js/events');\n    const OutputArea = require('notebook/js/outputarea').OutputArea;\n\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  }\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded(error = null) {\n    const el = document.getElementById(\"aa102799-d3d0-4033-91ab-327aa1f9f44d\");\n    if (el != null) {\n      const html = (() => {\n        if (typeof root.Bokeh === \"undefined\") {\n          if (error == null) {\n            return \"BokehJS is loading ...\";\n          } else {\n            return \"BokehJS failed to load.\";\n          }\n        } else {\n          const prefix = `BokehJS ${root.Bokeh.version}`;\n          if (error == null) {\n            return `${prefix} successfully loaded.`;\n          } else {\n            return `${prefix} <b>encountered errors</b> while loading and may not function as expected.`;\n          }\n        }\n      })();\n      el.innerHTML = html;\n\n      if (error != null) {\n        const wrapper = document.createElement(\"div\");\n        wrapper.style.overflow = \"auto\";\n        wrapper.style.height = \"5em\";\n        wrapper.style.resize = \"vertical\";\n        const content = document.createElement(\"div\");\n        content.style.fontFamily = \"monospace\";\n        content.style.whiteSpace = \"pre-wrap\";\n        content.style.backgroundColor = \"rgb(255, 221, 221)\";\n        content.textContent = error.stack ?? error.toString();\n        wrapper.append(content);\n        el.append(wrapper);\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(() => display_loaded(error), 100);\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.7.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.7.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.7.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.7.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.7.2.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n      try {\n            for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n\n      } catch (error) {display_loaded(error);throw error;\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"aa102799-d3d0-4033-91ab-327aa1f9f44d\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));",
      "application/vnd.bokehjs_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Относительные ссылки, включая импорты, относительно корневой папки проекта\n",
    "import os\n",
    "\n",
    "os.chdir(os.path.dirname(os.getcwd()))\n",
    "\n",
    "import main\n",
    "import logging\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from src.core import utils\n",
    "\n",
    "# initialize\n",
    "logger = logging.getLogger()\n",
    "# initialize config dict\n",
    "config = main.main_launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params for testing\n",
    "tickers = [\"^GSPC\"]\n",
    "interval = config.INTERVAL\n",
    "\n",
    "# Params for train-test-valid split\n",
    "# TEST - Q4'24 | VAL - Q1'25\n",
    "train_start = \"2020-01-01\"\n",
    "train_end = \"2024-10-01\"\n",
    "test_end = \"2025-01-01\"\n",
    "valid_end = \"2025-04-01\"  # захватим первый квартал 2025, тестовая выборка по длине такая же как валидационная - 3мес\n",
    "\n",
    "# Params inside Optimizer\n",
    "early_stopping_rounds = 50\n",
    "n_trials = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Список моделей и вариантов их гиперпараметров для тестирования\n",
    "# TODO: ML модели появятся здесь\n",
    "model_options = {\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO   ] 2025-05-13@19:38:55: Getting preprocessed history from local cache DB...\n",
      "[INFO   ] 2025-05-13@19:38:57: Got history of shape (1854, 8), 0 NaNs\n",
      "[INFO   ] 2025-05-13@19:38:58: Parsed features from JSON to separate columns: (1854, 302), 0 NaNs\n",
      "[INFO   ] 2025-05-13@19:38:58: Adding binary target...\n",
      "[INFO   ] 2025-05-13@19:38:58: Target added: (1854, 303), 0 NaNs\n",
      "[INFO   ] 2025-05-13@19:38:58: ~ ~ ~ Modelling for ^GSPC ~ ~ ~\n",
      "[INFO   ] 2025-05-13@19:38:58: Splitting ticker data to train/test/validation parts\n",
      "[INFO   ] 2025-05-13@19:38:58: X_train.shape=(1672, 296) | y_train.shape=(1672,) || X_test.shape=(92, 296) | y_test.shape=(92,) || X_val.shape=(90, 296) | y_val.shape=(90,)\n",
      "[INFO   ] 2025-05-13@19:38:58: Scaling features...\n"
     ]
    }
   ],
   "source": [
    "# Часть логики training_loop - получим в блокноте те же переменные что и при инициализации optimizer.ModelOptimizer\n",
    "# Get preprocessed data\n",
    "data, features = utils.get_preprocessed_history(\n",
    "    tickers=tickers, start=train_start, end=valid_end, interval=interval\n",
    ")\n",
    "\n",
    "# Add feature column\n",
    "data = utils.add_target(data)\n",
    "\n",
    "# Loop over tickers in dataset\n",
    "grand_result = []\n",
    "for ticker in data[\"Ticker\"].unique():\n",
    "    logger.info(f\"~ ~ ~ Modelling for {ticker} ~ ~ ~\")\n",
    "    ticker_data = data[data[\"Ticker\"] == ticker].reset_index(drop=True)\n",
    "\n",
    "    # Split dataset into parts\n",
    "    X_train, y_train, X_test, y_test, X_val, y_val = utils.train_test_valid_split(\n",
    "        ticker_data,\n",
    "        train_start=train_start,\n",
    "        train_end=train_end,\n",
    "        test_end=test_end,\n",
    "        valid_end=valid_end,\n",
    "        drop_leaky=True,\n",
    "        target_col=\"target\",\n",
    "    )\n",
    "    logger.info(\n",
    "        f\"{X_train.shape=} | {y_train.shape=} || {X_test.shape=} | {y_test.shape=} || {X_val.shape=} | {y_val.shape=}\"\n",
    "    )\n",
    "\n",
    "    # Scale train / test / validation datasets - fit on train\n",
    "    logger.info(\"Scaling features...\")\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train[features])\n",
    "    X_test_scaled = scaler.transform(X_test[features])\n",
    "    X_val_scaled = scaler.transform(X_val[features])\n",
    "\n",
    "    break\n",
    "\n",
    "    # # Для каждого потенциального типа модели:\n",
    "    # for model_type, param_dict in model_options.items():\n",
    "    #     logger.info(f\"~ ~ Iteration for {model_type.__name__} ~ ~\")\n",
    "    #     # Тюним гиперпараметры на train / test датасетах, выбираем лучшее\n",
    "    #     model_optimizer = optimizer.ModelOptimizer(\n",
    "    #         model_type,\n",
    "    #         param_dict,\n",
    "    #         X_train_scaled,\n",
    "    #         y_train,\n",
    "    #         X_test_scaled,\n",
    "    #         y_test,\n",
    "    #         X_val_scaled,\n",
    "    #         y_val,\n",
    "    #     )\n",
    "    #     (\n",
    "    #         model,\n",
    "    #         best_params,\n",
    "    #         (train_roc_auc, test_roc_auc, val_roc_auc),\n",
    "    #         (train_metrics_table, test_metrics_table, val_metrics_table),\n",
    "    #     ) = model_optimizer.optimize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Заготовки внутренних функций модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Потенциальные параметры для тюнинга\n",
    "window_size = 30\n",
    "batch_size = 32\n",
    "lr = 0.001\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_dataloader(X_raw, y_raw=None):\n",
    "    \"\"\"\n",
    "    Adapter to be used inside NN models\n",
    "    \"\"\"\n",
    "    logger.info(\"Converting numpy arrays to TensorDataset and DataLoader...\")\n",
    "    # Convert to PyTorch Tensors\n",
    "    X, y = [], []\n",
    "    for i in range(len(X_raw) - window_size - 1):\n",
    "        X.append(X_raw[i : i + window_size])\n",
    "        if y_raw is not None:\n",
    "            y.append(y_raw[i + window_size])\n",
    "        else:\n",
    "            y.append(0)\n",
    "\n",
    "    # Now convert lists to tensors\n",
    "    X = torch.tensor(X, dtype=torch.float32)\n",
    "    y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    # DataLoader\n",
    "    dataset = TensorDataset(X, y)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader):\n",
    "    \"\"\"\n",
    "    NN model training loop\n",
    "    \"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Switch model to train mode\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for inputs, labels in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolution Neural Network\n",
    "    \"\"\"\n",
    "    def __init__(self, n_features, kernel_size_conv1=3, kernel_size_conv2=3):\n",
    "        super(StockCNN, self).__init__()\n",
    "        self.cnn_stack = nn.Sequential(\n",
    "            nn.Conv1d(n_features, 16, kernel_size=kernel_size_conv1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(16, 32, kernel_size=kernel_size_conv2),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(), # flatten after convolutions\n",
    "            nn.Linear(\n",
    "                32 * (window_size - (kernel_size_conv1 - 1) - (kernel_size_conv2 - 1)), 64\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 2),\n",
    "            nn.Softmax(1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)  # Permute to (batch_size, channels, sequence_length)\n",
    "        probabilities = self.cnn_stack(x)\n",
    "        return probabilities\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализуем логику функций fit() и predict_proba() данных моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) - to be in model's .fit() method\n",
    "def fit(X_train, y_train):\n",
    "    # Convert data\n",
    "    dataloader = convert_to_dataloader(X_raw=X_train, y_raw=y_train)\n",
    "    # Initialize model\n",
    "    logger.info(\"Initializing model...\")\n",
    "    model = StockCNN(n_features=X_train.shape[1])\n",
    "    # Train it\n",
    "    logger.info(\"Training model...\")\n",
    "    model = train_model(model, dataloader)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2) - to be in predict_proba() method\n",
    "def predict_proba(model, X_raw):\n",
    "    dataloader = convert_to_dataloader(X_raw)\n",
    "\n",
    "    _ = model.eval()\n",
    "    predictions = [np.array([0.5, 0.5], dtype=\"float32\")] * (window_size + 1) # first ticks have no real prediction\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in dataloader:\n",
    "            logits = model(inputs)\n",
    "            predictions.extend(logits.numpy())\n",
    "\n",
    "    # Convert to a single np.array\n",
    "    predictions = np.stack(predictions, axis=0)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для референса - в model_optimizer происходит следующее:\n",
    "# = = = = = =\n",
    "# # Инициализация модели с заданными параметрами\n",
    "# model = self.model_type(**suggested_param)\n",
    "\n",
    "# # Обучение модели с валидационной выборкой\n",
    "# if self.param_dict.get(\"use_eval_set\", None):\n",
    "#     _ = model.fit(\n",
    "#         self.X_train_scaled,\n",
    "#         self.y_train,\n",
    "#         eval_set=(self.X_test_scaled, self.y_test),\n",
    "#     )\n",
    "# else:\n",
    "#     _ = model.fit(\n",
    "#         self.X_train_scaled,\n",
    "#         self.y_train,\n",
    "#     )\n",
    "\n",
    "# # Предсказания на тренировочной и тестовой выборках\n",
    "# y_train_pred_prob = model.predict_proba(self.X_train_scaled)[:, 1]\n",
    "# y_test_pred_prob = model.predict_proba(self.X_test_scaled)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO   ] 2025-05-13@19:39:21: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "C:\\Users\\Pavel\\AppData\\Local\\Temp\\ipykernel_16108\\3998794152.py:16: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:257.)\n",
      "  X = torch.tensor(X, dtype=torch.float32)\n",
      "[INFO   ] 2025-05-13@19:39:23: Initializing model...\n",
      "[INFO   ] 2025-05-13@19:39:23: Training model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.6659\n",
      "Epoch [2/10], Loss: 0.6443\n",
      "Epoch [3/10], Loss: 0.6821\n",
      "Epoch [4/10], Loss: 0.5328\n",
      "Epoch [5/10], Loss: 0.6582\n",
      "Epoch [6/10], Loss: 0.4607\n",
      "Epoch [7/10], Loss: 0.7626\n",
      "Epoch [8/10], Loss: 0.5211\n",
      "Epoch [9/10], Loss: 0.6265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO   ] 2025-05-13@19:39:29: Converting numpy arrays to TensorDataset and DataLoader...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Loss: 0.4940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO   ] 2025-05-13@19:39:31: Converting numpy arrays to TensorDataset and DataLoader...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_roc_auc=np.float64(0.5238771093519696) | test_roc_auc=np.float64(0.5639204545454546)\n"
     ]
    }
   ],
   "source": [
    "# То что получается у нас\n",
    "# fit\n",
    "model = fit(X_train=X_train_scaled, y_train=y_train)\n",
    "# predict\n",
    "y_train_pred_prob = predict_proba(model, X_train_scaled)[:, 1]\n",
    "y_test_pred_prob = predict_proba(model, X_test_scaled)[:, 1]\n",
    "# log metric\n",
    "train_roc_auc = roc_auc_score(y_train, y_train_pred_prob)\n",
    "test_roc_auc = roc_auc_score(y_test, y_test_pred_prob)\n",
    "\n",
    "print(f\"{train_roc_auc=} | {test_roc_auc=}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Получив работоспособный прототип, объединяем всё в ./app/src/models/cnn_model.py и протестируем цикл подбора гиперпараметров\n",
    "\n",
    "(запускать ячейки ниже после перезапуска ядра)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\miniconda3\\envs\\hw_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\ProgramData\\miniconda3\\envs\\hw_env\\lib\\site-packages\\backtesting\\_plotting.py:55: UserWarning: Jupyter Notebook detected. Setting Bokeh output to notebook. This may not work in Jupyter clients without JavaScript support, such as old IDEs. Reset with `backtesting.set_bokeh_output(notebook=False)`.\n",
      "  warnings.warn('Jupyter Notebook detected. '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "    <style>\n",
       "        .bk-notebook-logo {\n",
       "            display: block;\n",
       "            width: 20px;\n",
       "            height: 20px;\n",
       "            background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAABx0RVh0U29mdHdhcmUAQWRvYmUgRmlyZXdvcmtzIENTNui8sowAAAOkSURBVDiNjZRtaJVlGMd/1/08zzln5zjP1LWcU9N0NkN8m2CYjpgQYQXqSs0I84OLIC0hkEKoPtiH3gmKoiJDU7QpLgoLjLIQCpEsNJ1vqUOdO7ppbuec5+V+rj4ctwzd8IIbbi6u+8f1539dt3A78eXC7QizUF7gyV1fD1Yqg4JWz84yffhm0qkFqBogB9rM8tZdtwVsPUhWhGcFJngGeWrPzHm5oaMmkfEg1usvLFyc8jLRqDOMru7AyC8saQr7GG7f5fvDeH7Ej8CM66nIF+8yngt6HWaKh7k49Soy9nXurCi1o3qUbS3zWfrYeQDTB/Qj6kX6Ybhw4B+bOYoLKCC9H3Nu/leUTZ1JdRWkkn2ldcCamzrcf47KKXdAJllSlxAOkRgyHsGC/zRday5Qld9DyoM4/q/rUoy/CXh3jzOu3bHUVZeU+DEn8FInkPBFlu3+nW3Nw0mk6vCDiWg8CeJaxEwuHS3+z5RgY+YBR6V1Z1nxSOfoaPa4LASWxxdNp+VWTk7+4vzaou8v8PN+xo+KY2xsw6une2frhw05CTYOmQvsEhjhWjn0bmXPjpE1+kplmmkP3suftwTubK9Vq22qKmrBhpY4jvd5afdRA3wGjFAgcnTK2s4hY0/GPNIb0nErGMCRxWOOX64Z8RAC4oCXdklmEvcL8o0BfkNK4lUg9HTl+oPlQxdNo3Mg4Nv175e/1LDGzZen30MEjRUtmXSfiTVu1kK8W4txyV6BMKlbgk3lMwYCiusNy9fVfvvwMxv8Ynl6vxoByANLTWplvuj/nF9m2+PDtt1eiHPBr1oIfhCChQMBw6Aw0UulqTKZdfVvfG7VcfIqLG9bcldL/+pdWTLxLUy8Qq38heUIjh4XlzZxzQm19lLFlr8vdQ97rjZVOLf8nclzckbcD4wxXMidpX30sFd37Fv/GtwwhzhxGVAprjbg0gCAEeIgwCZyTV2Z1REEW8O4py0wsjeloKoMr6iCY6dP92H6Vw/oTyICIthibxjm/DfN9lVz8IqtqKYLUXfoKVMVQVVJOElGjrnnUt9T9wbgp8AyYKaGlqingHZU/uG2NTZSVqwHQTWkx9hxjkpWDaCg6Ckj5qebgBVbT3V3NNXMSiWSDdGV3hrtzla7J+duwPOToIg42ChPQOQjspnSlp1V+Gjdged7+8UN5CRAV7a5EdFNwCjEaBR27b3W890TE7g24NAP/mMDXRWrGoFPQI9ls/MWO2dWFAar/xcOIImbbpA3zgAAAABJRU5ErkJggg==);\n",
       "        }\n",
       "    </style>\n",
       "    <div>\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-notebook-logo\"></a>\n",
       "        <span id=\"fff98b1c-ffdd-4e10-a42c-e7be324b3776\">Loading BokehJS ...</span>\n",
       "    </div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "'use strict';\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\nconst JS_MIME_TYPE = 'application/javascript';\n  const HTML_MIME_TYPE = 'text/html';\n  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n  const CLASS_NAME = 'output_bokeh rendered_html';\n\n  /**\n   * Render data to the DOM node\n   */\n  function render(props, node) {\n    const script = document.createElement(\"script\");\n    node.appendChild(script);\n  }\n\n  /**\n   * Handle when an output is cleared or removed\n   */\n  function handleClearOutput(event, handle) {\n    function drop(id) {\n      const view = Bokeh.index.get_by_id(id)\n      if (view != null) {\n        view.model.document.clear()\n        Bokeh.index.delete(view)\n      }\n    }\n\n    const cell = handle.cell;\n\n    const id = cell.output_area._bokeh_element_id;\n    const server_id = cell.output_area._bokeh_server_id;\n\n    // Clean up Bokeh references\n    if (id != null) {\n      drop(id)\n    }\n\n    if (server_id !== undefined) {\n      // Clean up Bokeh references\n      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n      cell.notebook.kernel.execute(cmd_clean, {\n        iopub: {\n          output: function(msg) {\n            const id = msg.content.text.trim()\n            drop(id)\n          }\n        }\n      });\n      // Destroy server and session\n      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n      cell.notebook.kernel.execute(cmd_destroy);\n    }\n  }\n\n  /**\n   * Handle when a new output is added\n   */\n  function handleAddOutput(event, handle) {\n    const output_area = handle.output_area;\n    const output = handle.output;\n\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n      return\n    }\n\n    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n      // store reference to embed id on output_area\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n    }\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n      const bk_div = document.createElement(\"div\");\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n      const script_attrs = bk_div.children[0].attributes;\n      for (let i = 0; i < script_attrs.length; i++) {\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n      }\n      // store reference to server id on output_area\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n    }\n  }\n\n  function register_renderer(events, OutputArea) {\n\n    function append_mime(data, metadata, element) {\n      // create a DOM node to render to\n      const toinsert = this.create_output_subarea(\n        metadata,\n        CLASS_NAME,\n        EXEC_MIME_TYPE\n      );\n      this.keyboard_manager.register_events(toinsert);\n      // Render to node\n      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n      render(props, toinsert[toinsert.length - 1]);\n      element.append(toinsert);\n      return toinsert\n    }\n\n    /* Handle when an output is cleared or removed */\n    events.on('clear_output.CodeCell', handleClearOutput);\n    events.on('delete.Cell', handleClearOutput);\n\n    /* Handle when a new output is added */\n    events.on('output_added.OutputArea', handleAddOutput);\n\n    /**\n     * Register the mime type and append_mime function with output_area\n     */\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n      /* Is output safe? */\n      safe: true,\n      /* Index of renderer in `output_area.display_order` */\n      index: 0\n    });\n  }\n\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\n  if (root.Jupyter !== undefined) {\n    const events = require('base/js/events');\n    const OutputArea = require('notebook/js/outputarea').OutputArea;\n\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  }\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded(error = null) {\n    const el = document.getElementById(\"fff98b1c-ffdd-4e10-a42c-e7be324b3776\");\n    if (el != null) {\n      const html = (() => {\n        if (typeof root.Bokeh === \"undefined\") {\n          if (error == null) {\n            return \"BokehJS is loading ...\";\n          } else {\n            return \"BokehJS failed to load.\";\n          }\n        } else {\n          const prefix = `BokehJS ${root.Bokeh.version}`;\n          if (error == null) {\n            return `${prefix} successfully loaded.`;\n          } else {\n            return `${prefix} <b>encountered errors</b> while loading and may not function as expected.`;\n          }\n        }\n      })();\n      el.innerHTML = html;\n\n      if (error != null) {\n        const wrapper = document.createElement(\"div\");\n        wrapper.style.overflow = \"auto\";\n        wrapper.style.height = \"5em\";\n        wrapper.style.resize = \"vertical\";\n        const content = document.createElement(\"div\");\n        content.style.fontFamily = \"monospace\";\n        content.style.whiteSpace = \"pre-wrap\";\n        content.style.backgroundColor = \"rgb(255, 221, 221)\";\n        content.textContent = error.stack ?? error.toString();\n        wrapper.append(content);\n        el.append(wrapper);\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(() => display_loaded(error), 100);\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.7.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.7.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.7.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.7.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.7.2.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n      try {\n            for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n\n      } catch (error) {display_loaded(error);throw error;\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"fff98b1c-ffdd-4e10-a42c-e7be324b3776\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));",
      "application/vnd.bokehjs_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Относительные ссылки, включая импорты, относительно корневой папки проекта\n",
    "import os\n",
    "\n",
    "os.chdir(os.path.dirname(os.getcwd()))\n",
    "\n",
    "import main\n",
    "import logging\n",
    "\n",
    "from src.models.training_loop import ml_model_strategy_training_loop\n",
    "from src.models.cnn_model import CNNModel\n",
    "\n",
    "# initialize\n",
    "logger = logging.getLogger()\n",
    "# initialize config dict\n",
    "config = main.main_launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params for testing\n",
    "tickers = [\"^GSPC\"]\n",
    "interval = config.INTERVAL\n",
    "\n",
    "# Params for train-test-valid split\n",
    "# TEST - Q4'24 | VAL - Q1'25\n",
    "train_start = \"2020-01-01\"\n",
    "train_end = \"2024-10-01\"\n",
    "test_end = \"2025-01-01\"\n",
    "valid_end = \"2025-04-01\"  # захватим первый квартал 2025, тестовая выборка по длине такая же как валидационная - 3мес"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Потенциальные параметры для тюнинга\n",
    "# window_size = 30\n",
    "# batch_size = 32\n",
    "# lr = 0.001\n",
    "# num_epochs = 10\n",
    "model_options = {\n",
    "    CNNModel: {\n",
    "        \"int\": {\n",
    "            \"window_size\": {\"low\": 5, \"high\": 60},\n",
    "            \"batch_size\": {\"low\": 4, \"high\": 128},\n",
    "            \"num_epochs\": {\"low\": 5, \"high\": 50}\n",
    "        },\n",
    "        \"float\": {\n",
    "            \"lr\": {\"low\": 0.0005, \"high\": 0.01}\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO   ] 2025-05-13@20:20:59: Getting preprocessed history from local cache DB...\n",
      "[INFO   ] 2025-05-13@20:21:01: Got history of shape (1854, 8), 0 NaNs\n",
      "[INFO   ] 2025-05-13@20:21:02: Parsed features from JSON to separate columns: (1854, 302), 0 NaNs\n",
      "[INFO   ] 2025-05-13@20:21:02: Adding binary target...\n",
      "[INFO   ] 2025-05-13@20:21:02: Target added: (1854, 303), 0 NaNs\n",
      "[INFO   ] 2025-05-13@20:21:02: ~ ~ ~ Modelling for ^GSPC ~ ~ ~\n",
      "[INFO   ] 2025-05-13@20:21:02: Splitting ticker data to train/test/validation parts\n",
      "[INFO   ] 2025-05-13@20:21:02: X_train.shape=(1672, 296) | y_train.shape=(1672,) || X_test.shape=(92, 296) | y_test.shape=(92,) || X_val.shape=(90, 296) | y_val.shape=(90,)\n",
      "[INFO   ] 2025-05-13@20:21:02: Scaling features...\n",
      "[INFO   ] 2025-05-13@20:21:02: ~ ~ Iteration for CNNModel ~ ~\n",
      "[INFO   ] 2025-05-13@20:21:02: Searching for best hyperparameters using Optuna...\n",
      "[I 2025-05-13 17:21:02,733] A new study created in memory with name: no-name-238bfaa2-4e26-497c-a786-5fdc7bfdfb0f\n",
      "[INFO   ] 2025-05-13@20:21:02: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "d:\\Otus\\ML-Finance-Bot\\app\\src\\models\\cnn_model.py:42: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:257.)\n",
      "  X = torch.tensor(X, dtype=torch.float32)\n",
      "[INFO   ] 2025-05-13@20:21:04: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:21:04: Initializing model...\n",
      "[INFO   ] 2025-05-13@20:21:04: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:21:08: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:21:08: Initializing model...\n",
      "[INFO   ] 2025-05-13@20:21:08: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:21:08: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:21:08: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:21:08: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:21:11: Initializing model...\n",
      "[INFO   ] 2025-05-13@20:21:13: Training model...\n",
      "[INFO   ] 2025-05-13@20:21:13: Initializing model...\n",
      "[INFO   ] 2025-05-13@20:21:14: Initializing model...\n",
      "[INFO   ] 2025-05-13@20:21:17: Initializing model...\n",
      "[INFO   ] 2025-05-13@20:21:17: Training model...\n",
      "[INFO   ] 2025-05-13@20:21:20: Initializing model...\n",
      "[INFO   ] 2025-05-13@20:21:22: Initializing model...\n",
      "[INFO   ] 2025-05-13@20:21:22: Training model...\n",
      "[INFO   ] 2025-05-13@20:21:22: Training model...\n",
      "[INFO   ] 2025-05-13@20:21:22: Training model...\n",
      "[INFO   ] 2025-05-13@20:21:22: Training model...\n",
      "[INFO   ] 2025-05-13@20:21:22: Training model...\n",
      "[INFO   ] 2025-05-13@20:21:22: Training model...\n",
      "[INFO   ] 2025-05-13@20:21:25: Epoch [1/31], Loss: 0.6621\n",
      "[INFO   ] 2025-05-13@20:21:25: Epoch [1/42], Loss: 0.7007\n",
      "[INFO   ] 2025-05-13@20:21:26: Epoch [1/12], Loss: 0.9100\n",
      "[INFO   ] 2025-05-13@20:21:26: Epoch [1/13], Loss: 0.6815\n",
      "[INFO   ] 2025-05-13@20:21:26: Epoch [1/41], Loss: 0.6740\n",
      "[INFO   ] 2025-05-13@20:21:26: Epoch [2/31], Loss: 0.6673\n",
      "[INFO   ] 2025-05-13@20:21:26: Epoch [1/38], Loss: 0.6991\n",
      "[INFO   ] 2025-05-13@20:21:26: Epoch [2/42], Loss: 0.6803\n",
      "[INFO   ] 2025-05-13@20:21:26: Epoch [1/36], Loss: 0.6855\n",
      "[INFO   ] 2025-05-13@20:21:26: Epoch [2/12], Loss: 0.8294\n",
      "[INFO   ] 2025-05-13@20:21:27: Epoch [2/13], Loss: 0.6813\n",
      "[INFO   ] 2025-05-13@20:21:27: Epoch [3/31], Loss: 0.6377\n",
      "[INFO   ] 2025-05-13@20:21:27: Epoch [2/41], Loss: 0.6773\n",
      "[INFO   ] 2025-05-13@20:21:27: Epoch [3/42], Loss: 0.6643\n",
      "[INFO   ] 2025-05-13@20:21:27: Epoch [3/12], Loss: 0.7649\n",
      "[INFO   ] 2025-05-13@20:21:28: Epoch [4/31], Loss: 0.6769\n",
      "[INFO   ] 2025-05-13@20:21:28: Epoch [3/13], Loss: 0.7144\n",
      "[INFO   ] 2025-05-13@20:21:28: Epoch [3/41], Loss: 0.6703\n",
      "[INFO   ] 2025-05-13@20:21:28: Epoch [4/42], Loss: 0.6767\n",
      "[INFO   ] 2025-05-13@20:21:28: Epoch [2/38], Loss: 0.6309\n",
      "[INFO   ] 2025-05-13@20:21:28: Epoch [2/36], Loss: 0.7012\n",
      "[INFO   ] 2025-05-13@20:21:28: Epoch [4/12], Loss: 0.8455\n",
      "[INFO   ] 2025-05-13@20:21:28: Epoch [5/31], Loss: 0.6697\n",
      "[INFO   ] 2025-05-13@20:21:29: Epoch [4/13], Loss: 0.6596\n",
      "[INFO   ] 2025-05-13@20:21:29: Epoch [5/42], Loss: 0.6340\n",
      "[INFO   ] 2025-05-13@20:21:29: Epoch [6/31], Loss: 0.6550\n",
      "[INFO   ] 2025-05-13@20:21:29: Epoch [4/41], Loss: 0.6138\n",
      "[INFO   ] 2025-05-13@20:21:29: Epoch [5/12], Loss: 0.6842\n",
      "[INFO   ] 2025-05-13@20:21:30: Epoch [7/31], Loss: 0.7178\n",
      "[INFO   ] 2025-05-13@20:21:30: Epoch [3/38], Loss: 0.6675\n",
      "[INFO   ] 2025-05-13@20:21:30: Epoch [6/42], Loss: 0.6679\n",
      "[INFO   ] 2025-05-13@20:21:30: Epoch [5/13], Loss: 0.6649\n",
      "[INFO   ] 2025-05-13@20:21:30: Epoch [3/36], Loss: 0.6656\n",
      "[INFO   ] 2025-05-13@20:21:30: Epoch [6/12], Loss: 0.8294\n",
      "[INFO   ] 2025-05-13@20:21:30: Epoch [1/48], Loss: 0.6741\n",
      "[INFO   ] 2025-05-13@20:21:30: Epoch [5/41], Loss: 0.6530\n",
      "[INFO   ] 2025-05-13@20:21:31: Epoch [8/31], Loss: 0.6904\n",
      "[INFO   ] 2025-05-13@20:21:31: Epoch [7/42], Loss: 0.6221\n",
      "[INFO   ] 2025-05-13@20:21:31: Epoch [7/12], Loss: 0.7649\n",
      "[INFO   ] 2025-05-13@20:21:31: Epoch [6/13], Loss: 0.5808\n",
      "[INFO   ] 2025-05-13@20:21:31: Epoch [6/41], Loss: 0.7160\n",
      "[INFO   ] 2025-05-13@20:21:31: Epoch [9/31], Loss: 0.5042\n",
      "[INFO   ] 2025-05-13@20:21:32: Epoch [4/38], Loss: 0.5623\n",
      "[INFO   ] 2025-05-13@20:21:32: Epoch [8/42], Loss: 0.6185\n",
      "[INFO   ] 2025-05-13@20:21:32: Epoch [8/12], Loss: 0.7326\n",
      "[INFO   ] 2025-05-13@20:21:32: Epoch [4/36], Loss: 0.7378\n",
      "[INFO   ] 2025-05-13@20:21:32: Epoch [7/13], Loss: 0.6017\n",
      "[INFO   ] 2025-05-13@20:21:32: Epoch [10/31], Loss: 0.7853\n",
      "[INFO   ] 2025-05-13@20:21:33: Epoch [9/42], Loss: 0.5189\n",
      "[INFO   ] 2025-05-13@20:21:33: Epoch [7/41], Loss: 0.5610\n",
      "[INFO   ] 2025-05-13@20:21:33: Epoch [9/12], Loss: 0.6842\n",
      "[INFO   ] 2025-05-13@20:21:33: Epoch [11/31], Loss: 0.5767\n",
      "[INFO   ] 2025-05-13@20:21:33: Epoch [8/13], Loss: 0.5583\n",
      "[INFO   ] 2025-05-13@20:21:33: Epoch [5/38], Loss: 0.6612\n",
      "[INFO   ] 2025-05-13@20:21:33: Epoch [10/42], Loss: 0.5897\n",
      "[INFO   ] 2025-05-13@20:21:34: Epoch [12/31], Loss: 0.5915\n",
      "[INFO   ] 2025-05-13@20:21:34: Epoch [8/41], Loss: 0.7451\n",
      "[INFO   ] 2025-05-13@20:21:34: Epoch [10/12], Loss: 0.7326\n",
      "[INFO   ] 2025-05-13@20:21:34: Epoch [5/36], Loss: 0.8223\n",
      "[INFO   ] 2025-05-13@20:21:34: Epoch [11/42], Loss: 0.5343\n",
      "[INFO   ] 2025-05-13@20:21:34: Epoch [9/13], Loss: 0.6089\n",
      "[INFO   ] 2025-05-13@20:21:35: Epoch [13/31], Loss: 0.5746\n",
      "[INFO   ] 2025-05-13@20:21:35: Epoch [9/41], Loss: 0.6300\n",
      "[INFO   ] 2025-05-13@20:21:35: Epoch [11/12], Loss: 0.7487\n",
      "[INFO   ] 2025-05-13@20:21:35: Epoch [6/38], Loss: 0.8337\n",
      "[INFO   ] 2025-05-13@20:21:35: Epoch [12/42], Loss: 0.5461\n",
      "[INFO   ] 2025-05-13@20:21:35: Epoch [14/31], Loss: 0.5708\n",
      "[INFO   ] 2025-05-13@20:21:36: Epoch [10/13], Loss: 0.6603\n",
      "[INFO   ] 2025-05-13@20:21:36: Epoch [12/12], Loss: 0.8455\n",
      "[INFO   ] 2025-05-13@20:21:36: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:21:39: Epoch [6/36], Loss: 0.9246\n",
      "[INFO   ] 2025-05-13@20:21:39: Epoch [2/48], Loss: 0.6611\n",
      "[INFO   ] 2025-05-13@20:21:39: Epoch [13/42], Loss: 0.5800\n",
      "[INFO   ] 2025-05-13@20:21:39: Epoch [15/31], Loss: 0.5822\n",
      "[INFO   ] 2025-05-13@20:21:39: Epoch [10/41], Loss: 0.5683\n",
      "[INFO   ] 2025-05-13@20:21:39: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[I 2025-05-13 17:21:40,165] Trial 6 finished with value: 0.5081318349474775 and parameters: {'window_size': 49, 'batch_size': 120, 'num_epochs': 12, 'lr': 0.009388862570311405}. Best is trial 6 with value: 0.5081318349474775.\n",
      "[INFO   ] 2025-05-13@20:21:40: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:21:40: Initializing model...\n",
      "[INFO   ] 2025-05-13@20:21:40: Training model...\n",
      "[INFO   ] 2025-05-13@20:21:41: Epoch [11/13], Loss: 0.6150\n",
      "[INFO   ] 2025-05-13@20:21:41: Epoch [7/38], Loss: 0.7244\n",
      "[INFO   ] 2025-05-13@20:21:41: Epoch [14/42], Loss: 0.4792\n",
      "[INFO   ] 2025-05-13@20:21:41: Epoch [16/31], Loss: 0.4639\n",
      "[INFO   ] 2025-05-13@20:21:41: Epoch [1/5], Loss: 0.6814\n",
      "[INFO   ] 2025-05-13@20:21:41: Epoch [11/41], Loss: 0.4954\n",
      "[INFO   ] 2025-05-13@20:21:42: Epoch [12/13], Loss: 0.5605\n",
      "[INFO   ] 2025-05-13@20:21:42: Epoch [17/31], Loss: 0.3982\n",
      "[INFO   ] 2025-05-13@20:21:42: Epoch [15/42], Loss: 0.4891\n",
      "[INFO   ] 2025-05-13@20:21:42: Epoch [7/36], Loss: 0.7880\n",
      "[INFO   ] 2025-05-13@20:21:42: Epoch [2/5], Loss: 0.6842\n",
      "[INFO   ] 2025-05-13@20:21:42: Epoch [8/38], Loss: 0.6688\n",
      "[INFO   ] 2025-05-13@20:21:42: Epoch [12/41], Loss: 0.5502\n",
      "[INFO   ] 2025-05-13@20:21:43: Epoch [18/31], Loss: 0.4279\n",
      "[INFO   ] 2025-05-13@20:21:43: Epoch [16/42], Loss: 0.4649\n",
      "[INFO   ] 2025-05-13@20:21:43: Epoch [13/13], Loss: 0.5832\n",
      "[INFO   ] 2025-05-13@20:21:43: Epoch [3/5], Loss: 0.6416\n",
      "[INFO   ] 2025-05-13@20:21:43: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:21:47: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[I 2025-05-13 17:21:47,217] Trial 5 finished with value: 0.21506862236610824 and parameters: {'window_size': 52, 'batch_size': 91, 'num_epochs': 13, 'lr': 0.0023737841439787875}. Best is trial 6 with value: 0.5081318349474775.\n",
      "[INFO   ] 2025-05-13@20:21:47: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:21:48: Initializing model...\n",
      "[INFO   ] 2025-05-13@20:21:48: Training model...\n",
      "[INFO   ] 2025-05-13@20:21:48: Epoch [19/31], Loss: 0.4816\n",
      "[INFO   ] 2025-05-13@20:21:48: Epoch [17/42], Loss: 0.5230\n",
      "[INFO   ] 2025-05-13@20:21:48: Epoch [13/41], Loss: 0.5679\n",
      "[INFO   ] 2025-05-13@20:21:48: Epoch [4/5], Loss: 0.6903\n",
      "[INFO   ] 2025-05-13@20:21:48: Epoch [8/36], Loss: 0.9732\n",
      "[INFO   ] 2025-05-13@20:21:48: Epoch [9/38], Loss: 0.5711\n",
      "[INFO   ] 2025-05-13@20:21:48: Epoch [20/31], Loss: 0.5328\n",
      "[INFO   ] 2025-05-13@20:21:48: Epoch [18/42], Loss: 0.4605\n",
      "[INFO   ] 2025-05-13@20:21:49: Epoch [5/5], Loss: 0.6369\n",
      "[INFO   ] 2025-05-13@20:21:49: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:21:50: Epoch [14/41], Loss: 0.6419\n",
      "[INFO   ] 2025-05-13@20:21:50: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:21:50: Epoch [21/31], Loss: 0.5250\n",
      "[I 2025-05-13 17:21:50,339] Trial 8 finished with value: 0.24562450405676672 and parameters: {'window_size': 11, 'batch_size': 80, 'num_epochs': 5, 'lr': 0.0052234835930524415}. Best is trial 6 with value: 0.5081318349474775.\n",
      "[INFO   ] 2025-05-13@20:21:50: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:21:53: Initializing model...\n",
      "[INFO   ] 2025-05-13@20:21:53: Training model...\n",
      "[INFO   ] 2025-05-13@20:21:53: Epoch [19/42], Loss: 0.4970\n",
      "[INFO   ] 2025-05-13@20:21:53: Epoch [1/27], Loss: 0.7348\n",
      "[INFO   ] 2025-05-13@20:21:53: Epoch [3/48], Loss: 0.6139\n",
      "[INFO   ] 2025-05-13@20:21:53: Epoch [9/36], Loss: 0.6552\n",
      "[INFO   ] 2025-05-13@20:21:54: Epoch [22/31], Loss: 0.4828\n",
      "[INFO   ] 2025-05-13@20:21:54: Epoch [10/38], Loss: 0.8988\n",
      "[INFO   ] 2025-05-13@20:21:54: Epoch [15/41], Loss: 0.6389\n",
      "[INFO   ] 2025-05-13@20:21:54: Epoch [1/42], Loss: 0.7079\n",
      "[INFO   ] 2025-05-13@20:21:54: Epoch [20/42], Loss: 0.4603\n",
      "[INFO   ] 2025-05-13@20:21:54: Epoch [23/31], Loss: 0.5246\n",
      "[INFO   ] 2025-05-13@20:21:55: Epoch [16/41], Loss: 0.5979\n",
      "[INFO   ] 2025-05-13@20:21:55: Epoch [21/42], Loss: 0.4332\n",
      "[INFO   ] 2025-05-13@20:21:55: Epoch [2/42], Loss: 0.6989\n",
      "[INFO   ] 2025-05-13@20:21:55: Epoch [24/31], Loss: 0.4836\n",
      "[INFO   ] 2025-05-13@20:21:55: Epoch [2/27], Loss: 0.6216\n",
      "[INFO   ] 2025-05-13@20:21:55: Epoch [10/36], Loss: 0.5379\n",
      "[INFO   ] 2025-05-13@20:21:55: Epoch [11/38], Loss: 0.6682\n",
      "[INFO   ] 2025-05-13@20:21:55: Epoch [22/42], Loss: 0.4056\n",
      "[INFO   ] 2025-05-13@20:21:56: Epoch [25/31], Loss: 0.3714\n",
      "[INFO   ] 2025-05-13@20:21:56: Epoch [17/41], Loss: 0.6066\n",
      "[INFO   ] 2025-05-13@20:21:56: Epoch [3/42], Loss: 0.6760\n",
      "[INFO   ] 2025-05-13@20:21:56: Epoch [23/42], Loss: 0.4461\n",
      "[INFO   ] 2025-05-13@20:21:56: Epoch [26/31], Loss: 0.4520\n",
      "[INFO   ] 2025-05-13@20:21:57: Epoch [4/42], Loss: 0.6626\n",
      "[INFO   ] 2025-05-13@20:21:57: Epoch [18/41], Loss: 0.4939\n",
      "[INFO   ] 2025-05-13@20:21:57: Epoch [12/38], Loss: 0.7628\n",
      "[INFO   ] 2025-05-13@20:21:57: Epoch [11/36], Loss: 0.5735\n",
      "[INFO   ] 2025-05-13@20:21:57: Epoch [3/27], Loss: 0.5763\n",
      "[INFO   ] 2025-05-13@20:21:57: Epoch [24/42], Loss: 0.4302\n",
      "[INFO   ] 2025-05-13@20:21:57: Epoch [27/31], Loss: 0.4336\n",
      "[INFO   ] 2025-05-13@20:21:57: Epoch [5/42], Loss: 0.6920\n",
      "[INFO   ] 2025-05-13@20:21:58: Epoch [19/41], Loss: 0.6300\n",
      "[INFO   ] 2025-05-13@20:21:58: Epoch [28/31], Loss: 0.4632\n",
      "[INFO   ] 2025-05-13@20:21:58: Epoch [25/42], Loss: 0.4371\n",
      "[INFO   ] 2025-05-13@20:21:58: Epoch [4/48], Loss: 0.6129\n",
      "[INFO   ] 2025-05-13@20:21:58: Epoch [13/38], Loss: 0.5436\n",
      "[INFO   ] 2025-05-13@20:21:58: Epoch [6/42], Loss: 0.6370\n",
      "[INFO   ] 2025-05-13@20:21:58: Epoch [12/36], Loss: 0.9411\n",
      "[INFO   ] 2025-05-13@20:21:59: Epoch [29/31], Loss: 0.4464\n",
      "[INFO   ] 2025-05-13@20:21:59: Epoch [26/42], Loss: 0.3831\n",
      "[INFO   ] 2025-05-13@20:21:59: Epoch [20/41], Loss: 0.4384\n",
      "[INFO   ] 2025-05-13@20:21:59: Epoch [4/27], Loss: 0.7023\n",
      "[INFO   ] 2025-05-13@20:21:59: Epoch [30/31], Loss: 0.3500\n",
      "[INFO   ] 2025-05-13@20:21:59: Epoch [27/42], Loss: 0.4063\n",
      "[INFO   ] 2025-05-13@20:21:59: Epoch [7/42], Loss: 0.5793\n",
      "[INFO   ] 2025-05-13@20:22:00: Epoch [21/41], Loss: 0.4604\n",
      "[INFO   ] 2025-05-13@20:22:00: Epoch [14/38], Loss: 0.5637\n",
      "[INFO   ] 2025-05-13@20:22:00: Epoch [31/31], Loss: 0.4157\n",
      "[INFO   ] 2025-05-13@20:22:00: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:22:02: Epoch [28/42], Loss: 0.3817\n",
      "[INFO   ] 2025-05-13@20:22:02: Epoch [13/36], Loss: 0.7374\n",
      "[INFO   ] 2025-05-13@20:22:02: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[I 2025-05-13 17:22:02,897] Trial 7 finished with value: 0.11482794487333592 and parameters: {'window_size': 31, 'batch_size': 124, 'num_epochs': 31, 'lr': 0.002205438601357001}. Best is trial 6 with value: 0.5081318349474775.\n",
      "[INFO   ] 2025-05-13@20:22:02: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:22:03: Initializing model...\n",
      "[INFO   ] 2025-05-13@20:22:03: Training model...\n",
      "[INFO   ] 2025-05-13@20:22:03: Epoch [8/42], Loss: 0.6276\n",
      "[INFO   ] 2025-05-13@20:22:03: Epoch [22/41], Loss: 0.5019\n",
      "[INFO   ] 2025-05-13@20:22:03: Epoch [5/27], Loss: 0.7159\n",
      "[INFO   ] 2025-05-13@20:22:04: Epoch [29/42], Loss: 0.4220\n",
      "[INFO   ] 2025-05-13@20:22:04: Epoch [1/19], Loss: 0.6956\n",
      "[INFO   ] 2025-05-13@20:22:04: Epoch [9/42], Loss: 0.5051\n",
      "[INFO   ] 2025-05-13@20:22:04: Epoch [15/38], Loss: 0.5324\n",
      "[INFO   ] 2025-05-13@20:22:04: Epoch [23/41], Loss: 0.5527\n",
      "[INFO   ] 2025-05-13@20:22:04: Epoch [30/42], Loss: 0.3824\n",
      "[INFO   ] 2025-05-13@20:22:04: Epoch [2/19], Loss: 0.6983\n",
      "[INFO   ] 2025-05-13@20:22:04: Epoch [14/36], Loss: 0.8622\n",
      "[INFO   ] 2025-05-13@20:22:05: Epoch [10/42], Loss: 0.6760\n",
      "[INFO   ] 2025-05-13@20:22:05: Epoch [3/19], Loss: 0.6560\n",
      "[INFO   ] 2025-05-13@20:22:05: Epoch [31/42], Loss: 0.4177\n",
      "[INFO   ] 2025-05-13@20:22:05: Epoch [6/27], Loss: 0.6411\n",
      "[INFO   ] 2025-05-13@20:22:05: Epoch [24/41], Loss: 0.4227\n",
      "[INFO   ] 2025-05-13@20:22:05: Epoch [5/48], Loss: 0.6414\n",
      "[INFO   ] 2025-05-13@20:22:05: Epoch [16/38], Loss: 0.7409\n",
      "[INFO   ] 2025-05-13@20:22:06: Epoch [4/19], Loss: 0.6987\n",
      "[INFO   ] 2025-05-13@20:22:06: Epoch [11/42], Loss: 0.5268\n",
      "[INFO   ] 2025-05-13@20:22:06: Epoch [32/42], Loss: 0.3992\n",
      "[INFO   ] 2025-05-13@20:22:06: Epoch [15/36], Loss: 0.8304\n",
      "[INFO   ] 2025-05-13@20:22:06: Epoch [5/19], Loss: 0.6921\n",
      "[INFO   ] 2025-05-13@20:22:06: Epoch [25/41], Loss: 0.5491\n",
      "[INFO   ] 2025-05-13@20:22:06: Epoch [12/42], Loss: 0.5339\n",
      "[INFO   ] 2025-05-13@20:22:06: Epoch [33/42], Loss: 0.4086\n",
      "[INFO   ] 2025-05-13@20:22:07: Epoch [17/38], Loss: 0.6056\n",
      "[INFO   ] 2025-05-13@20:22:07: Epoch [6/19], Loss: 0.6797\n",
      "[INFO   ] 2025-05-13@20:22:07: Epoch [7/27], Loss: 0.6831\n",
      "[INFO   ] 2025-05-13@20:22:07: Epoch [34/42], Loss: 0.4407\n",
      "[INFO   ] 2025-05-13@20:22:07: Epoch [26/41], Loss: 0.5274\n",
      "[INFO   ] 2025-05-13@20:22:07: Epoch [13/42], Loss: 0.4342\n",
      "[INFO   ] 2025-05-13@20:22:08: Epoch [7/19], Loss: 0.6010\n",
      "[INFO   ] 2025-05-13@20:22:08: Epoch [16/36], Loss: 0.8583\n",
      "[INFO   ] 2025-05-13@20:22:08: Epoch [35/42], Loss: 0.3513\n",
      "[INFO   ] 2025-05-13@20:22:08: Epoch [8/19], Loss: 0.6765\n",
      "[INFO   ] 2025-05-13@20:22:08: Epoch [27/41], Loss: 0.5592\n",
      "[INFO   ] 2025-05-13@20:22:08: Epoch [14/42], Loss: 0.5597\n",
      "[INFO   ] 2025-05-13@20:22:08: Epoch [18/38], Loss: 0.7954\n",
      "[INFO   ] 2025-05-13@20:22:09: Epoch [8/27], Loss: 0.6724\n",
      "[INFO   ] 2025-05-13@20:22:09: Epoch [36/42], Loss: 0.3825\n",
      "[INFO   ] 2025-05-13@20:22:09: Epoch [9/19], Loss: 0.5853\n",
      "[INFO   ] 2025-05-13@20:22:09: Epoch [15/42], Loss: 0.5111\n",
      "[INFO   ] 2025-05-13@20:22:09: Epoch [28/41], Loss: 0.4352\n",
      "[INFO   ] 2025-05-13@20:22:09: Epoch [17/36], Loss: 0.7425\n",
      "[INFO   ] 2025-05-13@20:22:09: Epoch [10/19], Loss: 0.6480\n",
      "[INFO   ] 2025-05-13@20:22:09: Epoch [37/42], Loss: 0.3360\n",
      "[INFO   ] 2025-05-13@20:22:10: Epoch [19/38], Loss: 0.6826\n",
      "[INFO   ] 2025-05-13@20:22:10: Epoch [6/48], Loss: 0.5270\n",
      "[INFO   ] 2025-05-13@20:22:10: Epoch [16/42], Loss: 0.5339\n",
      "[INFO   ] 2025-05-13@20:22:10: Epoch [29/41], Loss: 0.5407\n",
      "[INFO   ] 2025-05-13@20:22:10: Epoch [11/19], Loss: 0.6336\n",
      "[INFO   ] 2025-05-13@20:22:10: Epoch [38/42], Loss: 0.3618\n",
      "[INFO   ] 2025-05-13@20:22:10: Epoch [9/27], Loss: 0.7221\n",
      "[INFO   ] 2025-05-13@20:22:11: Epoch [17/42], Loss: 0.6505\n",
      "[INFO   ] 2025-05-13@20:22:11: Epoch [12/19], Loss: 0.6313\n",
      "[INFO   ] 2025-05-13@20:22:11: Epoch [39/42], Loss: 0.3537\n",
      "[INFO   ] 2025-05-13@20:22:11: Epoch [18/36], Loss: 0.7501\n",
      "[INFO   ] 2025-05-13@20:22:11: Epoch [30/41], Loss: 0.4005\n",
      "[INFO   ] 2025-05-13@20:22:11: Epoch [20/38], Loss: 0.9032\n",
      "[INFO   ] 2025-05-13@20:22:11: Epoch [13/19], Loss: 0.5379\n",
      "[INFO   ] 2025-05-13@20:22:12: Epoch [18/42], Loss: 0.4250\n",
      "[INFO   ] 2025-05-13@20:22:12: Epoch [40/42], Loss: 0.4386\n",
      "[INFO   ] 2025-05-13@20:22:12: Epoch [31/41], Loss: 0.4605\n",
      "[INFO   ] 2025-05-13@20:22:12: Epoch [14/19], Loss: 0.5007\n",
      "[INFO   ] 2025-05-13@20:22:12: Epoch [10/27], Loss: 0.7090\n",
      "[INFO   ] 2025-05-13@20:22:12: Epoch [41/42], Loss: 0.3828\n",
      "[INFO   ] 2025-05-13@20:22:12: Epoch [19/42], Loss: 0.5411\n",
      "[INFO   ] 2025-05-13@20:22:13: Epoch [21/38], Loss: 0.7351\n",
      "[INFO   ] 2025-05-13@20:22:13: Epoch [19/36], Loss: 0.7984\n",
      "[INFO   ] 2025-05-13@20:22:13: Epoch [15/19], Loss: 0.5563\n",
      "[INFO   ] 2025-05-13@20:22:13: Epoch [32/41], Loss: 0.7039\n",
      "[INFO   ] 2025-05-13@20:22:13: Epoch [42/42], Loss: 0.3415\n",
      "[INFO   ] 2025-05-13@20:22:13: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:22:17: Epoch [20/42], Loss: 0.3995\n",
      "[INFO   ] 2025-05-13@20:22:17: Epoch [16/19], Loss: 0.5549\n",
      "[INFO   ] 2025-05-13@20:22:17: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[I 2025-05-13 17:22:17,893] Trial 2 finished with value: 0.06270851675879607 and parameters: {'window_size': 58, 'batch_size': 128, 'num_epochs': 42, 'lr': 0.0016894495253381443}. Best is trial 6 with value: 0.5081318349474775.\n",
      "[INFO   ] 2025-05-13@20:22:17: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:22:20: Initializing model...\n",
      "[INFO   ] 2025-05-13@20:22:20: Training model...\n",
      "[INFO   ] 2025-05-13@20:22:20: Epoch [11/27], Loss: 0.6700\n",
      "[INFO   ] 2025-05-13@20:22:20: Epoch [17/19], Loss: 0.6030\n",
      "[INFO   ] 2025-05-13@20:22:20: Epoch [33/41], Loss: 0.3981\n",
      "[INFO   ] 2025-05-13@20:22:21: Epoch [22/38], Loss: 0.6254\n",
      "[INFO   ] 2025-05-13@20:22:21: Epoch [21/42], Loss: 0.5779\n",
      "[INFO   ] 2025-05-13@20:22:21: Epoch [20/36], Loss: 0.7155\n",
      "[INFO   ] 2025-05-13@20:22:21: Epoch [7/48], Loss: 0.6418\n",
      "[INFO   ] 2025-05-13@20:22:21: Epoch [1/35], Loss: 0.8382\n",
      "[INFO   ] 2025-05-13@20:22:21: Epoch [18/19], Loss: 0.5639\n",
      "[INFO   ] 2025-05-13@20:22:21: Epoch [34/41], Loss: 0.4006\n",
      "[INFO   ] 2025-05-13@20:22:21: Epoch [22/42], Loss: 0.4886\n",
      "[INFO   ] 2025-05-13@20:22:22: Epoch [19/19], Loss: 0.6390\n",
      "[INFO   ] 2025-05-13@20:22:22: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:22:22: Epoch [2/35], Loss: 0.7263\n",
      "[INFO   ] 2025-05-13@20:22:22: Epoch [23/38], Loss: 0.6645\n",
      "[INFO   ] 2025-05-13@20:22:23: Epoch [12/27], Loss: 0.7223\n",
      "[INFO   ] 2025-05-13@20:22:23: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[I 2025-05-13 17:22:23,214] Trial 11 finished with value: 0.2080115299039882 and parameters: {'window_size': 8, 'batch_size': 90, 'num_epochs': 19, 'lr': 0.0031323488755500123}. Best is trial 6 with value: 0.5081318349474775.\n",
      "[INFO   ] 2025-05-13@20:22:23: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:22:23: Initializing model...\n",
      "[INFO   ] 2025-05-13@20:22:23: Training model...\n",
      "[INFO   ] 2025-05-13@20:22:23: Epoch [21/36], Loss: 0.7539\n",
      "[INFO   ] 2025-05-13@20:22:23: Epoch [23/42], Loss: 0.4024\n",
      "[INFO   ] 2025-05-13@20:22:24: Epoch [35/41], Loss: 0.5072\n",
      "[INFO   ] 2025-05-13@20:22:24: Epoch [3/35], Loss: 0.8567\n",
      "[INFO   ] 2025-05-13@20:22:24: Epoch [1/19], Loss: 0.6754\n",
      "[INFO   ] 2025-05-13@20:22:25: Epoch [24/42], Loss: 0.4413\n",
      "[INFO   ] 2025-05-13@20:22:25: Epoch [24/38], Loss: 0.6284\n",
      "[INFO   ] 2025-05-13@20:22:25: Epoch [36/41], Loss: 0.5906\n",
      "[INFO   ] 2025-05-13@20:22:25: Epoch [2/19], Loss: 0.7148\n",
      "[INFO   ] 2025-05-13@20:22:25: Epoch [4/35], Loss: 0.8567\n",
      "[INFO   ] 2025-05-13@20:22:25: Epoch [13/27], Loss: 0.6596\n",
      "[INFO   ] 2025-05-13@20:22:25: Epoch [22/36], Loss: 0.7628\n",
      "[INFO   ] 2025-05-13@20:22:25: Epoch [25/42], Loss: 0.4043\n",
      "[INFO   ] 2025-05-13@20:22:25: Epoch [3/19], Loss: 0.6566\n",
      "[INFO   ] 2025-05-13@20:22:26: Epoch [5/35], Loss: 0.9220\n",
      "[INFO   ] 2025-05-13@20:22:26: Epoch [37/41], Loss: 0.3695\n",
      "[INFO   ] 2025-05-13@20:22:26: Epoch [25/38], Loss: 0.5725\n",
      "[INFO   ] 2025-05-13@20:22:26: Epoch [4/19], Loss: 0.6853\n",
      "[INFO   ] 2025-05-13@20:22:26: Epoch [26/42], Loss: 0.3742\n",
      "[INFO   ] 2025-05-13@20:22:26: Epoch [8/48], Loss: 0.5456\n",
      "[INFO   ] 2025-05-13@20:22:26: Epoch [6/35], Loss: 0.7698\n",
      "[INFO   ] 2025-05-13@20:22:27: Epoch [23/36], Loss: 0.6091\n",
      "[INFO   ] 2025-05-13@20:22:27: Epoch [38/41], Loss: 0.3882\n",
      "[INFO   ] 2025-05-13@20:22:27: Epoch [14/27], Loss: 0.6828\n",
      "[INFO   ] 2025-05-13@20:22:27: Epoch [5/19], Loss: 0.6595\n",
      "[INFO   ] 2025-05-13@20:22:27: Epoch [27/42], Loss: 0.3441\n",
      "[INFO   ] 2025-05-13@20:22:27: Epoch [26/38], Loss: 0.5568\n",
      "[INFO   ] 2025-05-13@20:22:27: Epoch [7/35], Loss: 0.8785\n",
      "[INFO   ] 2025-05-13@20:22:28: Epoch [39/41], Loss: 0.5801\n",
      "[INFO   ] 2025-05-13@20:22:28: Epoch [6/19], Loss: 0.6469\n",
      "[INFO   ] 2025-05-13@20:22:28: Epoch [28/42], Loss: 0.3391\n",
      "[INFO   ] 2025-05-13@20:22:28: Epoch [24/36], Loss: 0.8430\n",
      "[INFO   ] 2025-05-13@20:22:28: Epoch [8/35], Loss: 0.7046\n",
      "[INFO   ] 2025-05-13@20:22:28: Epoch [7/19], Loss: 0.7139\n",
      "[INFO   ] 2025-05-13@20:22:28: Epoch [15/27], Loss: 0.6698\n",
      "[INFO   ] 2025-05-13@20:22:28: Epoch [40/41], Loss: 0.4007\n",
      "[INFO   ] 2025-05-13@20:22:29: Epoch [27/38], Loss: 0.4549\n",
      "[INFO   ] 2025-05-13@20:22:29: Epoch [29/42], Loss: 0.3245\n",
      "[INFO   ] 2025-05-13@20:22:29: Epoch [8/19], Loss: 0.6260\n",
      "[INFO   ] 2025-05-13@20:22:29: Epoch [9/35], Loss: 0.8133\n",
      "[INFO   ] 2025-05-13@20:22:29: Epoch [41/41], Loss: 0.3133\n",
      "[INFO   ] 2025-05-13@20:22:29: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:22:32: Epoch [25/36], Loss: 0.8723\n",
      "[INFO   ] 2025-05-13@20:22:32: Epoch [9/19], Loss: 0.5984\n",
      "[INFO   ] 2025-05-13@20:22:32: Epoch [30/42], Loss: 0.5208\n",
      "[INFO   ] 2025-05-13@20:22:32: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[I 2025-05-13 17:22:32,557] Trial 3 finished with value: 0.23820299136290768 and parameters: {'window_size': 31, 'batch_size': 65, 'num_epochs': 41, 'lr': 0.002890175501005575}. Best is trial 6 with value: 0.5081318349474775.\n",
      "[INFO   ] 2025-05-13@20:22:32: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:22:35: Initializing model...\n",
      "[INFO   ] 2025-05-13@20:22:35: Epoch [10/35], Loss: 0.8350\n",
      "[INFO   ] 2025-05-13@20:22:35: Training model...\n",
      "[INFO   ] 2025-05-13@20:22:35: Epoch [28/38], Loss: 0.4707\n",
      "[INFO   ] 2025-05-13@20:22:35: Epoch [16/27], Loss: 0.6828\n",
      "[INFO   ] 2025-05-13@20:22:35: Epoch [10/19], Loss: 0.5574\n",
      "[INFO   ] 2025-05-13@20:22:36: Epoch [9/48], Loss: 0.6424\n",
      "[INFO   ] 2025-05-13@20:22:36: Epoch [31/42], Loss: 0.3780\n",
      "[INFO   ] 2025-05-13@20:22:36: Epoch [11/35], Loss: 0.9220\n",
      "[INFO   ] 2025-05-13@20:22:36: Epoch [1/41], Loss: 0.7172\n",
      "[INFO   ] 2025-05-13@20:22:36: Epoch [11/19], Loss: 0.5403\n",
      "[INFO   ] 2025-05-13@20:22:36: Epoch [26/36], Loss: 0.8496\n",
      "[INFO   ] 2025-05-13@20:22:36: Epoch [29/38], Loss: 0.6046\n",
      "[INFO   ] 2025-05-13@20:22:37: Epoch [32/42], Loss: 0.3596\n",
      "[INFO   ] 2025-05-13@20:22:37: Epoch [12/35], Loss: 0.9437\n",
      "[INFO   ] 2025-05-13@20:22:37: Epoch [17/27], Loss: 0.6839\n",
      "[INFO   ] 2025-05-13@20:22:37: Epoch [2/41], Loss: 0.6758\n",
      "[INFO   ] 2025-05-13@20:22:37: Epoch [12/19], Loss: 0.5770\n",
      "[INFO   ] 2025-05-13@20:22:37: Epoch [33/42], Loss: 0.3321\n",
      "[INFO   ] 2025-05-13@20:22:37: Epoch [13/35], Loss: 0.7480\n",
      "[INFO   ] 2025-05-13@20:22:38: Epoch [13/19], Loss: 0.6089\n",
      "[INFO   ] 2025-05-13@20:22:38: Epoch [3/41], Loss: 0.6439\n",
      "[INFO   ] 2025-05-13@20:22:38: Epoch [30/38], Loss: 0.8702\n",
      "[INFO   ] 2025-05-13@20:22:38: Epoch [27/36], Loss: 0.6173\n",
      "[INFO   ] 2025-05-13@20:22:38: Epoch [14/19], Loss: 0.6033\n",
      "[INFO   ] 2025-05-13@20:22:38: Epoch [34/42], Loss: 0.4127\n",
      "[INFO   ] 2025-05-13@20:22:38: Epoch [14/35], Loss: 0.9220\n",
      "[INFO   ] 2025-05-13@20:22:38: Epoch [18/27], Loss: 0.6961\n",
      "[INFO   ] 2025-05-13@20:22:38: Epoch [4/41], Loss: 0.6675\n",
      "[INFO   ] 2025-05-13@20:22:39: Epoch [15/19], Loss: 0.6184\n",
      "[INFO   ] 2025-05-13@20:22:39: Epoch [15/35], Loss: 0.8785\n",
      "[INFO   ] 2025-05-13@20:22:39: Epoch [31/38], Loss: 0.3449\n",
      "[INFO   ] 2025-05-13@20:22:39: Epoch [35/42], Loss: 0.4316\n",
      "[INFO   ] 2025-05-13@20:22:39: Epoch [5/41], Loss: 0.5947\n",
      "[INFO   ] 2025-05-13@20:22:40: Epoch [28/36], Loss: 0.8777\n",
      "[INFO   ] 2025-05-13@20:22:40: Epoch [16/19], Loss: 0.5875\n",
      "[INFO   ] 2025-05-13@20:22:40: Epoch [16/35], Loss: 0.8133\n",
      "[INFO   ] 2025-05-13@20:22:40: Epoch [10/48], Loss: 0.7614\n",
      "[INFO   ] 2025-05-13@20:22:40: Epoch [19/27], Loss: 0.6960\n",
      "[INFO   ] 2025-05-13@20:22:40: Epoch [36/42], Loss: 0.5333\n",
      "[INFO   ] 2025-05-13@20:22:40: Epoch [6/41], Loss: 0.7121\n",
      "[INFO   ] 2025-05-13@20:22:40: Epoch [17/19], Loss: 0.6332\n",
      "[INFO   ] 2025-05-13@20:22:41: Epoch [32/38], Loss: 0.5686\n",
      "[INFO   ] 2025-05-13@20:22:41: Epoch [17/35], Loss: 0.9220\n",
      "[INFO   ] 2025-05-13@20:22:41: Epoch [7/41], Loss: 0.5738\n",
      "[INFO   ] 2025-05-13@20:22:41: Epoch [29/36], Loss: 0.8907\n",
      "[INFO   ] 2025-05-13@20:22:41: Epoch [37/42], Loss: 0.3752\n",
      "[INFO   ] 2025-05-13@20:22:41: Epoch [18/19], Loss: 0.4806\n",
      "[INFO   ] 2025-05-13@20:22:42: Epoch [18/35], Loss: 0.8567\n",
      "[INFO   ] 2025-05-13@20:22:42: Epoch [19/19], Loss: 0.5786\n",
      "[INFO   ] 2025-05-13@20:22:42: Epoch [20/27], Loss: 0.6828\n",
      "[INFO   ] 2025-05-13@20:22:42: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:22:42: Epoch [8/41], Loss: 0.6926\n",
      "[INFO   ] 2025-05-13@20:22:42: Epoch [38/42], Loss: 0.3986\n",
      "[INFO   ] 2025-05-13@20:22:42: Epoch [33/38], Loss: 0.6100\n",
      "[INFO   ] 2025-05-13@20:22:43: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[I 2025-05-13 17:22:43,192] Trial 13 finished with value: 0.36633160267936804 and parameters: {'window_size': 6, 'batch_size': 74, 'num_epochs': 19, 'lr': 0.0028579352682015008}. Best is trial 6 with value: 0.5081318349474775.\n",
      "[INFO   ] 2025-05-13@20:22:43: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:22:44: Initializing model...\n",
      "[INFO   ] 2025-05-13@20:22:44: Training model...\n",
      "[INFO   ] 2025-05-13@20:22:44: Epoch [30/36], Loss: 0.5467\n",
      "[INFO   ] 2025-05-13@20:22:44: Epoch [19/35], Loss: 0.9437\n",
      "[INFO   ] 2025-05-13@20:22:44: Epoch [9/41], Loss: 0.6353\n",
      "[INFO   ] 2025-05-13@20:22:44: Epoch [1/34], Loss: 0.7072\n",
      "[INFO   ] 2025-05-13@20:22:44: Epoch [39/42], Loss: 0.3520\n",
      "[INFO   ] 2025-05-13@20:22:45: Epoch [20/35], Loss: 0.8133\n",
      "[INFO   ] 2025-05-13@20:22:45: Epoch [34/38], Loss: 0.6128\n",
      "[INFO   ] 2025-05-13@20:22:45: Epoch [2/34], Loss: 0.6934\n",
      "[INFO   ] 2025-05-13@20:22:45: Epoch [21/27], Loss: 0.7220\n",
      "[INFO   ] 2025-05-13@20:22:45: Epoch [40/42], Loss: 0.3964\n",
      "[INFO   ] 2025-05-13@20:22:45: Epoch [10/41], Loss: 0.6312\n",
      "[INFO   ] 2025-05-13@20:22:46: Epoch [31/36], Loss: 0.7155\n",
      "[INFO   ] 2025-05-13@20:22:46: Epoch [3/34], Loss: 0.7015\n",
      "[INFO   ] 2025-05-13@20:22:46: Epoch [21/35], Loss: 0.8133\n",
      "[INFO   ] 2025-05-13@20:22:46: Epoch [11/48], Loss: 0.7873\n",
      "[INFO   ] 2025-05-13@20:22:46: Epoch [11/41], Loss: 0.5871\n",
      "[INFO   ] 2025-05-13@20:22:46: Epoch [41/42], Loss: 0.3521\n",
      "[INFO   ] 2025-05-13@20:22:46: Epoch [4/34], Loss: 0.6857\n",
      "[INFO   ] 2025-05-13@20:22:47: Epoch [35/38], Loss: 0.6490\n",
      "[INFO   ] 2025-05-13@20:22:47: Epoch [22/35], Loss: 0.8567\n",
      "[INFO   ] 2025-05-13@20:22:47: Epoch [5/34], Loss: 0.6706\n",
      "[INFO   ] 2025-05-13@20:22:47: Epoch [22/27], Loss: 0.6841\n",
      "[INFO   ] 2025-05-13@20:22:47: Epoch [12/41], Loss: 0.5024\n",
      "[INFO   ] 2025-05-13@20:22:47: Epoch [42/42], Loss: 0.3156\n",
      "[INFO   ] 2025-05-13@20:22:47: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:22:51: Epoch [32/36], Loss: 0.7054\n",
      "[INFO   ] 2025-05-13@20:22:51: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[I 2025-05-13 17:22:51,303] Trial 10 finished with value: 0.18639440608714342 and parameters: {'window_size': 47, 'batch_size': 94, 'num_epochs': 42, 'lr': 0.0025869386099320224}. Best is trial 6 with value: 0.5081318349474775.\n",
      "[INFO   ] 2025-05-13@20:22:51: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:22:52: Initializing model...\n",
      "[INFO   ] 2025-05-13@20:22:52: Training model...\n",
      "[INFO   ] 2025-05-13@20:22:52: Epoch [6/34], Loss: 0.6401\n",
      "[INFO   ] 2025-05-13@20:22:52: Epoch [23/35], Loss: 0.7915\n",
      "[INFO   ] 2025-05-13@20:22:52: Epoch [13/41], Loss: 0.5881\n",
      "[INFO   ] 2025-05-13@20:22:52: Epoch [36/38], Loss: 0.3918\n",
      "[INFO   ] 2025-05-13@20:22:52: Epoch [7/34], Loss: 0.6317\n",
      "[INFO   ] 2025-05-13@20:22:53: Epoch [24/35], Loss: 0.7698\n",
      "[INFO   ] 2025-05-13@20:22:53: Epoch [1/40], Loss: 0.6936\n",
      "[INFO   ] 2025-05-13@20:22:53: Epoch [23/27], Loss: 0.6963\n",
      "[INFO   ] 2025-05-13@20:22:53: Epoch [33/36], Loss: 0.8420\n",
      "[INFO   ] 2025-05-13@20:22:53: Epoch [14/41], Loss: 0.4816\n",
      "[INFO   ] 2025-05-13@20:22:53: Epoch [8/34], Loss: 0.7152\n",
      "[INFO   ] 2025-05-13@20:22:54: Epoch [37/38], Loss: 0.4441\n",
      "[INFO   ] 2025-05-13@20:22:54: Epoch [25/35], Loss: 0.7915\n",
      "[INFO   ] 2025-05-13@20:22:54: Epoch [9/34], Loss: 0.7155\n",
      "[INFO   ] 2025-05-13@20:22:54: Epoch [15/41], Loss: 0.5926\n",
      "[INFO   ] 2025-05-13@20:22:54: Epoch [2/40], Loss: 0.6934\n",
      "[INFO   ] 2025-05-13@20:22:54: Epoch [26/35], Loss: 0.9437\n",
      "[INFO   ] 2025-05-13@20:22:54: Epoch [24/27], Loss: 0.7097\n",
      "[INFO   ] 2025-05-13@20:22:54: Epoch [34/36], Loss: 0.7163\n",
      "[INFO   ] 2025-05-13@20:22:55: Epoch [10/34], Loss: 0.7239\n",
      "[INFO   ] 2025-05-13@20:22:55: Epoch [12/48], Loss: 0.5072\n",
      "[INFO   ] 2025-05-13@20:22:55: Epoch [16/41], Loss: 0.4596\n",
      "[INFO   ] 2025-05-13@20:22:55: Epoch [38/38], Loss: 0.6520\n",
      "[INFO   ] 2025-05-13@20:22:55: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:22:56: Epoch [3/40], Loss: 0.6309\n",
      "[INFO   ] 2025-05-13@20:22:56: Epoch [27/35], Loss: 0.9220\n",
      "[INFO   ] 2025-05-13@20:22:56: Epoch [11/34], Loss: 0.6918\n",
      "[INFO   ] 2025-05-13@20:22:56: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[I 2025-05-13 17:22:56,834] Trial 4 finished with value: 0.0753417963962657 and parameters: {'window_size': 12, 'batch_size': 33, 'num_epochs': 38, 'lr': 0.0030388955643862936}. Best is trial 6 with value: 0.5081318349474775.\n",
      "[INFO   ] 2025-05-13@20:22:56: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:22:59: Initializing model...\n",
      "[INFO   ] 2025-05-13@20:22:59: Training model...\n",
      "[INFO   ] 2025-05-13@20:22:59: Epoch [17/41], Loss: 0.5502\n",
      "[INFO   ] 2025-05-13@20:22:59: Epoch [12/34], Loss: 0.5978\n",
      "[INFO   ] 2025-05-13@20:22:59: Epoch [28/35], Loss: 0.9002\n",
      "[INFO   ] 2025-05-13@20:23:00: Epoch [35/36], Loss: 0.7000\n",
      "[INFO   ] 2025-05-13@20:23:00: Epoch [25/27], Loss: 0.6684\n",
      "[INFO   ] 2025-05-13@20:23:00: Epoch [4/40], Loss: 0.6710\n",
      "[INFO   ] 2025-05-13@20:23:00: Epoch [18/41], Loss: 0.5912\n",
      "[INFO   ] 2025-05-13@20:23:00: Epoch [13/34], Loss: 0.7338\n",
      "[INFO   ] 2025-05-13@20:23:00: Epoch [29/35], Loss: 0.7915\n",
      "[INFO   ] 2025-05-13@20:23:01: Epoch [14/34], Loss: 0.5302\n",
      "[INFO   ] 2025-05-13@20:23:01: Epoch [5/40], Loss: 0.7122\n",
      "[INFO   ] 2025-05-13@20:23:01: Epoch [19/41], Loss: 0.5514\n",
      "[INFO   ] 2025-05-13@20:23:01: Epoch [36/36], Loss: 0.6318\n",
      "[INFO   ] 2025-05-13@20:23:01: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:23:03: Epoch [30/35], Loss: 0.7915\n",
      "[INFO   ] 2025-05-13@20:23:03: Epoch [26/27], Loss: 0.6961\n",
      "[INFO   ] 2025-05-13@20:23:03: Epoch [15/34], Loss: 0.6514\n",
      "[INFO   ] 2025-05-13@20:23:03: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:23:04: Epoch [20/41], Loss: 0.4878\n",
      "[I 2025-05-13 17:23:04,113] Trial 0 finished with value: 0.46070308409693883 and parameters: {'window_size': 27, 'batch_size': 34, 'num_epochs': 36, 'lr': 0.00451296225965972}. Best is trial 6 with value: 0.5081318349474775.\n",
      "[INFO   ] 2025-05-13@20:23:04: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:23:06: Initializing model...\n",
      "[INFO   ] 2025-05-13@20:23:06: Training model...\n",
      "[INFO   ] 2025-05-13@20:23:07: Epoch [31/35], Loss: 0.8785\n",
      "[INFO   ] 2025-05-13@20:23:07: Epoch [6/40], Loss: 0.6208\n",
      "[INFO   ] 2025-05-13@20:23:07: Epoch [16/34], Loss: 0.6933\n",
      "[INFO   ] 2025-05-13@20:23:07: Epoch [1/6], Loss: 0.8133\n",
      "[INFO   ] 2025-05-13@20:23:07: Epoch [13/48], Loss: 0.7877\n",
      "[INFO   ] 2025-05-13@20:23:07: Epoch [21/41], Loss: 0.5040\n",
      "[INFO   ] 2025-05-13@20:23:07: Epoch [32/35], Loss: 1.0089\n",
      "[INFO   ] 2025-05-13@20:23:07: Epoch [17/34], Loss: 0.6053\n",
      "[INFO   ] 2025-05-13@20:23:08: Epoch [27/27], Loss: 0.7092\n",
      "[INFO   ] 2025-05-13@20:23:08: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:23:09: Epoch [7/40], Loss: 0.6620\n",
      "[INFO   ] 2025-05-13@20:23:09: Epoch [18/34], Loss: 0.7423\n",
      "[INFO   ] 2025-05-13@20:23:09: Epoch [22/41], Loss: 0.5986\n",
      "[INFO   ] 2025-05-13@20:23:09: Epoch [33/35], Loss: 0.8567\n",
      "[INFO   ] 2025-05-13@20:23:09: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[I 2025-05-13 17:23:09,804] Trial 9 finished with value: 0.44100850832694405 and parameters: {'window_size': 12, 'batch_size': 27, 'num_epochs': 27, 'lr': 0.008556753040735922}. Best is trial 6 with value: 0.5081318349474775.\n",
      "[INFO   ] 2025-05-13@20:23:09: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:23:12: Initializing model...\n",
      "[INFO   ] 2025-05-13@20:23:12: Training model...\n",
      "[INFO   ] 2025-05-13@20:23:12: Epoch [19/34], Loss: 0.6435\n",
      "[INFO   ] 2025-05-13@20:23:13: Epoch [8/40], Loss: 0.7009\n",
      "[INFO   ] 2025-05-13@20:23:13: Epoch [34/35], Loss: 0.8785\n",
      "[INFO   ] 2025-05-13@20:23:13: Epoch [23/41], Loss: 0.5200\n",
      "[INFO   ] 2025-05-13@20:23:13: Epoch [20/34], Loss: 0.6689\n",
      "[INFO   ] 2025-05-13@20:23:13: Epoch [35/35], Loss: 0.7915\n",
      "[INFO   ] 2025-05-13@20:23:13: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:23:16: Epoch [2/6], Loss: 1.0133\n",
      "[INFO   ] 2025-05-13@20:23:16: Epoch [1/31], Loss: 0.5633\n",
      "[INFO   ] 2025-05-13@20:23:16: Epoch [24/41], Loss: 0.4776\n",
      "[INFO   ] 2025-05-13@20:23:16: Epoch [9/40], Loss: 0.6255\n",
      "[INFO   ] 2025-05-13@20:23:16: Epoch [21/34], Loss: 0.6628\n",
      "[INFO   ] 2025-05-13@20:23:16: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[I 2025-05-13 17:23:17,150] Trial 12 finished with value: 0.4966020290042636 and parameters: {'window_size': 41, 'batch_size': 99, 'num_epochs': 35, 'lr': 0.00900092830042101}. Best is trial 6 with value: 0.5081318349474775.\n",
      "[INFO   ] 2025-05-13@20:23:17: Epoch [22/34], Loss: 0.6178\n",
      "[INFO   ] 2025-05-13@20:23:17: Epoch [25/41], Loss: 0.4530\n",
      "[INFO   ] 2025-05-13@20:23:17: Epoch [10/40], Loss: 0.7015\n",
      "[INFO   ] 2025-05-13@20:23:17: Epoch [14/48], Loss: 0.6609\n",
      "[INFO   ] 2025-05-13@20:23:18: Epoch [23/34], Loss: 0.6163\n",
      "[INFO   ] 2025-05-13@20:23:18: Epoch [26/41], Loss: 0.4526\n",
      "[INFO   ] 2025-05-13@20:23:18: Epoch [24/34], Loss: 0.6180\n",
      "[INFO   ] 2025-05-13@20:23:18: Epoch [11/40], Loss: 0.6907\n",
      "[INFO   ] 2025-05-13@20:23:18: Epoch [27/41], Loss: 0.5209\n",
      "[INFO   ] 2025-05-13@20:23:19: Epoch [25/34], Loss: 0.6519\n",
      "[INFO   ] 2025-05-13@20:23:19: Epoch [3/6], Loss: 0.5133\n",
      "[INFO   ] 2025-05-13@20:23:19: Epoch [12/40], Loss: 0.7126\n",
      "[INFO   ] 2025-05-13@20:23:19: Epoch [2/31], Loss: 1.0633\n",
      "[INFO   ] 2025-05-13@20:23:19: Epoch [26/34], Loss: 0.6255\n",
      "[INFO   ] 2025-05-13@20:23:19: Epoch [28/41], Loss: 0.5003\n",
      "[INFO   ] 2025-05-13@20:23:20: Epoch [27/34], Loss: 0.5745\n",
      "[INFO   ] 2025-05-13@20:23:20: Epoch [29/41], Loss: 0.4647\n",
      "[INFO   ] 2025-05-13@20:23:20: Epoch [13/40], Loss: 0.7706\n",
      "[INFO   ] 2025-05-13@20:23:20: Epoch [28/34], Loss: 0.6248\n",
      "[INFO   ] 2025-05-13@20:23:21: Epoch [30/41], Loss: 0.6148\n",
      "[INFO   ] 2025-05-13@20:23:21: Epoch [29/34], Loss: 0.7036\n",
      "[INFO   ] 2025-05-13@20:23:21: Epoch [15/48], Loss: 0.5633\n",
      "[INFO   ] 2025-05-13@20:23:21: Epoch [14/40], Loss: 0.7061\n",
      "[INFO   ] 2025-05-13@20:23:21: Epoch [4/6], Loss: 0.9133\n",
      "[INFO   ] 2025-05-13@20:23:21: Epoch [30/34], Loss: 0.6621\n",
      "[INFO   ] 2025-05-13@20:23:21: Epoch [31/41], Loss: 0.5337\n",
      "[INFO   ] 2025-05-13@20:23:22: Epoch [15/40], Loss: 0.7057\n",
      "[INFO   ] 2025-05-13@20:23:22: Epoch [31/34], Loss: 0.6536\n",
      "[INFO   ] 2025-05-13@20:23:22: Epoch [1/49], Loss: 0.7133\n",
      "[INFO   ] 2025-05-13@20:23:22: Epoch [3/31], Loss: 0.8133\n",
      "[INFO   ] 2025-05-13@20:23:22: Epoch [32/41], Loss: 0.5130\n",
      "[INFO   ] 2025-05-13@20:23:22: Epoch [32/34], Loss: 0.6355\n",
      "[INFO   ] 2025-05-13@20:23:23: Epoch [16/40], Loss: 0.7629\n",
      "[INFO   ] 2025-05-13@20:23:23: Epoch [33/41], Loss: 0.4527\n",
      "[INFO   ] 2025-05-13@20:23:23: Epoch [33/34], Loss: 0.7488\n",
      "[INFO   ] 2025-05-13@20:23:23: Epoch [34/34], Loss: 0.7037\n",
      "[INFO   ] 2025-05-13@20:23:23: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:23:24: Epoch [34/41], Loss: 0.4699\n",
      "[INFO   ] 2025-05-13@20:23:25: Epoch [17/40], Loss: 0.5645\n",
      "[INFO   ] 2025-05-13@20:23:25: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[I 2025-05-13 17:23:25,189] Trial 15 finished with value: 0.5555453357478496 and parameters: {'window_size': 16, 'batch_size': 98, 'num_epochs': 34, 'lr': 0.007362634552544321}. Best is trial 15 with value: 0.5555453357478496.\n",
      "[INFO   ] 2025-05-13@20:23:25: Epoch [5/6], Loss: 0.8133\n",
      "[INFO   ] 2025-05-13@20:23:25: Epoch [16/48], Loss: 0.5483\n",
      "[INFO   ] 2025-05-13@20:23:25: Epoch [35/41], Loss: 0.4812\n",
      "[INFO   ] 2025-05-13@20:23:25: Epoch [18/40], Loss: 0.6706\n",
      "[INFO   ] 2025-05-13@20:23:26: Epoch [4/31], Loss: 1.3133\n",
      "[INFO   ] 2025-05-13@20:23:26: Epoch [36/41], Loss: 0.4584\n",
      "[INFO   ] 2025-05-13@20:23:26: Epoch [19/40], Loss: 0.7166\n",
      "[INFO   ] 2025-05-13@20:23:26: Epoch [37/41], Loss: 0.3993\n",
      "[INFO   ] 2025-05-13@20:23:27: Epoch [6/6], Loss: 0.8133\n",
      "[INFO   ] 2025-05-13@20:23:27: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:23:30: Epoch [38/41], Loss: 0.4183\n",
      "[INFO   ] 2025-05-13@20:23:30: Epoch [20/40], Loss: 0.7746\n",
      "[INFO   ] 2025-05-13@20:23:30: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[I 2025-05-13 17:23:30,806] Trial 17 finished with value: 0.49082397168989345 and parameters: {'window_size': 41, 'batch_size': 15, 'num_epochs': 6, 'lr': 0.00978421977853116}. Best is trial 15 with value: 0.5555453357478496.\n",
      "[INFO   ] 2025-05-13@20:23:30: Epoch [39/41], Loss: 0.3501\n",
      "[INFO   ] 2025-05-13@20:23:31: Epoch [21/40], Loss: 0.6854\n",
      "[INFO   ] 2025-05-13@20:23:31: Epoch [5/31], Loss: 0.8133\n",
      "[INFO   ] 2025-05-13@20:23:31: Epoch [17/48], Loss: 0.7682\n",
      "[INFO   ] 2025-05-13@20:23:31: Epoch [40/41], Loss: 0.4087\n",
      "[INFO   ] 2025-05-13@20:23:31: Epoch [22/40], Loss: 0.7827\n",
      "[INFO   ] 2025-05-13@20:23:31: Epoch [41/41], Loss: 0.4714\n",
      "[INFO   ] 2025-05-13@20:23:31: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:23:34: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[I 2025-05-13 17:23:34,945] Trial 14 finished with value: 0.055356699612984506 and parameters: {'window_size': 43, 'batch_size': 84, 'num_epochs': 41, 'lr': 0.003611639864989722}. Best is trial 15 with value: 0.5555453357478496.\n",
      "[INFO   ] 2025-05-13@20:23:34: Epoch [2/49], Loss: 1.1133\n",
      "[INFO   ] 2025-05-13@20:23:35: Epoch [23/40], Loss: 0.7693\n",
      "[INFO   ] 2025-05-13@20:23:35: Epoch [24/40], Loss: 0.8370\n",
      "[INFO   ] 2025-05-13@20:23:35: Epoch [6/31], Loss: 0.8133\n",
      "[INFO   ] 2025-05-13@20:23:36: Epoch [18/48], Loss: 0.3598\n",
      "[INFO   ] 2025-05-13@20:23:36: Epoch [25/40], Loss: 0.9412\n",
      "[INFO   ] 2025-05-13@20:23:36: Epoch [26/40], Loss: 0.7692\n",
      "[INFO   ] 2025-05-13@20:23:36: Epoch [27/40], Loss: 0.7816\n",
      "[INFO   ] 2025-05-13@20:23:37: Epoch [7/31], Loss: 0.8133\n",
      "[INFO   ] 2025-05-13@20:23:37: Epoch [28/40], Loss: 0.8028\n",
      "[INFO   ] 2025-05-13@20:23:37: Epoch [19/48], Loss: 0.4698\n",
      "[INFO   ] 2025-05-13@20:23:37: Epoch [29/40], Loss: 0.5943\n",
      "[INFO   ] 2025-05-13@20:23:38: Epoch [30/40], Loss: 0.6985\n",
      "[INFO   ] 2025-05-13@20:23:38: Epoch [8/31], Loss: 1.0633\n",
      "[INFO   ] 2025-05-13@20:23:38: Epoch [3/49], Loss: 1.3133\n",
      "[INFO   ] 2025-05-13@20:23:38: Epoch [31/40], Loss: 0.6983\n",
      "[INFO   ] 2025-05-13@20:23:39: Epoch [32/40], Loss: 0.8184\n",
      "[INFO   ] 2025-05-13@20:23:39: Epoch [20/48], Loss: 0.4271\n",
      "[INFO   ] 2025-05-13@20:23:39: Epoch [33/40], Loss: 0.7823\n",
      "[INFO   ] 2025-05-13@20:23:39: Epoch [9/31], Loss: 0.5633\n",
      "[INFO   ] 2025-05-13@20:23:40: Epoch [34/40], Loss: 0.7895\n",
      "[INFO   ] 2025-05-13@20:23:40: Epoch [35/40], Loss: 0.8847\n",
      "[INFO   ] 2025-05-13@20:23:41: Epoch [36/40], Loss: 0.7895\n",
      "[INFO   ] 2025-05-13@20:23:41: Epoch [21/48], Loss: 0.5332\n",
      "[INFO   ] 2025-05-13@20:23:41: Epoch [10/31], Loss: 0.8133\n",
      "[INFO   ] 2025-05-13@20:23:41: Epoch [37/40], Loss: 0.7895\n",
      "[INFO   ] 2025-05-13@20:23:42: Epoch [38/40], Loss: 0.7895\n",
      "[INFO   ] 2025-05-13@20:23:42: Epoch [4/49], Loss: 0.3133\n",
      "[INFO   ] 2025-05-13@20:23:42: Epoch [39/40], Loss: 0.8847\n",
      "[INFO   ] 2025-05-13@20:23:42: Epoch [11/31], Loss: 0.8133\n",
      "[INFO   ] 2025-05-13@20:23:42: Epoch [22/48], Loss: 0.6162\n",
      "[INFO   ] 2025-05-13@20:23:42: Epoch [40/40], Loss: 0.7418\n",
      "[INFO   ] 2025-05-13@20:23:42: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:23:44: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[I 2025-05-13 17:23:44,104] Trial 16 finished with value: 0.49017349352544876 and parameters: {'window_size': 12, 'batch_size': 42, 'num_epochs': 40, 'lr': 0.009742379059853331}. Best is trial 15 with value: 0.5555453357478496.\n",
      "[INFO   ] 2025-05-13@20:23:44: Epoch [12/31], Loss: 0.5633\n",
      "[INFO   ] 2025-05-13@20:23:45: Epoch [23/48], Loss: 0.3948\n",
      "[INFO   ] 2025-05-13@20:23:45: Epoch [13/31], Loss: 0.8133\n",
      "[INFO   ] 2025-05-13@20:23:46: Epoch [5/49], Loss: 0.9133\n",
      "[INFO   ] 2025-05-13@20:23:46: Epoch [24/48], Loss: 0.3482\n",
      "[INFO   ] 2025-05-13@20:23:47: Epoch [14/31], Loss: 0.8133\n",
      "[INFO   ] 2025-05-13@20:23:47: Epoch [25/48], Loss: 0.3562\n",
      "[INFO   ] 2025-05-13@20:23:48: Epoch [15/31], Loss: 0.8133\n",
      "[INFO   ] 2025-05-13@20:23:48: Epoch [6/49], Loss: 0.5133\n",
      "[INFO   ] 2025-05-13@20:23:49: Epoch [26/48], Loss: 0.4432\n",
      "[INFO   ] 2025-05-13@20:23:49: Epoch [16/31], Loss: 1.0633\n",
      "[INFO   ] 2025-05-13@20:23:50: Epoch [17/31], Loss: 0.8133\n",
      "[INFO   ] 2025-05-13@20:23:50: Epoch [27/48], Loss: 0.5370\n",
      "[INFO   ] 2025-05-13@20:23:51: Epoch [18/31], Loss: 1.0633\n",
      "[INFO   ] 2025-05-13@20:23:51: Epoch [28/48], Loss: 0.5188\n",
      "[INFO   ] 2025-05-13@20:23:51: Epoch [7/49], Loss: 0.5133\n",
      "[INFO   ] 2025-05-13@20:23:52: Epoch [19/31], Loss: 0.5633\n",
      "[INFO   ] 2025-05-13@20:23:52: Epoch [29/48], Loss: 0.5464\n",
      "[INFO   ] 2025-05-13@20:23:53: Epoch [20/31], Loss: 0.5633\n",
      "[INFO   ] 2025-05-13@20:23:54: Epoch [30/48], Loss: 0.4332\n",
      "[INFO   ] 2025-05-13@20:23:54: Epoch [8/49], Loss: 0.7133\n",
      "[INFO   ] 2025-05-13@20:23:54: Epoch [21/31], Loss: 0.8133\n",
      "[INFO   ] 2025-05-13@20:23:55: Epoch [31/48], Loss: 0.5229\n",
      "[INFO   ] 2025-05-13@20:23:55: Epoch [22/31], Loss: 1.0633\n",
      "[INFO   ] 2025-05-13@20:23:56: Epoch [23/31], Loss: 1.0633\n",
      "[INFO   ] 2025-05-13@20:23:56: Epoch [32/48], Loss: 0.5087\n",
      "[INFO   ] 2025-05-13@20:23:57: Epoch [9/49], Loss: 0.5133\n",
      "[INFO   ] 2025-05-13@20:23:57: Epoch [24/31], Loss: 0.5633\n",
      "[INFO   ] 2025-05-13@20:23:58: Epoch [33/48], Loss: 0.3493\n",
      "[INFO   ] 2025-05-13@20:23:58: Epoch [25/31], Loss: 1.0633\n",
      "[INFO   ] 2025-05-13@20:23:59: Epoch [34/48], Loss: 0.6129\n",
      "[INFO   ] 2025-05-13@20:23:59: Epoch [26/31], Loss: 0.8133\n",
      "[INFO   ] 2025-05-13@20:23:59: Epoch [10/49], Loss: 0.9133\n",
      "[INFO   ] 2025-05-13@20:24:00: Epoch [35/48], Loss: 0.6533\n",
      "[INFO   ] 2025-05-13@20:24:00: Epoch [27/31], Loss: 0.5633\n",
      "[INFO   ] 2025-05-13@20:24:02: Epoch [28/31], Loss: 0.8133\n",
      "[INFO   ] 2025-05-13@20:24:02: Epoch [36/48], Loss: 0.5190\n",
      "[INFO   ] 2025-05-13@20:24:02: Epoch [11/49], Loss: 1.1133\n",
      "[INFO   ] 2025-05-13@20:24:03: Epoch [29/31], Loss: 1.0633\n",
      "[INFO   ] 2025-05-13@20:24:03: Epoch [37/48], Loss: 0.4133\n",
      "[INFO   ] 2025-05-13@20:24:04: Epoch [30/31], Loss: 0.8133\n",
      "[INFO   ] 2025-05-13@20:24:04: Epoch [38/48], Loss: 0.3207\n",
      "[INFO   ] 2025-05-13@20:24:05: Epoch [31/31], Loss: 1.0633\n",
      "[INFO   ] 2025-05-13@20:24:05: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:24:08: Epoch [12/49], Loss: 1.1133\n",
      "[INFO   ] 2025-05-13@20:24:08: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[I 2025-05-13 17:24:08,486] Trial 18 finished with value: 0.4705492975744374 and parameters: {'window_size': 42, 'batch_size': 13, 'num_epochs': 31, 'lr': 0.009757331256764168}. Best is trial 15 with value: 0.5555453357478496.\n",
      "[INFO   ] 2025-05-13@20:24:08: Epoch [39/48], Loss: 0.4142\n",
      "[INFO   ] 2025-05-13@20:24:10: Epoch [40/48], Loss: 0.3512\n",
      "[INFO   ] 2025-05-13@20:24:10: Epoch [13/49], Loss: 1.1133\n",
      "[INFO   ] 2025-05-13@20:24:11: Epoch [41/48], Loss: 0.4344\n",
      "[INFO   ] 2025-05-13@20:24:12: Epoch [42/48], Loss: 0.5525\n",
      "[INFO   ] 2025-05-13@20:24:12: Epoch [14/49], Loss: 0.7133\n",
      "[INFO   ] 2025-05-13@20:24:13: Epoch [43/48], Loss: 0.3492\n",
      "[INFO   ] 2025-05-13@20:24:14: Epoch [44/48], Loss: 0.4133\n",
      "[INFO   ] 2025-05-13@20:24:14: Epoch [15/49], Loss: 0.9133\n",
      "[INFO   ] 2025-05-13@20:24:15: Epoch [45/48], Loss: 0.6369\n",
      "[INFO   ] 2025-05-13@20:24:16: Epoch [46/48], Loss: 0.5765\n",
      "[INFO   ] 2025-05-13@20:24:16: Epoch [16/49], Loss: 0.7133\n",
      "[INFO   ] 2025-05-13@20:24:17: Epoch [47/48], Loss: 0.3138\n",
      "[INFO   ] 2025-05-13@20:24:18: Epoch [17/49], Loss: 0.5133\n",
      "[INFO   ] 2025-05-13@20:24:18: Epoch [48/48], Loss: 0.4273\n",
      "[INFO   ] 2025-05-13@20:24:18: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:24:22: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[I 2025-05-13 17:24:22,950] Trial 1 finished with value: 0.20004956981213962 and parameters: {'window_size': 55, 'batch_size': 11, 'num_epochs': 48, 'lr': 0.0017332904994354736}. Best is trial 15 with value: 0.5555453357478496.\n",
      "[INFO   ] 2025-05-13@20:24:24: Epoch [18/49], Loss: 0.3133\n",
      "[INFO   ] 2025-05-13@20:24:25: Epoch [19/49], Loss: 0.9133\n",
      "[INFO   ] 2025-05-13@20:24:26: Epoch [20/49], Loss: 1.1133\n",
      "[INFO   ] 2025-05-13@20:24:28: Epoch [21/49], Loss: 0.7133\n",
      "[INFO   ] 2025-05-13@20:24:29: Epoch [22/49], Loss: 1.1133\n",
      "[INFO   ] 2025-05-13@20:24:30: Epoch [23/49], Loss: 0.9133\n",
      "[INFO   ] 2025-05-13@20:24:32: Epoch [24/49], Loss: 1.1133\n",
      "[INFO   ] 2025-05-13@20:24:33: Epoch [25/49], Loss: 1.1133\n",
      "[INFO   ] 2025-05-13@20:24:34: Epoch [26/49], Loss: 0.9133\n",
      "[INFO   ] 2025-05-13@20:24:36: Epoch [27/49], Loss: 0.9133\n",
      "[INFO   ] 2025-05-13@20:24:37: Epoch [28/49], Loss: 0.9133\n",
      "[INFO   ] 2025-05-13@20:24:38: Epoch [29/49], Loss: 0.9133\n",
      "[INFO   ] 2025-05-13@20:24:40: Epoch [30/49], Loss: 1.1133\n",
      "[INFO   ] 2025-05-13@20:24:41: Epoch [31/49], Loss: 0.5133\n",
      "[INFO   ] 2025-05-13@20:24:43: Epoch [32/49], Loss: 0.9133\n",
      "[INFO   ] 2025-05-13@20:24:44: Epoch [33/49], Loss: 0.5133\n",
      "[INFO   ] 2025-05-13@20:24:45: Epoch [34/49], Loss: 1.1133\n",
      "[INFO   ] 2025-05-13@20:24:47: Epoch [35/49], Loss: 1.1133\n",
      "[INFO   ] 2025-05-13@20:24:48: Epoch [36/49], Loss: 0.9133\n",
      "[INFO   ] 2025-05-13@20:24:49: Epoch [37/49], Loss: 0.5133\n",
      "[INFO   ] 2025-05-13@20:24:51: Epoch [38/49], Loss: 1.1133\n",
      "[INFO   ] 2025-05-13@20:24:52: Epoch [39/49], Loss: 0.9133\n",
      "[INFO   ] 2025-05-13@20:24:53: Epoch [40/49], Loss: 1.3133\n",
      "[INFO   ] 2025-05-13@20:24:55: Epoch [41/49], Loss: 0.9133\n",
      "[INFO   ] 2025-05-13@20:24:56: Epoch [42/49], Loss: 0.7133\n",
      "[INFO   ] 2025-05-13@20:24:57: Epoch [43/49], Loss: 0.7133\n",
      "[INFO   ] 2025-05-13@20:24:59: Epoch [44/49], Loss: 1.1133\n",
      "[INFO   ] 2025-05-13@20:25:00: Epoch [45/49], Loss: 0.7133\n",
      "[INFO   ] 2025-05-13@20:25:02: Epoch [46/49], Loss: 1.1133\n",
      "[INFO   ] 2025-05-13@20:25:03: Epoch [47/49], Loss: 0.9133\n",
      "[INFO   ] 2025-05-13@20:25:04: Epoch [48/49], Loss: 0.7133\n",
      "[INFO   ] 2025-05-13@20:25:06: Epoch [49/49], Loss: 1.1133\n",
      "[INFO   ] 2025-05-13@20:25:06: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:25:09: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[I 2025-05-13 17:25:09,239] Trial 19 finished with value: 0.4946118504777723 and parameters: {'window_size': 41, 'batch_size': 5, 'num_epochs': 49, 'lr': 0.009081316007230295}. Best is trial 15 with value: 0.5555453357478496.\n",
      "[INFO   ] 2025-05-13@20:25:09: Лучшие параметры: {'window_size': 16, 'batch_size': 98, 'num_epochs': 34, 'lr': 0.007362634552544321}\n",
      "[INFO   ] 2025-05-13@20:25:09: Лучший скорректированный ROC AUC Score: 0.5555453357478496\n",
      "[INFO   ] 2025-05-13@20:25:09: Fitting model with best parameters...\n",
      "[INFO   ] 2025-05-13@20:25:09: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:25:10: Initializing model...\n",
      "[INFO   ] 2025-05-13@20:25:10: Training model...\n",
      "[INFO   ] 2025-05-13@20:25:10: Epoch [1/34], Loss: 0.7113\n",
      "[INFO   ] 2025-05-13@20:25:10: Epoch [2/34], Loss: 0.6717\n",
      "[INFO   ] 2025-05-13@20:25:10: Epoch [3/34], Loss: 0.6941\n",
      "[INFO   ] 2025-05-13@20:25:10: Epoch [4/34], Loss: 0.6582\n",
      "[INFO   ] 2025-05-13@20:25:10: Epoch [5/34], Loss: 0.6787\n",
      "[INFO   ] 2025-05-13@20:25:11: Epoch [6/34], Loss: 0.7734\n",
      "[INFO   ] 2025-05-13@20:25:11: Epoch [7/34], Loss: 0.6611\n",
      "[INFO   ] 2025-05-13@20:25:11: Epoch [8/34], Loss: 0.6754\n",
      "[INFO   ] 2025-05-13@20:25:11: Epoch [9/34], Loss: 0.6793\n",
      "[INFO   ] 2025-05-13@20:25:11: Epoch [10/34], Loss: 0.6970\n",
      "[INFO   ] 2025-05-13@20:25:11: Epoch [11/34], Loss: 0.6936\n",
      "[INFO   ] 2025-05-13@20:25:12: Epoch [12/34], Loss: 0.6498\n",
      "[INFO   ] 2025-05-13@20:25:12: Epoch [13/34], Loss: 0.6284\n",
      "[INFO   ] 2025-05-13@20:25:12: Epoch [14/34], Loss: 0.7187\n",
      "[INFO   ] 2025-05-13@20:25:12: Epoch [15/34], Loss: 0.5895\n",
      "[INFO   ] 2025-05-13@20:25:12: Epoch [16/34], Loss: 0.6439\n",
      "[INFO   ] 2025-05-13@20:25:12: Epoch [17/34], Loss: 0.6515\n",
      "[INFO   ] 2025-05-13@20:25:12: Epoch [18/34], Loss: 0.6718\n",
      "[INFO   ] 2025-05-13@20:25:12: Epoch [19/34], Loss: 0.5781\n",
      "[INFO   ] 2025-05-13@20:25:13: Epoch [20/34], Loss: 0.6760\n",
      "[INFO   ] 2025-05-13@20:25:13: Epoch [21/34], Loss: 0.6193\n",
      "[INFO   ] 2025-05-13@20:25:13: Epoch [22/34], Loss: 0.5804\n",
      "[INFO   ] 2025-05-13@20:25:13: Epoch [23/34], Loss: 0.6761\n",
      "[INFO   ] 2025-05-13@20:25:13: Epoch [24/34], Loss: 0.6014\n",
      "[INFO   ] 2025-05-13@20:25:13: Epoch [25/34], Loss: 0.5978\n",
      "[INFO   ] 2025-05-13@20:25:13: Epoch [26/34], Loss: 0.6417\n",
      "[INFO   ] 2025-05-13@20:25:14: Epoch [27/34], Loss: 0.6582\n",
      "[INFO   ] 2025-05-13@20:25:14: Epoch [28/34], Loss: 0.6326\n",
      "[INFO   ] 2025-05-13@20:25:14: Epoch [29/34], Loss: 0.6506\n",
      "[INFO   ] 2025-05-13@20:25:14: Epoch [30/34], Loss: 0.6313\n",
      "[INFO   ] 2025-05-13@20:25:14: Epoch [31/34], Loss: 0.6390\n",
      "[INFO   ] 2025-05-13@20:25:14: Epoch [32/34], Loss: 0.5833\n",
      "[INFO   ] 2025-05-13@20:25:14: Epoch [33/34], Loss: 0.6333\n",
      "[INFO   ] 2025-05-13@20:25:15: Epoch [34/34], Loss: 0.6543\n",
      "[INFO   ] 2025-05-13@20:25:15: Generating predictions...\n",
      "[INFO   ] 2025-05-13@20:25:15: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:25:16: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:25:16: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:25:16: Evaluating ROC AUC...\n",
      "[INFO   ] 2025-05-13@20:25:16: Evaluating metrics by threshold...\n",
      "[INFO   ] 2025-05-13@20:25:16: Running backtesting and collecting all metrics..\n",
      "[INFO   ] 2025-05-13@20:25:16: Converting numpy arrays to TensorDataset and DataLoader...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Метрики для TRAIN выборки ===\n",
      "ROC AUC: 0.7388\n",
      "   Cutoff  Precision     Recall   Accuracy   F1-Score\n",
      "0    50.0  65.763324  81.340782  67.344498  72.727273\n",
      "1    60.0  67.707317  77.541899  68.181818  72.291667\n",
      "2    70.0  69.296375  72.625698  68.122010  70.921986\n",
      "3    80.0  70.616114  66.592179  67.284689  68.545141\n",
      "\n",
      "=== Метрики для TEST выборки ===\n",
      "ROC AUC: 0.3840\n",
      "   Cutoff  Precision     Recall   Accuracy   F1-Score\n",
      "0    50.0  50.000000  87.500000  47.826087  63.636364\n",
      "1    60.0  48.333333  60.416667  45.652174  53.703704\n",
      "2    70.0  47.916667  47.916667  45.652174  47.916667\n",
      "3    80.0  41.379310  25.000000  42.391304  31.168831\n",
      "\n",
      "=== Метрики для VAL выборки ===\n",
      "ROC AUC: 0.5012\n",
      "   Cutoff  Precision     Recall   Accuracy   F1-Score\n",
      "0    50.0  47.826087  76.744186  48.888889  58.928571\n",
      "1    60.0  41.666667  46.511628  43.333333  43.956044\n",
      "2    70.0  50.000000  27.906977  52.222222  35.820896\n",
      "3    80.0  66.666667   9.302326  54.444444  16.326531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO   ] 2025-05-13@20:25:17: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:25:19: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:25:20: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:25:21: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:25:21: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:25:22: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:25:22: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:25:22: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:25:22: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:25:22: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:25:22: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:25:22: Saving results to DB...\n",
      "[INFO   ] 2025-05-13@20:25:23: ML Model Strategy training loop complete!\n"
     ]
    }
   ],
   "source": [
    "result = ml_model_strategy_training_loop(\n",
    "    tickers=tickers,\n",
    "    interval=interval,\n",
    "    train_start=train_start,\n",
    "    train_end=train_end,\n",
    "    test_end=test_end,\n",
    "    valid_end=valid_end,\n",
    "    model_options=model_options,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Experiment_ID</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Interval</th>\n",
       "      <th>Type</th>\n",
       "      <th>START_DT</th>\n",
       "      <th>END_DT</th>\n",
       "      <th>Model</th>\n",
       "      <th>Model_params</th>\n",
       "      <th>Cutoff</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1_Score</th>\n",
       "      <th>ROC_AUC</th>\n",
       "      <th>Return_pct</th>\n",
       "      <th>Win_Rate_pct</th>\n",
       "      <th>Num_Trades</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17307833-3006-11f0-959c-240a64112db6</td>\n",
       "      <td>^GSPC</td>\n",
       "      <td>1d</td>\n",
       "      <td>TRAIN</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>2024-10-01</td>\n",
       "      <td>CNNModel</td>\n",
       "      <td>{\"window_size\": 16, \"batch_size\": 98, \"num_epo...</td>\n",
       "      <td>50.0</td>\n",
       "      <td>65.763324</td>\n",
       "      <td>81.340782</td>\n",
       "      <td>67.344498</td>\n",
       "      <td>72.727273</td>\n",
       "      <td>0.738796</td>\n",
       "      <td>325.101135</td>\n",
       "      <td>70.38835</td>\n",
       "      <td>206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17307833-3006-11f0-959c-240a64112db6</td>\n",
       "      <td>^GSPC</td>\n",
       "      <td>1d</td>\n",
       "      <td>TRAIN</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>2024-10-01</td>\n",
       "      <td>CNNModel</td>\n",
       "      <td>{\"window_size\": 16, \"batch_size\": 98, \"num_epo...</td>\n",
       "      <td>60.0</td>\n",
       "      <td>67.707317</td>\n",
       "      <td>77.541899</td>\n",
       "      <td>68.181818</td>\n",
       "      <td>72.291667</td>\n",
       "      <td>0.738796</td>\n",
       "      <td>486.080849</td>\n",
       "      <td>66.028708</td>\n",
       "      <td>209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17307833-3006-11f0-959c-240a64112db6</td>\n",
       "      <td>^GSPC</td>\n",
       "      <td>1d</td>\n",
       "      <td>TRAIN</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>2024-10-01</td>\n",
       "      <td>CNNModel</td>\n",
       "      <td>{\"window_size\": 16, \"batch_size\": 98, \"num_epo...</td>\n",
       "      <td>70.0</td>\n",
       "      <td>69.296375</td>\n",
       "      <td>72.625698</td>\n",
       "      <td>68.122010</td>\n",
       "      <td>70.921986</td>\n",
       "      <td>0.738796</td>\n",
       "      <td>467.10368</td>\n",
       "      <td>66.509434</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17307833-3006-11f0-959c-240a64112db6</td>\n",
       "      <td>^GSPC</td>\n",
       "      <td>1d</td>\n",
       "      <td>TRAIN</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>2024-10-01</td>\n",
       "      <td>CNNModel</td>\n",
       "      <td>{\"window_size\": 16, \"batch_size\": 98, \"num_epo...</td>\n",
       "      <td>80.0</td>\n",
       "      <td>70.616114</td>\n",
       "      <td>66.592179</td>\n",
       "      <td>67.284689</td>\n",
       "      <td>68.545141</td>\n",
       "      <td>0.738796</td>\n",
       "      <td>498.668633</td>\n",
       "      <td>69.387755</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17307833-3006-11f0-959c-240a64112db6</td>\n",
       "      <td>^GSPC</td>\n",
       "      <td>1d</td>\n",
       "      <td>TEST</td>\n",
       "      <td>2024-10-01</td>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>CNNModel</td>\n",
       "      <td>{\"window_size\": 16, \"batch_size\": 98, \"num_epo...</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>87.500000</td>\n",
       "      <td>47.826087</td>\n",
       "      <td>63.636364</td>\n",
       "      <td>0.383996</td>\n",
       "      <td>1.777532</td>\n",
       "      <td>71.428571</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>17307833-3006-11f0-959c-240a64112db6</td>\n",
       "      <td>^GSPC</td>\n",
       "      <td>1d</td>\n",
       "      <td>TEST</td>\n",
       "      <td>2024-10-01</td>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>CNNModel</td>\n",
       "      <td>{\"window_size\": 16, \"batch_size\": 98, \"num_epo...</td>\n",
       "      <td>60.0</td>\n",
       "      <td>48.333333</td>\n",
       "      <td>60.416667</td>\n",
       "      <td>45.652174</td>\n",
       "      <td>53.703704</td>\n",
       "      <td>0.383996</td>\n",
       "      <td>-4.156214</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>17307833-3006-11f0-959c-240a64112db6</td>\n",
       "      <td>^GSPC</td>\n",
       "      <td>1d</td>\n",
       "      <td>TEST</td>\n",
       "      <td>2024-10-01</td>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>CNNModel</td>\n",
       "      <td>{\"window_size\": 16, \"batch_size\": 98, \"num_epo...</td>\n",
       "      <td>70.0</td>\n",
       "      <td>47.916667</td>\n",
       "      <td>47.916667</td>\n",
       "      <td>45.652174</td>\n",
       "      <td>47.916667</td>\n",
       "      <td>0.383996</td>\n",
       "      <td>-4.256129</td>\n",
       "      <td>43.75</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>17307833-3006-11f0-959c-240a64112db6</td>\n",
       "      <td>^GSPC</td>\n",
       "      <td>1d</td>\n",
       "      <td>TEST</td>\n",
       "      <td>2024-10-01</td>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>CNNModel</td>\n",
       "      <td>{\"window_size\": 16, \"batch_size\": 98, \"num_epo...</td>\n",
       "      <td>80.0</td>\n",
       "      <td>41.379310</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>42.391304</td>\n",
       "      <td>31.168831</td>\n",
       "      <td>0.383996</td>\n",
       "      <td>-11.165178</td>\n",
       "      <td>50.0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>17307833-3006-11f0-959c-240a64112db6</td>\n",
       "      <td>^GSPC</td>\n",
       "      <td>1d</td>\n",
       "      <td>VALID</td>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>CNNModel</td>\n",
       "      <td>{\"window_size\": 16, \"batch_size\": 98, \"num_epo...</td>\n",
       "      <td>50.0</td>\n",
       "      <td>47.826087</td>\n",
       "      <td>76.744186</td>\n",
       "      <td>48.888889</td>\n",
       "      <td>58.928571</td>\n",
       "      <td>0.501237</td>\n",
       "      <td>-6.225919</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>17307833-3006-11f0-959c-240a64112db6</td>\n",
       "      <td>^GSPC</td>\n",
       "      <td>1d</td>\n",
       "      <td>VALID</td>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>CNNModel</td>\n",
       "      <td>{\"window_size\": 16, \"batch_size\": 98, \"num_epo...</td>\n",
       "      <td>60.0</td>\n",
       "      <td>41.666667</td>\n",
       "      <td>46.511628</td>\n",
       "      <td>43.333333</td>\n",
       "      <td>43.956044</td>\n",
       "      <td>0.501237</td>\n",
       "      <td>-8.535883</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>17307833-3006-11f0-959c-240a64112db6</td>\n",
       "      <td>^GSPC</td>\n",
       "      <td>1d</td>\n",
       "      <td>VALID</td>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>CNNModel</td>\n",
       "      <td>{\"window_size\": 16, \"batch_size\": 98, \"num_epo...</td>\n",
       "      <td>70.0</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>27.906977</td>\n",
       "      <td>52.222222</td>\n",
       "      <td>35.820896</td>\n",
       "      <td>0.501237</td>\n",
       "      <td>-2.556942</td>\n",
       "      <td>62.5</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>17307833-3006-11f0-959c-240a64112db6</td>\n",
       "      <td>^GSPC</td>\n",
       "      <td>1d</td>\n",
       "      <td>VALID</td>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>CNNModel</td>\n",
       "      <td>{\"window_size\": 16, \"batch_size\": 98, \"num_epo...</td>\n",
       "      <td>80.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>9.302326</td>\n",
       "      <td>54.444444</td>\n",
       "      <td>16.326531</td>\n",
       "      <td>0.501237</td>\n",
       "      <td>5.695134</td>\n",
       "      <td>75.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Experiment_ID Ticker Interval   Type    START_DT  \\\n",
       "0   17307833-3006-11f0-959c-240a64112db6  ^GSPC       1d  TRAIN  2020-01-01   \n",
       "1   17307833-3006-11f0-959c-240a64112db6  ^GSPC       1d  TRAIN  2020-01-01   \n",
       "2   17307833-3006-11f0-959c-240a64112db6  ^GSPC       1d  TRAIN  2020-01-01   \n",
       "3   17307833-3006-11f0-959c-240a64112db6  ^GSPC       1d  TRAIN  2020-01-01   \n",
       "4   17307833-3006-11f0-959c-240a64112db6  ^GSPC       1d   TEST  2024-10-01   \n",
       "5   17307833-3006-11f0-959c-240a64112db6  ^GSPC       1d   TEST  2024-10-01   \n",
       "6   17307833-3006-11f0-959c-240a64112db6  ^GSPC       1d   TEST  2024-10-01   \n",
       "7   17307833-3006-11f0-959c-240a64112db6  ^GSPC       1d   TEST  2024-10-01   \n",
       "8   17307833-3006-11f0-959c-240a64112db6  ^GSPC       1d  VALID  2025-01-01   \n",
       "9   17307833-3006-11f0-959c-240a64112db6  ^GSPC       1d  VALID  2025-01-01   \n",
       "10  17307833-3006-11f0-959c-240a64112db6  ^GSPC       1d  VALID  2025-01-01   \n",
       "11  17307833-3006-11f0-959c-240a64112db6  ^GSPC       1d  VALID  2025-01-01   \n",
       "\n",
       "        END_DT     Model                                       Model_params  \\\n",
       "0   2024-10-01  CNNModel  {\"window_size\": 16, \"batch_size\": 98, \"num_epo...   \n",
       "1   2024-10-01  CNNModel  {\"window_size\": 16, \"batch_size\": 98, \"num_epo...   \n",
       "2   2024-10-01  CNNModel  {\"window_size\": 16, \"batch_size\": 98, \"num_epo...   \n",
       "3   2024-10-01  CNNModel  {\"window_size\": 16, \"batch_size\": 98, \"num_epo...   \n",
       "4   2025-01-01  CNNModel  {\"window_size\": 16, \"batch_size\": 98, \"num_epo...   \n",
       "5   2025-01-01  CNNModel  {\"window_size\": 16, \"batch_size\": 98, \"num_epo...   \n",
       "6   2025-01-01  CNNModel  {\"window_size\": 16, \"batch_size\": 98, \"num_epo...   \n",
       "7   2025-01-01  CNNModel  {\"window_size\": 16, \"batch_size\": 98, \"num_epo...   \n",
       "8   2025-04-01  CNNModel  {\"window_size\": 16, \"batch_size\": 98, \"num_epo...   \n",
       "9   2025-04-01  CNNModel  {\"window_size\": 16, \"batch_size\": 98, \"num_epo...   \n",
       "10  2025-04-01  CNNModel  {\"window_size\": 16, \"batch_size\": 98, \"num_epo...   \n",
       "11  2025-04-01  CNNModel  {\"window_size\": 16, \"batch_size\": 98, \"num_epo...   \n",
       "\n",
       "    Cutoff  Precision     Recall   Accuracy   F1_Score   ROC_AUC  Return_pct  \\\n",
       "0     50.0  65.763324  81.340782  67.344498  72.727273  0.738796  325.101135   \n",
       "1     60.0  67.707317  77.541899  68.181818  72.291667  0.738796  486.080849   \n",
       "2     70.0  69.296375  72.625698  68.122010  70.921986  0.738796   467.10368   \n",
       "3     80.0  70.616114  66.592179  67.284689  68.545141  0.738796  498.668633   \n",
       "4     50.0  50.000000  87.500000  47.826087  63.636364  0.383996    1.777532   \n",
       "5     60.0  48.333333  60.416667  45.652174  53.703704  0.383996   -4.156214   \n",
       "6     70.0  47.916667  47.916667  45.652174  47.916667  0.383996   -4.256129   \n",
       "7     80.0  41.379310  25.000000  42.391304  31.168831  0.383996  -11.165178   \n",
       "8     50.0  47.826087  76.744186  48.888889  58.928571  0.501237   -6.225919   \n",
       "9     60.0  41.666667  46.511628  43.333333  43.956044  0.501237   -8.535883   \n",
       "10    70.0  50.000000  27.906977  52.222222  35.820896  0.501237   -2.556942   \n",
       "11    80.0  66.666667   9.302326  54.444444  16.326531  0.501237    5.695134   \n",
       "\n",
       "   Win_Rate_pct Num_Trades  \n",
       "0      70.38835        206  \n",
       "1     66.028708        209  \n",
       "2     66.509434        212  \n",
       "3     69.387755        196  \n",
       "4     71.428571          7  \n",
       "5     33.333333         12  \n",
       "6         43.75         16  \n",
       "7          50.0         18  \n",
       "8     66.666667          3  \n",
       "9     66.666667          6  \n",
       "10         62.5         16  \n",
       "11         75.0          8  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# см блокноты \"CNN LSTM GRU\" и практику из 24го урока\n",
    "# возможно добавить Transformer из 25го урока?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hw_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
