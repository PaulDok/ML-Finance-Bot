{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Описание Домашнего Задания"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нейросети – ML - модель\n",
    "\n",
    "<b>Цель:</b>\n",
    "В данном домашнем задании вы потренируетесь в построении модели машинного обучения для формирования вашей торговой стратегии на основе нейросетей.\n",
    "\n",
    "\n",
    "<b>Описание/Пошаговая инструкция выполнения домашнего задания:</b>\n",
    "Уважаемый слушатель!\n",
    "\n",
    "\n",
    "Вы успешно создали полноценную торговую стратегию на основе модели машинного обучения и обеспечили фиксацию и сравнение метрик полученных моделей.\n",
    "\n",
    "\n",
    "Вы решаете построить более сложные модели машинного обучения и хотите использовать нейросетевые модели, в том числе для обработки временных рядов и глубокие нейронные сети. Здесь вам могут помочь методы рекуррентных сетей и современные трансформерные архитектуры.\n",
    "\n",
    "\n",
    "Поговорив с коллегами, вы понимаете, что самостоятельно построить и обучить действительно сложные архитектуры будет сложно и решаете воспользоваться предобученными свободно распространяемыми моделями.\n",
    "\n",
    "\n",
    "На основании вышесказанного вам необходимо построить несколько моделей на основе нейронных сетей, позволяющих прогнозировать оптимальное торговое действие.\n",
    "\n",
    "\n",
    "<b>На основе представленной информации, вам предлагается:</b>\n",
    "\n",
    "\n",
    "1) Создать модель (торговую стратегию) на основе нейронных сетей для прогнозирования оптимального торгового действия. Можно использовать, как самостоятельно обученные архитектуры, так и использовать предобученные сети или фреймворки.\n",
    "2) Провести тестирование разработанной стратегии на валидационном датасете.\n",
    "3) Зафиксировать метрики модели для дальнейшего сравнения экспериментов.\n",
    "4) Сформировать дашборд, показывающий эффективность различных торговых\n",
    "стратегий."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подход к реализации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В рамках ДЗ №4 был подготовлен фреймворк для проведения экспериментов, логгирования их результатов и отображения в UI\n",
    "\n",
    "Следовательно, задача на данном этапе сводится к:\n",
    "- созданию нескольких классов нейросетевых моделей\n",
    "- добавлению их в пул исследуемых (landing.py -> ml_model_strategy_training_loop_callback -> model_options)\n",
    "- проведению тестирования с использованием имеющегося функционала\n",
    "\n",
    "Реализацию проведу с использованием фреймворка pytorch (он мне ближе из альтернатив, а написание нейросети на чистом numpy не рассматриваю т.к. цель задания - не демонстрация понимания низкоуровневой логики)\n",
    "\n",
    "Из архитектур - реализую CNN, LSTM, GRU как наиболее подходящие к домену прогнозирования временных рядов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Создадим класс-обёртку для PyTorch моделей в нашем проекте"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Для облегчения разработки достанем данные в том виде, в котором они используются в training_loop.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\miniconda3\\envs\\hw_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\ProgramData\\miniconda3\\envs\\hw_env\\lib\\site-packages\\backtesting\\_plotting.py:55: UserWarning: Jupyter Notebook detected. Setting Bokeh output to notebook. This may not work in Jupyter clients without JavaScript support, such as old IDEs. Reset with `backtesting.set_bokeh_output(notebook=False)`.\n",
      "  warnings.warn('Jupyter Notebook detected. '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "    <style>\n",
       "        .bk-notebook-logo {\n",
       "            display: block;\n",
       "            width: 20px;\n",
       "            height: 20px;\n",
       "            background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAABx0RVh0U29mdHdhcmUAQWRvYmUgRmlyZXdvcmtzIENTNui8sowAAAOkSURBVDiNjZRtaJVlGMd/1/08zzln5zjP1LWcU9N0NkN8m2CYjpgQYQXqSs0I84OLIC0hkEKoPtiH3gmKoiJDU7QpLgoLjLIQCpEsNJ1vqUOdO7ppbuec5+V+rj4ctwzd8IIbbi6u+8f1539dt3A78eXC7QizUF7gyV1fD1Yqg4JWz84yffhm0qkFqBogB9rM8tZdtwVsPUhWhGcFJngGeWrPzHm5oaMmkfEg1usvLFyc8jLRqDOMru7AyC8saQr7GG7f5fvDeH7Ej8CM66nIF+8yngt6HWaKh7k49Soy9nXurCi1o3qUbS3zWfrYeQDTB/Qj6kX6Ybhw4B+bOYoLKCC9H3Nu/leUTZ1JdRWkkn2ldcCamzrcf47KKXdAJllSlxAOkRgyHsGC/zRday5Qld9DyoM4/q/rUoy/CXh3jzOu3bHUVZeU+DEn8FInkPBFlu3+nW3Nw0mk6vCDiWg8CeJaxEwuHS3+z5RgY+YBR6V1Z1nxSOfoaPa4LASWxxdNp+VWTk7+4vzaou8v8PN+xo+KY2xsw6une2frhw05CTYOmQvsEhjhWjn0bmXPjpE1+kplmmkP3suftwTubK9Vq22qKmrBhpY4jvd5afdRA3wGjFAgcnTK2s4hY0/GPNIb0nErGMCRxWOOX64Z8RAC4oCXdklmEvcL8o0BfkNK4lUg9HTl+oPlQxdNo3Mg4Nv175e/1LDGzZen30MEjRUtmXSfiTVu1kK8W4txyV6BMKlbgk3lMwYCiusNy9fVfvvwMxv8Ynl6vxoByANLTWplvuj/nF9m2+PDtt1eiHPBr1oIfhCChQMBw6Aw0UulqTKZdfVvfG7VcfIqLG9bcldL/+pdWTLxLUy8Qq38heUIjh4XlzZxzQm19lLFlr8vdQ97rjZVOLf8nclzckbcD4wxXMidpX30sFd37Fv/GtwwhzhxGVAprjbg0gCAEeIgwCZyTV2Z1REEW8O4py0wsjeloKoMr6iCY6dP92H6Vw/oTyICIthibxjm/DfN9lVz8IqtqKYLUXfoKVMVQVVJOElGjrnnUt9T9wbgp8AyYKaGlqingHZU/uG2NTZSVqwHQTWkx9hxjkpWDaCg6Ckj5qebgBVbT3V3NNXMSiWSDdGV3hrtzla7J+duwPOToIg42ChPQOQjspnSlp1V+Gjdged7+8UN5CRAV7a5EdFNwCjEaBR27b3W890TE7g24NAP/mMDXRWrGoFPQI9ls/MWO2dWFAar/xcOIImbbpA3zgAAAABJRU5ErkJggg==);\n",
       "        }\n",
       "    </style>\n",
       "    <div>\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-notebook-logo\"></a>\n",
       "        <span id=\"aa102799-d3d0-4033-91ab-327aa1f9f44d\">Loading BokehJS ...</span>\n",
       "    </div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "'use strict';\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\nconst JS_MIME_TYPE = 'application/javascript';\n  const HTML_MIME_TYPE = 'text/html';\n  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n  const CLASS_NAME = 'output_bokeh rendered_html';\n\n  /**\n   * Render data to the DOM node\n   */\n  function render(props, node) {\n    const script = document.createElement(\"script\");\n    node.appendChild(script);\n  }\n\n  /**\n   * Handle when an output is cleared or removed\n   */\n  function handleClearOutput(event, handle) {\n    function drop(id) {\n      const view = Bokeh.index.get_by_id(id)\n      if (view != null) {\n        view.model.document.clear()\n        Bokeh.index.delete(view)\n      }\n    }\n\n    const cell = handle.cell;\n\n    const id = cell.output_area._bokeh_element_id;\n    const server_id = cell.output_area._bokeh_server_id;\n\n    // Clean up Bokeh references\n    if (id != null) {\n      drop(id)\n    }\n\n    if (server_id !== undefined) {\n      // Clean up Bokeh references\n      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n      cell.notebook.kernel.execute(cmd_clean, {\n        iopub: {\n          output: function(msg) {\n            const id = msg.content.text.trim()\n            drop(id)\n          }\n        }\n      });\n      // Destroy server and session\n      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n      cell.notebook.kernel.execute(cmd_destroy);\n    }\n  }\n\n  /**\n   * Handle when a new output is added\n   */\n  function handleAddOutput(event, handle) {\n    const output_area = handle.output_area;\n    const output = handle.output;\n\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n      return\n    }\n\n    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n      // store reference to embed id on output_area\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n    }\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n      const bk_div = document.createElement(\"div\");\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n      const script_attrs = bk_div.children[0].attributes;\n      for (let i = 0; i < script_attrs.length; i++) {\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n      }\n      // store reference to server id on output_area\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n    }\n  }\n\n  function register_renderer(events, OutputArea) {\n\n    function append_mime(data, metadata, element) {\n      // create a DOM node to render to\n      const toinsert = this.create_output_subarea(\n        metadata,\n        CLASS_NAME,\n        EXEC_MIME_TYPE\n      );\n      this.keyboard_manager.register_events(toinsert);\n      // Render to node\n      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n      render(props, toinsert[toinsert.length - 1]);\n      element.append(toinsert);\n      return toinsert\n    }\n\n    /* Handle when an output is cleared or removed */\n    events.on('clear_output.CodeCell', handleClearOutput);\n    events.on('delete.Cell', handleClearOutput);\n\n    /* Handle when a new output is added */\n    events.on('output_added.OutputArea', handleAddOutput);\n\n    /**\n     * Register the mime type and append_mime function with output_area\n     */\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n      /* Is output safe? */\n      safe: true,\n      /* Index of renderer in `output_area.display_order` */\n      index: 0\n    });\n  }\n\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\n  if (root.Jupyter !== undefined) {\n    const events = require('base/js/events');\n    const OutputArea = require('notebook/js/outputarea').OutputArea;\n\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  }\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded(error = null) {\n    const el = document.getElementById(\"aa102799-d3d0-4033-91ab-327aa1f9f44d\");\n    if (el != null) {\n      const html = (() => {\n        if (typeof root.Bokeh === \"undefined\") {\n          if (error == null) {\n            return \"BokehJS is loading ...\";\n          } else {\n            return \"BokehJS failed to load.\";\n          }\n        } else {\n          const prefix = `BokehJS ${root.Bokeh.version}`;\n          if (error == null) {\n            return `${prefix} successfully loaded.`;\n          } else {\n            return `${prefix} <b>encountered errors</b> while loading and may not function as expected.`;\n          }\n        }\n      })();\n      el.innerHTML = html;\n\n      if (error != null) {\n        const wrapper = document.createElement(\"div\");\n        wrapper.style.overflow = \"auto\";\n        wrapper.style.height = \"5em\";\n        wrapper.style.resize = \"vertical\";\n        const content = document.createElement(\"div\");\n        content.style.fontFamily = \"monospace\";\n        content.style.whiteSpace = \"pre-wrap\";\n        content.style.backgroundColor = \"rgb(255, 221, 221)\";\n        content.textContent = error.stack ?? error.toString();\n        wrapper.append(content);\n        el.append(wrapper);\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(() => display_loaded(error), 100);\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.7.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.7.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.7.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.7.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.7.2.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n      try {\n            for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n\n      } catch (error) {display_loaded(error);throw error;\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"aa102799-d3d0-4033-91ab-327aa1f9f44d\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));",
      "application/vnd.bokehjs_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Относительные ссылки, включая импорты, относительно корневой папки проекта\n",
    "import os\n",
    "\n",
    "os.chdir(os.path.dirname(os.getcwd()))\n",
    "\n",
    "import main\n",
    "import logging\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from src.core import utils\n",
    "\n",
    "# initialize\n",
    "logger = logging.getLogger()\n",
    "# initialize config dict\n",
    "config = main.main_launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params for testing\n",
    "tickers = [\"^GSPC\"]\n",
    "interval = config.INTERVAL\n",
    "\n",
    "# Params for train-test-valid split\n",
    "# TEST - Q4'24 | VAL - Q1'25\n",
    "train_start = \"2020-01-01\"\n",
    "train_end = \"2024-10-01\"\n",
    "test_end = \"2025-01-01\"\n",
    "valid_end = \"2025-04-01\"  # захватим первый квартал 2025, тестовая выборка по длине такая же как валидационная - 3мес\n",
    "\n",
    "# Params inside Optimizer\n",
    "early_stopping_rounds = 50\n",
    "n_trials = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Список моделей и вариантов их гиперпараметров для тестирования\n",
    "# TODO: ML модели появятся здесь\n",
    "model_options = {\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO   ] 2025-05-13@19:38:55: Getting preprocessed history from local cache DB...\n",
      "[INFO   ] 2025-05-13@19:38:57: Got history of shape (1854, 8), 0 NaNs\n",
      "[INFO   ] 2025-05-13@19:38:58: Parsed features from JSON to separate columns: (1854, 302), 0 NaNs\n",
      "[INFO   ] 2025-05-13@19:38:58: Adding binary target...\n",
      "[INFO   ] 2025-05-13@19:38:58: Target added: (1854, 303), 0 NaNs\n",
      "[INFO   ] 2025-05-13@19:38:58: ~ ~ ~ Modelling for ^GSPC ~ ~ ~\n",
      "[INFO   ] 2025-05-13@19:38:58: Splitting ticker data to train/test/validation parts\n",
      "[INFO   ] 2025-05-13@19:38:58: X_train.shape=(1672, 296) | y_train.shape=(1672,) || X_test.shape=(92, 296) | y_test.shape=(92,) || X_val.shape=(90, 296) | y_val.shape=(90,)\n",
      "[INFO   ] 2025-05-13@19:38:58: Scaling features...\n"
     ]
    }
   ],
   "source": [
    "# Часть логики training_loop - получим в блокноте те же переменные что и при инициализации optimizer.ModelOptimizer\n",
    "# Get preprocessed data\n",
    "data, features = utils.get_preprocessed_history(\n",
    "    tickers=tickers, start=train_start, end=valid_end, interval=interval\n",
    ")\n",
    "\n",
    "# Add feature column\n",
    "data = utils.add_target(data)\n",
    "\n",
    "# Loop over tickers in dataset\n",
    "grand_result = []\n",
    "for ticker in data[\"Ticker\"].unique():\n",
    "    logger.info(f\"~ ~ ~ Modelling for {ticker} ~ ~ ~\")\n",
    "    ticker_data = data[data[\"Ticker\"] == ticker].reset_index(drop=True)\n",
    "\n",
    "    # Split dataset into parts\n",
    "    X_train, y_train, X_test, y_test, X_val, y_val = utils.train_test_valid_split(\n",
    "        ticker_data,\n",
    "        train_start=train_start,\n",
    "        train_end=train_end,\n",
    "        test_end=test_end,\n",
    "        valid_end=valid_end,\n",
    "        drop_leaky=True,\n",
    "        target_col=\"target\",\n",
    "    )\n",
    "    logger.info(\n",
    "        f\"{X_train.shape=} | {y_train.shape=} || {X_test.shape=} | {y_test.shape=} || {X_val.shape=} | {y_val.shape=}\"\n",
    "    )\n",
    "\n",
    "    # Scale train / test / validation datasets - fit on train\n",
    "    logger.info(\"Scaling features...\")\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train[features])\n",
    "    X_test_scaled = scaler.transform(X_test[features])\n",
    "    X_val_scaled = scaler.transform(X_val[features])\n",
    "\n",
    "    break\n",
    "\n",
    "    # # Для каждого потенциального типа модели:\n",
    "    # for model_type, param_dict in model_options.items():\n",
    "    #     logger.info(f\"~ ~ Iteration for {model_type.__name__} ~ ~\")\n",
    "    #     # Тюним гиперпараметры на train / test датасетах, выбираем лучшее\n",
    "    #     model_optimizer = optimizer.ModelOptimizer(\n",
    "    #         model_type,\n",
    "    #         param_dict,\n",
    "    #         X_train_scaled,\n",
    "    #         y_train,\n",
    "    #         X_test_scaled,\n",
    "    #         y_test,\n",
    "    #         X_val_scaled,\n",
    "    #         y_val,\n",
    "    #     )\n",
    "    #     (\n",
    "    #         model,\n",
    "    #         best_params,\n",
    "    #         (train_roc_auc, test_roc_auc, val_roc_auc),\n",
    "    #         (train_metrics_table, test_metrics_table, val_metrics_table),\n",
    "    #     ) = model_optimizer.optimize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Заготовки внутренних функций модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Потенциальные параметры для тюнинга\n",
    "window_size = 30\n",
    "batch_size = 32\n",
    "lr = 0.001\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_dataloader(X_raw, y_raw=None):\n",
    "    \"\"\"\n",
    "    Adapter to be used inside NN models\n",
    "    \"\"\"\n",
    "    logger.info(\"Converting numpy arrays to TensorDataset and DataLoader...\")\n",
    "    # Convert to PyTorch Tensors\n",
    "    X, y = [], []\n",
    "    for i in range(len(X_raw) - window_size - 1):\n",
    "        X.append(X_raw[i : i + window_size])\n",
    "        if y_raw is not None:\n",
    "            y.append(y_raw[i + window_size])\n",
    "        else:\n",
    "            y.append(0)\n",
    "\n",
    "    # Now convert lists to tensors\n",
    "    X = torch.tensor(X, dtype=torch.float32)\n",
    "    y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    # DataLoader\n",
    "    dataset = TensorDataset(X, y)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader):\n",
    "    \"\"\"\n",
    "    NN model training loop\n",
    "    \"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Switch model to train mode\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for inputs, labels in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolution Neural Network\n",
    "    \"\"\"\n",
    "    def __init__(self, n_features, kernel_size_conv1=3, kernel_size_conv2=3):\n",
    "        super(StockCNN, self).__init__()\n",
    "        self.cnn_stack = nn.Sequential(\n",
    "            nn.Conv1d(n_features, 16, kernel_size=kernel_size_conv1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(16, 32, kernel_size=kernel_size_conv2),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(), # flatten after convolutions\n",
    "            nn.Linear(\n",
    "                32 * (window_size - (kernel_size_conv1 - 1) - (kernel_size_conv2 - 1)), 64\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 2),\n",
    "            nn.Softmax(1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)  # Permute to (batch_size, channels, sequence_length)\n",
    "        probabilities = self.cnn_stack(x)\n",
    "        return probabilities\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализуем логику функций fit() и predict_proba() данных моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) - to be in model's .fit() method\n",
    "def fit(X_train, y_train):\n",
    "    # Convert data\n",
    "    dataloader = convert_to_dataloader(X_raw=X_train, y_raw=y_train)\n",
    "    # Initialize model\n",
    "    logger.info(\"Initializing model...\")\n",
    "    model = StockCNN(n_features=X_train.shape[1])\n",
    "    # Train it\n",
    "    logger.info(\"Training model...\")\n",
    "    model = train_model(model, dataloader)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2) - to be in predict_proba() method\n",
    "def predict_proba(model, X_raw):\n",
    "    dataloader = convert_to_dataloader(X_raw)\n",
    "\n",
    "    _ = model.eval()\n",
    "    predictions = [np.array([0.5, 0.5], dtype=\"float32\")] * (window_size + 1) # first ticks have no real prediction\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in dataloader:\n",
    "            logits = model(inputs)\n",
    "            predictions.extend(logits.numpy())\n",
    "\n",
    "    # Convert to a single np.array\n",
    "    predictions = np.stack(predictions, axis=0)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для референса - в model_optimizer происходит следующее:\n",
    "# = = = = = =\n",
    "# # Инициализация модели с заданными параметрами\n",
    "# model = self.model_type(**suggested_param)\n",
    "\n",
    "# # Обучение модели с валидационной выборкой\n",
    "# if self.param_dict.get(\"use_eval_set\", None):\n",
    "#     _ = model.fit(\n",
    "#         self.X_train_scaled,\n",
    "#         self.y_train,\n",
    "#         eval_set=(self.X_test_scaled, self.y_test),\n",
    "#     )\n",
    "# else:\n",
    "#     _ = model.fit(\n",
    "#         self.X_train_scaled,\n",
    "#         self.y_train,\n",
    "#     )\n",
    "\n",
    "# # Предсказания на тренировочной и тестовой выборках\n",
    "# y_train_pred_prob = model.predict_proba(self.X_train_scaled)[:, 1]\n",
    "# y_test_pred_prob = model.predict_proba(self.X_test_scaled)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO   ] 2025-05-13@19:39:21: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "C:\\Users\\Pavel\\AppData\\Local\\Temp\\ipykernel_16108\\3998794152.py:16: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:257.)\n",
      "  X = torch.tensor(X, dtype=torch.float32)\n",
      "[INFO   ] 2025-05-13@19:39:23: Initializing model...\n",
      "[INFO   ] 2025-05-13@19:39:23: Training model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.6659\n",
      "Epoch [2/10], Loss: 0.6443\n",
      "Epoch [3/10], Loss: 0.6821\n",
      "Epoch [4/10], Loss: 0.5328\n",
      "Epoch [5/10], Loss: 0.6582\n",
      "Epoch [6/10], Loss: 0.4607\n",
      "Epoch [7/10], Loss: 0.7626\n",
      "Epoch [8/10], Loss: 0.5211\n",
      "Epoch [9/10], Loss: 0.6265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO   ] 2025-05-13@19:39:29: Converting numpy arrays to TensorDataset and DataLoader...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Loss: 0.4940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO   ] 2025-05-13@19:39:31: Converting numpy arrays to TensorDataset and DataLoader...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_roc_auc=np.float64(0.5238771093519696) | test_roc_auc=np.float64(0.5639204545454546)\n"
     ]
    }
   ],
   "source": [
    "# То что получается у нас\n",
    "# fit\n",
    "model = fit(X_train=X_train_scaled, y_train=y_train)\n",
    "# predict\n",
    "y_train_pred_prob = predict_proba(model, X_train_scaled)[:, 1]\n",
    "y_test_pred_prob = predict_proba(model, X_test_scaled)[:, 1]\n",
    "# log metric\n",
    "train_roc_auc = roc_auc_score(y_train, y_train_pred_prob)\n",
    "test_roc_auc = roc_auc_score(y_test, y_test_pred_prob)\n",
    "\n",
    "print(f\"{train_roc_auc=} | {test_roc_auc=}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Получив работоспособный прототип, объединяем всё в ./app/src/models/cnn_model.py и протестируем цикл подбора гиперпараметров\n",
    "\n",
    "(запускать ячейки ниже после перезапуска ядра)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\miniconda3\\envs\\hw_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\ProgramData\\miniconda3\\envs\\hw_env\\lib\\site-packages\\backtesting\\_plotting.py:55: UserWarning: Jupyter Notebook detected. Setting Bokeh output to notebook. This may not work in Jupyter clients without JavaScript support, such as old IDEs. Reset with `backtesting.set_bokeh_output(notebook=False)`.\n",
      "  warnings.warn('Jupyter Notebook detected. '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "    <style>\n",
       "        .bk-notebook-logo {\n",
       "            display: block;\n",
       "            width: 20px;\n",
       "            height: 20px;\n",
       "            background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAABx0RVh0U29mdHdhcmUAQWRvYmUgRmlyZXdvcmtzIENTNui8sowAAAOkSURBVDiNjZRtaJVlGMd/1/08zzln5zjP1LWcU9N0NkN8m2CYjpgQYQXqSs0I84OLIC0hkEKoPtiH3gmKoiJDU7QpLgoLjLIQCpEsNJ1vqUOdO7ppbuec5+V+rj4ctwzd8IIbbi6u+8f1539dt3A78eXC7QizUF7gyV1fD1Yqg4JWz84yffhm0qkFqBogB9rM8tZdtwVsPUhWhGcFJngGeWrPzHm5oaMmkfEg1usvLFyc8jLRqDOMru7AyC8saQr7GG7f5fvDeH7Ej8CM66nIF+8yngt6HWaKh7k49Soy9nXurCi1o3qUbS3zWfrYeQDTB/Qj6kX6Ybhw4B+bOYoLKCC9H3Nu/leUTZ1JdRWkkn2ldcCamzrcf47KKXdAJllSlxAOkRgyHsGC/zRday5Qld9DyoM4/q/rUoy/CXh3jzOu3bHUVZeU+DEn8FInkPBFlu3+nW3Nw0mk6vCDiWg8CeJaxEwuHS3+z5RgY+YBR6V1Z1nxSOfoaPa4LASWxxdNp+VWTk7+4vzaou8v8PN+xo+KY2xsw6une2frhw05CTYOmQvsEhjhWjn0bmXPjpE1+kplmmkP3suftwTubK9Vq22qKmrBhpY4jvd5afdRA3wGjFAgcnTK2s4hY0/GPNIb0nErGMCRxWOOX64Z8RAC4oCXdklmEvcL8o0BfkNK4lUg9HTl+oPlQxdNo3Mg4Nv175e/1LDGzZen30MEjRUtmXSfiTVu1kK8W4txyV6BMKlbgk3lMwYCiusNy9fVfvvwMxv8Ynl6vxoByANLTWplvuj/nF9m2+PDtt1eiHPBr1oIfhCChQMBw6Aw0UulqTKZdfVvfG7VcfIqLG9bcldL/+pdWTLxLUy8Qq38heUIjh4XlzZxzQm19lLFlr8vdQ97rjZVOLf8nclzckbcD4wxXMidpX30sFd37Fv/GtwwhzhxGVAprjbg0gCAEeIgwCZyTV2Z1REEW8O4py0wsjeloKoMr6iCY6dP92H6Vw/oTyICIthibxjm/DfN9lVz8IqtqKYLUXfoKVMVQVVJOElGjrnnUt9T9wbgp8AyYKaGlqingHZU/uG2NTZSVqwHQTWkx9hxjkpWDaCg6Ckj5qebgBVbT3V3NNXMSiWSDdGV3hrtzla7J+duwPOToIg42ChPQOQjspnSlp1V+Gjdged7+8UN5CRAV7a5EdFNwCjEaBR27b3W890TE7g24NAP/mMDXRWrGoFPQI9ls/MWO2dWFAar/xcOIImbbpA3zgAAAABJRU5ErkJggg==);\n",
       "        }\n",
       "    </style>\n",
       "    <div>\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-notebook-logo\"></a>\n",
       "        <span id=\"fff98b1c-ffdd-4e10-a42c-e7be324b3776\">Loading BokehJS ...</span>\n",
       "    </div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "'use strict';\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\nconst JS_MIME_TYPE = 'application/javascript';\n  const HTML_MIME_TYPE = 'text/html';\n  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n  const CLASS_NAME = 'output_bokeh rendered_html';\n\n  /**\n   * Render data to the DOM node\n   */\n  function render(props, node) {\n    const script = document.createElement(\"script\");\n    node.appendChild(script);\n  }\n\n  /**\n   * Handle when an output is cleared or removed\n   */\n  function handleClearOutput(event, handle) {\n    function drop(id) {\n      const view = Bokeh.index.get_by_id(id)\n      if (view != null) {\n        view.model.document.clear()\n        Bokeh.index.delete(view)\n      }\n    }\n\n    const cell = handle.cell;\n\n    const id = cell.output_area._bokeh_element_id;\n    const server_id = cell.output_area._bokeh_server_id;\n\n    // Clean up Bokeh references\n    if (id != null) {\n      drop(id)\n    }\n\n    if (server_id !== undefined) {\n      // Clean up Bokeh references\n      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n      cell.notebook.kernel.execute(cmd_clean, {\n        iopub: {\n          output: function(msg) {\n            const id = msg.content.text.trim()\n            drop(id)\n          }\n        }\n      });\n      // Destroy server and session\n      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n      cell.notebook.kernel.execute(cmd_destroy);\n    }\n  }\n\n  /**\n   * Handle when a new output is added\n   */\n  function handleAddOutput(event, handle) {\n    const output_area = handle.output_area;\n    const output = handle.output;\n\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n      return\n    }\n\n    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n      // store reference to embed id on output_area\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n    }\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n      const bk_div = document.createElement(\"div\");\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n      const script_attrs = bk_div.children[0].attributes;\n      for (let i = 0; i < script_attrs.length; i++) {\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n      }\n      // store reference to server id on output_area\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n    }\n  }\n\n  function register_renderer(events, OutputArea) {\n\n    function append_mime(data, metadata, element) {\n      // create a DOM node to render to\n      const toinsert = this.create_output_subarea(\n        metadata,\n        CLASS_NAME,\n        EXEC_MIME_TYPE\n      );\n      this.keyboard_manager.register_events(toinsert);\n      // Render to node\n      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n      render(props, toinsert[toinsert.length - 1]);\n      element.append(toinsert);\n      return toinsert\n    }\n\n    /* Handle when an output is cleared or removed */\n    events.on('clear_output.CodeCell', handleClearOutput);\n    events.on('delete.Cell', handleClearOutput);\n\n    /* Handle when a new output is added */\n    events.on('output_added.OutputArea', handleAddOutput);\n\n    /**\n     * Register the mime type and append_mime function with output_area\n     */\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n      /* Is output safe? */\n      safe: true,\n      /* Index of renderer in `output_area.display_order` */\n      index: 0\n    });\n  }\n\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\n  if (root.Jupyter !== undefined) {\n    const events = require('base/js/events');\n    const OutputArea = require('notebook/js/outputarea').OutputArea;\n\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  }\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded(error = null) {\n    const el = document.getElementById(\"fff98b1c-ffdd-4e10-a42c-e7be324b3776\");\n    if (el != null) {\n      const html = (() => {\n        if (typeof root.Bokeh === \"undefined\") {\n          if (error == null) {\n            return \"BokehJS is loading ...\";\n          } else {\n            return \"BokehJS failed to load.\";\n          }\n        } else {\n          const prefix = `BokehJS ${root.Bokeh.version}`;\n          if (error == null) {\n            return `${prefix} successfully loaded.`;\n          } else {\n            return `${prefix} <b>encountered errors</b> while loading and may not function as expected.`;\n          }\n        }\n      })();\n      el.innerHTML = html;\n\n      if (error != null) {\n        const wrapper = document.createElement(\"div\");\n        wrapper.style.overflow = \"auto\";\n        wrapper.style.height = \"5em\";\n        wrapper.style.resize = \"vertical\";\n        const content = document.createElement(\"div\");\n        content.style.fontFamily = \"monospace\";\n        content.style.whiteSpace = \"pre-wrap\";\n        content.style.backgroundColor = \"rgb(255, 221, 221)\";\n        content.textContent = error.stack ?? error.toString();\n        wrapper.append(content);\n        el.append(wrapper);\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(() => display_loaded(error), 100);\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.7.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.7.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.7.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.7.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.7.2.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n      try {\n            for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n\n      } catch (error) {display_loaded(error);throw error;\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"fff98b1c-ffdd-4e10-a42c-e7be324b3776\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));",
      "application/vnd.bokehjs_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Относительные ссылки, включая импорты, относительно корневой папки проекта\n",
    "import os\n",
    "\n",
    "os.chdir(os.path.dirname(os.getcwd()))\n",
    "\n",
    "import main\n",
    "import logging\n",
    "\n",
    "from src.models.training_loop import ml_model_strategy_training_loop\n",
    "from src.models.cnn_model import CNNModel\n",
    "\n",
    "# initialize\n",
    "logger = logging.getLogger()\n",
    "# initialize config dict\n",
    "config = main.main_launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params for testing\n",
    "tickers = [\"^GSPC\"]\n",
    "interval = config.INTERVAL\n",
    "\n",
    "# Params for train-test-valid split\n",
    "# TEST - Q4'24 | VAL - Q1'25\n",
    "train_start = \"2020-01-01\"\n",
    "train_end = \"2024-10-01\"\n",
    "test_end = \"2025-01-01\"\n",
    "valid_end = \"2025-04-01\"  # захватим первый квартал 2025, тестовая выборка по длине такая же как валидационная - 3мес"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Потенциальные параметры для тюнинга\n",
    "# window_size = 30\n",
    "# batch_size = 32\n",
    "# lr = 0.001\n",
    "# num_epochs = 10\n",
    "model_options = {\n",
    "    CNNModel: {\n",
    "        \"int\": {\n",
    "            \"window_size\": {\"low\": 5, \"high\": 60},\n",
    "            \"batch_size\": {\"low\": 4, \"high\": 128},\n",
    "            \"num_epochs\": {\"low\": 5, \"high\": 50}\n",
    "        },\n",
    "        \"float\": {\n",
    "            \"lr\": {\"low\": 0.0005, \"high\": 0.01}\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO   ] 2025-05-13@20:20:59: Getting preprocessed history from local cache DB...\n",
      "[INFO   ] 2025-05-13@20:21:01: Got history of shape (1854, 8), 0 NaNs\n",
      "[INFO   ] 2025-05-13@20:21:02: Parsed features from JSON to separate columns: (1854, 302), 0 NaNs\n",
      "[INFO   ] 2025-05-13@20:21:02: Adding binary target...\n",
      "[INFO   ] 2025-05-13@20:21:02: Target added: (1854, 303), 0 NaNs\n",
      "[INFO   ] 2025-05-13@20:21:02: ~ ~ ~ Modelling for ^GSPC ~ ~ ~\n",
      "[INFO   ] 2025-05-13@20:21:02: Splitting ticker data to train/test/validation parts\n",
      "[INFO   ] 2025-05-13@20:21:02: X_train.shape=(1672, 296) | y_train.shape=(1672,) || X_test.shape=(92, 296) | y_test.shape=(92,) || X_val.shape=(90, 296) | y_val.shape=(90,)\n",
      "[INFO   ] 2025-05-13@20:21:02: Scaling features...\n",
      "[INFO   ] 2025-05-13@20:21:02: ~ ~ Iteration for CNNModel ~ ~\n",
      "[INFO   ] 2025-05-13@20:21:02: Searching for best hyperparameters using Optuna...\n",
      "[I 2025-05-13 17:21:02,733] A new study created in memory with name: no-name-238bfaa2-4e26-497c-a786-5fdc7bfdfb0f\n",
      "[INFO   ] 2025-05-13@20:21:02: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "d:\\Otus\\ML-Finance-Bot\\app\\src\\models\\cnn_model.py:42: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:257.)\n",
      "  X = torch.tensor(X, dtype=torch.float32)\n",
      "[INFO   ] 2025-05-13@20:21:04: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:21:04: Initializing model...\n",
      "[INFO   ] 2025-05-13@20:21:04: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:21:08: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:21:08: Initializing model...\n",
      "[INFO   ] 2025-05-13@20:21:08: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:21:08: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:21:08: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:21:08: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:21:11: Initializing model...\n",
      "[INFO   ] 2025-05-13@20:21:13: Training model...\n",
      "[INFO   ] 2025-05-13@20:21:13: Initializing model...\n",
      "[INFO   ] 2025-05-13@20:21:14: Initializing model...\n",
      "[INFO   ] 2025-05-13@20:21:17: Initializing model...\n",
      "[INFO   ] 2025-05-13@20:21:17: Training model...\n",
      "[INFO   ] 2025-05-13@20:21:20: Initializing model...\n",
      "[INFO   ] 2025-05-13@20:21:22: Initializing model...\n",
      "[INFO   ] 2025-05-13@20:21:22: Training model...\n",
      "[INFO   ] 2025-05-13@20:21:22: Training model...\n",
      "[INFO   ] 2025-05-13@20:21:22: Training model...\n",
      "[INFO   ] 2025-05-13@20:21:22: Training model...\n",
      "[INFO   ] 2025-05-13@20:21:22: Training model...\n",
      "[INFO   ] 2025-05-13@20:21:22: Training model...\n",
      "[INFO   ] 2025-05-13@20:21:25: Epoch [1/31], Loss: 0.6621\n",
      "[INFO   ] 2025-05-13@20:21:25: Epoch [1/42], Loss: 0.7007\n",
      "[INFO   ] 2025-05-13@20:21:26: Epoch [1/12], Loss: 0.9100\n",
      "[INFO   ] 2025-05-13@20:21:26: Epoch [1/13], Loss: 0.6815\n",
      "[INFO   ] 2025-05-13@20:21:26: Epoch [1/41], Loss: 0.6740\n",
      "[INFO   ] 2025-05-13@20:21:26: Epoch [2/31], Loss: 0.6673\n",
      "[INFO   ] 2025-05-13@20:21:26: Epoch [1/38], Loss: 0.6991\n",
      "[INFO   ] 2025-05-13@20:21:26: Epoch [2/42], Loss: 0.6803\n",
      "[INFO   ] 2025-05-13@20:21:26: Epoch [1/36], Loss: 0.6855\n",
      "[INFO   ] 2025-05-13@20:21:26: Epoch [2/12], Loss: 0.8294\n",
      "[INFO   ] 2025-05-13@20:21:27: Epoch [2/13], Loss: 0.6813\n",
      "[INFO   ] 2025-05-13@20:21:27: Epoch [3/31], Loss: 0.6377\n",
      "[INFO   ] 2025-05-13@20:21:27: Epoch [2/41], Loss: 0.6773\n",
      "[INFO   ] 2025-05-13@20:21:27: Epoch [3/42], Loss: 0.6643\n",
      "[INFO   ] 2025-05-13@20:21:27: Epoch [3/12], Loss: 0.7649\n",
      "[INFO   ] 2025-05-13@20:21:28: Epoch [4/31], Loss: 0.6769\n",
      "[INFO   ] 2025-05-13@20:21:28: Epoch [3/13], Loss: 0.7144\n",
      "[INFO   ] 2025-05-13@20:21:28: Epoch [3/41], Loss: 0.6703\n",
      "[INFO   ] 2025-05-13@20:21:28: Epoch [4/42], Loss: 0.6767\n",
      "[INFO   ] 2025-05-13@20:21:28: Epoch [2/38], Loss: 0.6309\n",
      "[INFO   ] 2025-05-13@20:21:28: Epoch [2/36], Loss: 0.7012\n",
      "[INFO   ] 2025-05-13@20:21:28: Epoch [4/12], Loss: 0.8455\n",
      "[INFO   ] 2025-05-13@20:21:28: Epoch [5/31], Loss: 0.6697\n",
      "[INFO   ] 2025-05-13@20:21:29: Epoch [4/13], Loss: 0.6596\n",
      "[INFO   ] 2025-05-13@20:21:29: Epoch [5/42], Loss: 0.6340\n",
      "[INFO   ] 2025-05-13@20:21:29: Epoch [6/31], Loss: 0.6550\n",
      "[INFO   ] 2025-05-13@20:21:29: Epoch [4/41], Loss: 0.6138\n",
      "[INFO   ] 2025-05-13@20:21:29: Epoch [5/12], Loss: 0.6842\n",
      "[INFO   ] 2025-05-13@20:21:30: Epoch [7/31], Loss: 0.7178\n",
      "[INFO   ] 2025-05-13@20:21:30: Epoch [3/38], Loss: 0.6675\n",
      "[INFO   ] 2025-05-13@20:21:30: Epoch [6/42], Loss: 0.6679\n",
      "[INFO   ] 2025-05-13@20:21:30: Epoch [5/13], Loss: 0.6649\n",
      "[INFO   ] 2025-05-13@20:21:30: Epoch [3/36], Loss: 0.6656\n",
      "[INFO   ] 2025-05-13@20:21:30: Epoch [6/12], Loss: 0.8294\n",
      "[INFO   ] 2025-05-13@20:21:30: Epoch [1/48], Loss: 0.6741\n",
      "[INFO   ] 2025-05-13@20:21:30: Epoch [5/41], Loss: 0.6530\n",
      "[INFO   ] 2025-05-13@20:21:31: Epoch [8/31], Loss: 0.6904\n",
      "[INFO   ] 2025-05-13@20:21:31: Epoch [7/42], Loss: 0.6221\n",
      "[INFO   ] 2025-05-13@20:21:31: Epoch [7/12], Loss: 0.7649\n",
      "[INFO   ] 2025-05-13@20:21:31: Epoch [6/13], Loss: 0.5808\n",
      "[INFO   ] 2025-05-13@20:21:31: Epoch [6/41], Loss: 0.7160\n",
      "[INFO   ] 2025-05-13@20:21:31: Epoch [9/31], Loss: 0.5042\n",
      "[INFO   ] 2025-05-13@20:21:32: Epoch [4/38], Loss: 0.5623\n",
      "[INFO   ] 2025-05-13@20:21:32: Epoch [8/42], Loss: 0.6185\n",
      "[INFO   ] 2025-05-13@20:21:32: Epoch [8/12], Loss: 0.7326\n",
      "[INFO   ] 2025-05-13@20:21:32: Epoch [4/36], Loss: 0.7378\n",
      "[INFO   ] 2025-05-13@20:21:32: Epoch [7/13], Loss: 0.6017\n",
      "[INFO   ] 2025-05-13@20:21:32: Epoch [10/31], Loss: 0.7853\n",
      "[INFO   ] 2025-05-13@20:21:33: Epoch [9/42], Loss: 0.5189\n",
      "[INFO   ] 2025-05-13@20:21:33: Epoch [7/41], Loss: 0.5610\n",
      "[INFO   ] 2025-05-13@20:21:33: Epoch [9/12], Loss: 0.6842\n",
      "[INFO   ] 2025-05-13@20:21:33: Epoch [11/31], Loss: 0.5767\n",
      "[INFO   ] 2025-05-13@20:21:33: Epoch [8/13], Loss: 0.5583\n",
      "[INFO   ] 2025-05-13@20:21:33: Epoch [5/38], Loss: 0.6612\n",
      "[INFO   ] 2025-05-13@20:21:33: Epoch [10/42], Loss: 0.5897\n",
      "[INFO   ] 2025-05-13@20:21:34: Epoch [12/31], Loss: 0.5915\n",
      "[INFO   ] 2025-05-13@20:21:34: Epoch [8/41], Loss: 0.7451\n",
      "[INFO   ] 2025-05-13@20:21:34: Epoch [10/12], Loss: 0.7326\n",
      "[INFO   ] 2025-05-13@20:21:34: Epoch [5/36], Loss: 0.8223\n",
      "[INFO   ] 2025-05-13@20:21:34: Epoch [11/42], Loss: 0.5343\n",
      "[INFO   ] 2025-05-13@20:21:34: Epoch [9/13], Loss: 0.6089\n",
      "[INFO   ] 2025-05-13@20:21:35: Epoch [13/31], Loss: 0.5746\n",
      "[INFO   ] 2025-05-13@20:21:35: Epoch [9/41], Loss: 0.6300\n",
      "[INFO   ] 2025-05-13@20:21:35: Epoch [11/12], Loss: 0.7487\n",
      "[INFO   ] 2025-05-13@20:21:35: Epoch [6/38], Loss: 0.8337\n",
      "[INFO   ] 2025-05-13@20:21:35: Epoch [12/42], Loss: 0.5461\n",
      "[INFO   ] 2025-05-13@20:21:35: Epoch [14/31], Loss: 0.5708\n",
      "[INFO   ] 2025-05-13@20:21:36: Epoch [10/13], Loss: 0.6603\n",
      "[INFO   ] 2025-05-13@20:21:36: Epoch [12/12], Loss: 0.8455\n",
      "[INFO   ] 2025-05-13@20:21:36: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:21:39: Epoch [6/36], Loss: 0.9246\n",
      "[INFO   ] 2025-05-13@20:21:39: Epoch [2/48], Loss: 0.6611\n",
      "[INFO   ] 2025-05-13@20:21:39: Epoch [13/42], Loss: 0.5800\n",
      "[INFO   ] 2025-05-13@20:21:39: Epoch [15/31], Loss: 0.5822\n",
      "[INFO   ] 2025-05-13@20:21:39: Epoch [10/41], Loss: 0.5683\n",
      "[INFO   ] 2025-05-13@20:21:39: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[I 2025-05-13 17:21:40,165] Trial 6 finished with value: 0.5081318349474775 and parameters: {'window_size': 49, 'batch_size': 120, 'num_epochs': 12, 'lr': 0.009388862570311405}. Best is trial 6 with value: 0.5081318349474775.\n",
      "[INFO   ] 2025-05-13@20:21:40: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:21:40: Initializing model...\n",
      "[INFO   ] 2025-05-13@20:21:40: Training model...\n",
      "[INFO   ] 2025-05-13@20:21:41: Epoch [11/13], Loss: 0.6150\n",
      "[INFO   ] 2025-05-13@20:21:41: Epoch [7/38], Loss: 0.7244\n",
      "[INFO   ] 2025-05-13@20:21:41: Epoch [14/42], Loss: 0.4792\n",
      "[INFO   ] 2025-05-13@20:21:41: Epoch [16/31], Loss: 0.4639\n",
      "[INFO   ] 2025-05-13@20:21:41: Epoch [1/5], Loss: 0.6814\n",
      "[INFO   ] 2025-05-13@20:21:41: Epoch [11/41], Loss: 0.4954\n",
      "[INFO   ] 2025-05-13@20:21:42: Epoch [12/13], Loss: 0.5605\n",
      "[INFO   ] 2025-05-13@20:21:42: Epoch [17/31], Loss: 0.3982\n",
      "[INFO   ] 2025-05-13@20:21:42: Epoch [15/42], Loss: 0.4891\n",
      "[INFO   ] 2025-05-13@20:21:42: Epoch [7/36], Loss: 0.7880\n",
      "[INFO   ] 2025-05-13@20:21:42: Epoch [2/5], Loss: 0.6842\n",
      "[INFO   ] 2025-05-13@20:21:42: Epoch [8/38], Loss: 0.6688\n",
      "[INFO   ] 2025-05-13@20:21:42: Epoch [12/41], Loss: 0.5502\n",
      "[INFO   ] 2025-05-13@20:21:43: Epoch [18/31], Loss: 0.4279\n",
      "[INFO   ] 2025-05-13@20:21:43: Epoch [16/42], Loss: 0.4649\n",
      "[INFO   ] 2025-05-13@20:21:43: Epoch [13/13], Loss: 0.5832\n",
      "[INFO   ] 2025-05-13@20:21:43: Epoch [3/5], Loss: 0.6416\n",
      "[INFO   ] 2025-05-13@20:21:43: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:21:47: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[I 2025-05-13 17:21:47,217] Trial 5 finished with value: 0.21506862236610824 and parameters: {'window_size': 52, 'batch_size': 91, 'num_epochs': 13, 'lr': 0.0023737841439787875}. Best is trial 6 with value: 0.5081318349474775.\n",
      "[INFO   ] 2025-05-13@20:21:47: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:21:48: Initializing model...\n",
      "[INFO   ] 2025-05-13@20:21:48: Training model...\n",
      "[INFO   ] 2025-05-13@20:21:48: Epoch [19/31], Loss: 0.4816\n",
      "[INFO   ] 2025-05-13@20:21:48: Epoch [17/42], Loss: 0.5230\n",
      "[INFO   ] 2025-05-13@20:21:48: Epoch [13/41], Loss: 0.5679\n",
      "[INFO   ] 2025-05-13@20:21:48: Epoch [4/5], Loss: 0.6903\n",
      "[INFO   ] 2025-05-13@20:21:48: Epoch [8/36], Loss: 0.9732\n",
      "[INFO   ] 2025-05-13@20:21:48: Epoch [9/38], Loss: 0.5711\n",
      "[INFO   ] 2025-05-13@20:21:48: Epoch [20/31], Loss: 0.5328\n",
      "[INFO   ] 2025-05-13@20:21:48: Epoch [18/42], Loss: 0.4605\n",
      "[INFO   ] 2025-05-13@20:21:49: Epoch [5/5], Loss: 0.6369\n",
      "[INFO   ] 2025-05-13@20:21:49: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:21:50: Epoch [14/41], Loss: 0.6419\n",
      "[INFO   ] 2025-05-13@20:21:50: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:21:50: Epoch [21/31], Loss: 0.5250\n",
      "[I 2025-05-13 17:21:50,339] Trial 8 finished with value: 0.24562450405676672 and parameters: {'window_size': 11, 'batch_size': 80, 'num_epochs': 5, 'lr': 0.0052234835930524415}. Best is trial 6 with value: 0.5081318349474775.\n",
      "[INFO   ] 2025-05-13@20:21:50: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:21:53: Initializing model...\n",
      "[INFO   ] 2025-05-13@20:21:53: Training model...\n",
      "[INFO   ] 2025-05-13@20:21:53: Epoch [19/42], Loss: 0.4970\n",
      "[INFO   ] 2025-05-13@20:21:53: Epoch [1/27], Loss: 0.7348\n",
      "[INFO   ] 2025-05-13@20:21:53: Epoch [3/48], Loss: 0.6139\n",
      "[INFO   ] 2025-05-13@20:21:53: Epoch [9/36], Loss: 0.6552\n",
      "[INFO   ] 2025-05-13@20:21:54: Epoch [22/31], Loss: 0.4828\n",
      "[INFO   ] 2025-05-13@20:21:54: Epoch [10/38], Loss: 0.8988\n",
      "[INFO   ] 2025-05-13@20:21:54: Epoch [15/41], Loss: 0.6389\n",
      "[INFO   ] 2025-05-13@20:21:54: Epoch [1/42], Loss: 0.7079\n",
      "[INFO   ] 2025-05-13@20:21:54: Epoch [20/42], Loss: 0.4603\n",
      "[INFO   ] 2025-05-13@20:21:54: Epoch [23/31], Loss: 0.5246\n",
      "[INFO   ] 2025-05-13@20:21:55: Epoch [16/41], Loss: 0.5979\n",
      "[INFO   ] 2025-05-13@20:21:55: Epoch [21/42], Loss: 0.4332\n",
      "[INFO   ] 2025-05-13@20:21:55: Epoch [2/42], Loss: 0.6989\n",
      "[INFO   ] 2025-05-13@20:21:55: Epoch [24/31], Loss: 0.4836\n",
      "[INFO   ] 2025-05-13@20:21:55: Epoch [2/27], Loss: 0.6216\n",
      "[INFO   ] 2025-05-13@20:21:55: Epoch [10/36], Loss: 0.5379\n",
      "[INFO   ] 2025-05-13@20:21:55: Epoch [11/38], Loss: 0.6682\n",
      "[INFO   ] 2025-05-13@20:21:55: Epoch [22/42], Loss: 0.4056\n",
      "[INFO   ] 2025-05-13@20:21:56: Epoch [25/31], Loss: 0.3714\n",
      "[INFO   ] 2025-05-13@20:21:56: Epoch [17/41], Loss: 0.6066\n",
      "[INFO   ] 2025-05-13@20:21:56: Epoch [3/42], Loss: 0.6760\n",
      "[INFO   ] 2025-05-13@20:21:56: Epoch [23/42], Loss: 0.4461\n",
      "[INFO   ] 2025-05-13@20:21:56: Epoch [26/31], Loss: 0.4520\n",
      "[INFO   ] 2025-05-13@20:21:57: Epoch [4/42], Loss: 0.6626\n",
      "[INFO   ] 2025-05-13@20:21:57: Epoch [18/41], Loss: 0.4939\n",
      "[INFO   ] 2025-05-13@20:21:57: Epoch [12/38], Loss: 0.7628\n",
      "[INFO   ] 2025-05-13@20:21:57: Epoch [11/36], Loss: 0.5735\n",
      "[INFO   ] 2025-05-13@20:21:57: Epoch [3/27], Loss: 0.5763\n",
      "[INFO   ] 2025-05-13@20:21:57: Epoch [24/42], Loss: 0.4302\n",
      "[INFO   ] 2025-05-13@20:21:57: Epoch [27/31], Loss: 0.4336\n",
      "[INFO   ] 2025-05-13@20:21:57: Epoch [5/42], Loss: 0.6920\n",
      "[INFO   ] 2025-05-13@20:21:58: Epoch [19/41], Loss: 0.6300\n",
      "[INFO   ] 2025-05-13@20:21:58: Epoch [28/31], Loss: 0.4632\n",
      "[INFO   ] 2025-05-13@20:21:58: Epoch [25/42], Loss: 0.4371\n",
      "[INFO   ] 2025-05-13@20:21:58: Epoch [4/48], Loss: 0.6129\n",
      "[INFO   ] 2025-05-13@20:21:58: Epoch [13/38], Loss: 0.5436\n",
      "[INFO   ] 2025-05-13@20:21:58: Epoch [6/42], Loss: 0.6370\n",
      "[INFO   ] 2025-05-13@20:21:58: Epoch [12/36], Loss: 0.9411\n",
      "[INFO   ] 2025-05-13@20:21:59: Epoch [29/31], Loss: 0.4464\n",
      "[INFO   ] 2025-05-13@20:21:59: Epoch [26/42], Loss: 0.3831\n",
      "[INFO   ] 2025-05-13@20:21:59: Epoch [20/41], Loss: 0.4384\n",
      "[INFO   ] 2025-05-13@20:21:59: Epoch [4/27], Loss: 0.7023\n",
      "[INFO   ] 2025-05-13@20:21:59: Epoch [30/31], Loss: 0.3500\n",
      "[INFO   ] 2025-05-13@20:21:59: Epoch [27/42], Loss: 0.4063\n",
      "[INFO   ] 2025-05-13@20:21:59: Epoch [7/42], Loss: 0.5793\n",
      "[INFO   ] 2025-05-13@20:22:00: Epoch [21/41], Loss: 0.4604\n",
      "[INFO   ] 2025-05-13@20:22:00: Epoch [14/38], Loss: 0.5637\n",
      "[INFO   ] 2025-05-13@20:22:00: Epoch [31/31], Loss: 0.4157\n",
      "[INFO   ] 2025-05-13@20:22:00: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:22:02: Epoch [28/42], Loss: 0.3817\n",
      "[INFO   ] 2025-05-13@20:22:02: Epoch [13/36], Loss: 0.7374\n",
      "[INFO   ] 2025-05-13@20:22:02: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[I 2025-05-13 17:22:02,897] Trial 7 finished with value: 0.11482794487333592 and parameters: {'window_size': 31, 'batch_size': 124, 'num_epochs': 31, 'lr': 0.002205438601357001}. Best is trial 6 with value: 0.5081318349474775.\n",
      "[INFO   ] 2025-05-13@20:22:02: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:22:03: Initializing model...\n",
      "[INFO   ] 2025-05-13@20:22:03: Training model...\n",
      "[INFO   ] 2025-05-13@20:22:03: Epoch [8/42], Loss: 0.6276\n",
      "[INFO   ] 2025-05-13@20:22:03: Epoch [22/41], Loss: 0.5019\n",
      "[INFO   ] 2025-05-13@20:22:03: Epoch [5/27], Loss: 0.7159\n",
      "[INFO   ] 2025-05-13@20:22:04: Epoch [29/42], Loss: 0.4220\n",
      "[INFO   ] 2025-05-13@20:22:04: Epoch [1/19], Loss: 0.6956\n",
      "[INFO   ] 2025-05-13@20:22:04: Epoch [9/42], Loss: 0.5051\n",
      "[INFO   ] 2025-05-13@20:22:04: Epoch [15/38], Loss: 0.5324\n",
      "[INFO   ] 2025-05-13@20:22:04: Epoch [23/41], Loss: 0.5527\n",
      "[INFO   ] 2025-05-13@20:22:04: Epoch [30/42], Loss: 0.3824\n",
      "[INFO   ] 2025-05-13@20:22:04: Epoch [2/19], Loss: 0.6983\n",
      "[INFO   ] 2025-05-13@20:22:04: Epoch [14/36], Loss: 0.8622\n",
      "[INFO   ] 2025-05-13@20:22:05: Epoch [10/42], Loss: 0.6760\n",
      "[INFO   ] 2025-05-13@20:22:05: Epoch [3/19], Loss: 0.6560\n",
      "[INFO   ] 2025-05-13@20:22:05: Epoch [31/42], Loss: 0.4177\n",
      "[INFO   ] 2025-05-13@20:22:05: Epoch [6/27], Loss: 0.6411\n",
      "[INFO   ] 2025-05-13@20:22:05: Epoch [24/41], Loss: 0.4227\n",
      "[INFO   ] 2025-05-13@20:22:05: Epoch [5/48], Loss: 0.6414\n",
      "[INFO   ] 2025-05-13@20:22:05: Epoch [16/38], Loss: 0.7409\n",
      "[INFO   ] 2025-05-13@20:22:06: Epoch [4/19], Loss: 0.6987\n",
      "[INFO   ] 2025-05-13@20:22:06: Epoch [11/42], Loss: 0.5268\n",
      "[INFO   ] 2025-05-13@20:22:06: Epoch [32/42], Loss: 0.3992\n",
      "[INFO   ] 2025-05-13@20:22:06: Epoch [15/36], Loss: 0.8304\n",
      "[INFO   ] 2025-05-13@20:22:06: Epoch [5/19], Loss: 0.6921\n",
      "[INFO   ] 2025-05-13@20:22:06: Epoch [25/41], Loss: 0.5491\n",
      "[INFO   ] 2025-05-13@20:22:06: Epoch [12/42], Loss: 0.5339\n",
      "[INFO   ] 2025-05-13@20:22:06: Epoch [33/42], Loss: 0.4086\n",
      "[INFO   ] 2025-05-13@20:22:07: Epoch [17/38], Loss: 0.6056\n",
      "[INFO   ] 2025-05-13@20:22:07: Epoch [6/19], Loss: 0.6797\n",
      "[INFO   ] 2025-05-13@20:22:07: Epoch [7/27], Loss: 0.6831\n",
      "[INFO   ] 2025-05-13@20:22:07: Epoch [34/42], Loss: 0.4407\n",
      "[INFO   ] 2025-05-13@20:22:07: Epoch [26/41], Loss: 0.5274\n",
      "[INFO   ] 2025-05-13@20:22:07: Epoch [13/42], Loss: 0.4342\n",
      "[INFO   ] 2025-05-13@20:22:08: Epoch [7/19], Loss: 0.6010\n",
      "[INFO   ] 2025-05-13@20:22:08: Epoch [16/36], Loss: 0.8583\n",
      "[INFO   ] 2025-05-13@20:22:08: Epoch [35/42], Loss: 0.3513\n",
      "[INFO   ] 2025-05-13@20:22:08: Epoch [8/19], Loss: 0.6765\n",
      "[INFO   ] 2025-05-13@20:22:08: Epoch [27/41], Loss: 0.5592\n",
      "[INFO   ] 2025-05-13@20:22:08: Epoch [14/42], Loss: 0.5597\n",
      "[INFO   ] 2025-05-13@20:22:08: Epoch [18/38], Loss: 0.7954\n",
      "[INFO   ] 2025-05-13@20:22:09: Epoch [8/27], Loss: 0.6724\n",
      "[INFO   ] 2025-05-13@20:22:09: Epoch [36/42], Loss: 0.3825\n",
      "[INFO   ] 2025-05-13@20:22:09: Epoch [9/19], Loss: 0.5853\n",
      "[INFO   ] 2025-05-13@20:22:09: Epoch [15/42], Loss: 0.5111\n",
      "[INFO   ] 2025-05-13@20:22:09: Epoch [28/41], Loss: 0.4352\n",
      "[INFO   ] 2025-05-13@20:22:09: Epoch [17/36], Loss: 0.7425\n",
      "[INFO   ] 2025-05-13@20:22:09: Epoch [10/19], Loss: 0.6480\n",
      "[INFO   ] 2025-05-13@20:22:09: Epoch [37/42], Loss: 0.3360\n",
      "[INFO   ] 2025-05-13@20:22:10: Epoch [19/38], Loss: 0.6826\n",
      "[INFO   ] 2025-05-13@20:22:10: Epoch [6/48], Loss: 0.5270\n",
      "[INFO   ] 2025-05-13@20:22:10: Epoch [16/42], Loss: 0.5339\n",
      "[INFO   ] 2025-05-13@20:22:10: Epoch [29/41], Loss: 0.5407\n",
      "[INFO   ] 2025-05-13@20:22:10: Epoch [11/19], Loss: 0.6336\n",
      "[INFO   ] 2025-05-13@20:22:10: Epoch [38/42], Loss: 0.3618\n",
      "[INFO   ] 2025-05-13@20:22:10: Epoch [9/27], Loss: 0.7221\n",
      "[INFO   ] 2025-05-13@20:22:11: Epoch [17/42], Loss: 0.6505\n",
      "[INFO   ] 2025-05-13@20:22:11: Epoch [12/19], Loss: 0.6313\n",
      "[INFO   ] 2025-05-13@20:22:11: Epoch [39/42], Loss: 0.3537\n",
      "[INFO   ] 2025-05-13@20:22:11: Epoch [18/36], Loss: 0.7501\n",
      "[INFO   ] 2025-05-13@20:22:11: Epoch [30/41], Loss: 0.4005\n",
      "[INFO   ] 2025-05-13@20:22:11: Epoch [20/38], Loss: 0.9032\n",
      "[INFO   ] 2025-05-13@20:22:11: Epoch [13/19], Loss: 0.5379\n",
      "[INFO   ] 2025-05-13@20:22:12: Epoch [18/42], Loss: 0.4250\n",
      "[INFO   ] 2025-05-13@20:22:12: Epoch [40/42], Loss: 0.4386\n",
      "[INFO   ] 2025-05-13@20:22:12: Epoch [31/41], Loss: 0.4605\n",
      "[INFO   ] 2025-05-13@20:22:12: Epoch [14/19], Loss: 0.5007\n",
      "[INFO   ] 2025-05-13@20:22:12: Epoch [10/27], Loss: 0.7090\n",
      "[INFO   ] 2025-05-13@20:22:12: Epoch [41/42], Loss: 0.3828\n",
      "[INFO   ] 2025-05-13@20:22:12: Epoch [19/42], Loss: 0.5411\n",
      "[INFO   ] 2025-05-13@20:22:13: Epoch [21/38], Loss: 0.7351\n",
      "[INFO   ] 2025-05-13@20:22:13: Epoch [19/36], Loss: 0.7984\n",
      "[INFO   ] 2025-05-13@20:22:13: Epoch [15/19], Loss: 0.5563\n",
      "[INFO   ] 2025-05-13@20:22:13: Epoch [32/41], Loss: 0.7039\n",
      "[INFO   ] 2025-05-13@20:22:13: Epoch [42/42], Loss: 0.3415\n",
      "[INFO   ] 2025-05-13@20:22:13: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:22:17: Epoch [20/42], Loss: 0.3995\n",
      "[INFO   ] 2025-05-13@20:22:17: Epoch [16/19], Loss: 0.5549\n",
      "[INFO   ] 2025-05-13@20:22:17: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[I 2025-05-13 17:22:17,893] Trial 2 finished with value: 0.06270851675879607 and parameters: {'window_size': 58, 'batch_size': 128, 'num_epochs': 42, 'lr': 0.0016894495253381443}. Best is trial 6 with value: 0.5081318349474775.\n",
      "[INFO   ] 2025-05-13@20:22:17: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:22:20: Initializing model...\n",
      "[INFO   ] 2025-05-13@20:22:20: Training model...\n",
      "[INFO   ] 2025-05-13@20:22:20: Epoch [11/27], Loss: 0.6700\n",
      "[INFO   ] 2025-05-13@20:22:20: Epoch [17/19], Loss: 0.6030\n",
      "[INFO   ] 2025-05-13@20:22:20: Epoch [33/41], Loss: 0.3981\n",
      "[INFO   ] 2025-05-13@20:22:21: Epoch [22/38], Loss: 0.6254\n",
      "[INFO   ] 2025-05-13@20:22:21: Epoch [21/42], Loss: 0.5779\n",
      "[INFO   ] 2025-05-13@20:22:21: Epoch [20/36], Loss: 0.7155\n",
      "[INFO   ] 2025-05-13@20:22:21: Epoch [7/48], Loss: 0.6418\n",
      "[INFO   ] 2025-05-13@20:22:21: Epoch [1/35], Loss: 0.8382\n",
      "[INFO   ] 2025-05-13@20:22:21: Epoch [18/19], Loss: 0.5639\n",
      "[INFO   ] 2025-05-13@20:22:21: Epoch [34/41], Loss: 0.4006\n",
      "[INFO   ] 2025-05-13@20:22:21: Epoch [22/42], Loss: 0.4886\n",
      "[INFO   ] 2025-05-13@20:22:22: Epoch [19/19], Loss: 0.6390\n",
      "[INFO   ] 2025-05-13@20:22:22: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:22:22: Epoch [2/35], Loss: 0.7263\n",
      "[INFO   ] 2025-05-13@20:22:22: Epoch [23/38], Loss: 0.6645\n",
      "[INFO   ] 2025-05-13@20:22:23: Epoch [12/27], Loss: 0.7223\n",
      "[INFO   ] 2025-05-13@20:22:23: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[I 2025-05-13 17:22:23,214] Trial 11 finished with value: 0.2080115299039882 and parameters: {'window_size': 8, 'batch_size': 90, 'num_epochs': 19, 'lr': 0.0031323488755500123}. Best is trial 6 with value: 0.5081318349474775.\n",
      "[INFO   ] 2025-05-13@20:22:23: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:22:23: Initializing model...\n",
      "[INFO   ] 2025-05-13@20:22:23: Training model...\n",
      "[INFO   ] 2025-05-13@20:22:23: Epoch [21/36], Loss: 0.7539\n",
      "[INFO   ] 2025-05-13@20:22:23: Epoch [23/42], Loss: 0.4024\n",
      "[INFO   ] 2025-05-13@20:22:24: Epoch [35/41], Loss: 0.5072\n",
      "[INFO   ] 2025-05-13@20:22:24: Epoch [3/35], Loss: 0.8567\n",
      "[INFO   ] 2025-05-13@20:22:24: Epoch [1/19], Loss: 0.6754\n",
      "[INFO   ] 2025-05-13@20:22:25: Epoch [24/42], Loss: 0.4413\n",
      "[INFO   ] 2025-05-13@20:22:25: Epoch [24/38], Loss: 0.6284\n",
      "[INFO   ] 2025-05-13@20:22:25: Epoch [36/41], Loss: 0.5906\n",
      "[INFO   ] 2025-05-13@20:22:25: Epoch [2/19], Loss: 0.7148\n",
      "[INFO   ] 2025-05-13@20:22:25: Epoch [4/35], Loss: 0.8567\n",
      "[INFO   ] 2025-05-13@20:22:25: Epoch [13/27], Loss: 0.6596\n",
      "[INFO   ] 2025-05-13@20:22:25: Epoch [22/36], Loss: 0.7628\n",
      "[INFO   ] 2025-05-13@20:22:25: Epoch [25/42], Loss: 0.4043\n",
      "[INFO   ] 2025-05-13@20:22:25: Epoch [3/19], Loss: 0.6566\n",
      "[INFO   ] 2025-05-13@20:22:26: Epoch [5/35], Loss: 0.9220\n",
      "[INFO   ] 2025-05-13@20:22:26: Epoch [37/41], Loss: 0.3695\n",
      "[INFO   ] 2025-05-13@20:22:26: Epoch [25/38], Loss: 0.5725\n",
      "[INFO   ] 2025-05-13@20:22:26: Epoch [4/19], Loss: 0.6853\n",
      "[INFO   ] 2025-05-13@20:22:26: Epoch [26/42], Loss: 0.3742\n",
      "[INFO   ] 2025-05-13@20:22:26: Epoch [8/48], Loss: 0.5456\n",
      "[INFO   ] 2025-05-13@20:22:26: Epoch [6/35], Loss: 0.7698\n",
      "[INFO   ] 2025-05-13@20:22:27: Epoch [23/36], Loss: 0.6091\n",
      "[INFO   ] 2025-05-13@20:22:27: Epoch [38/41], Loss: 0.3882\n",
      "[INFO   ] 2025-05-13@20:22:27: Epoch [14/27], Loss: 0.6828\n",
      "[INFO   ] 2025-05-13@20:22:27: Epoch [5/19], Loss: 0.6595\n",
      "[INFO   ] 2025-05-13@20:22:27: Epoch [27/42], Loss: 0.3441\n",
      "[INFO   ] 2025-05-13@20:22:27: Epoch [26/38], Loss: 0.5568\n",
      "[INFO   ] 2025-05-13@20:22:27: Epoch [7/35], Loss: 0.8785\n",
      "[INFO   ] 2025-05-13@20:22:28: Epoch [39/41], Loss: 0.5801\n",
      "[INFO   ] 2025-05-13@20:22:28: Epoch [6/19], Loss: 0.6469\n",
      "[INFO   ] 2025-05-13@20:22:28: Epoch [28/42], Loss: 0.3391\n",
      "[INFO   ] 2025-05-13@20:22:28: Epoch [24/36], Loss: 0.8430\n",
      "[INFO   ] 2025-05-13@20:22:28: Epoch [8/35], Loss: 0.7046\n",
      "[INFO   ] 2025-05-13@20:22:28: Epoch [7/19], Loss: 0.7139\n",
      "[INFO   ] 2025-05-13@20:22:28: Epoch [15/27], Loss: 0.6698\n",
      "[INFO   ] 2025-05-13@20:22:28: Epoch [40/41], Loss: 0.4007\n",
      "[INFO   ] 2025-05-13@20:22:29: Epoch [27/38], Loss: 0.4549\n",
      "[INFO   ] 2025-05-13@20:22:29: Epoch [29/42], Loss: 0.3245\n",
      "[INFO   ] 2025-05-13@20:22:29: Epoch [8/19], Loss: 0.6260\n",
      "[INFO   ] 2025-05-13@20:22:29: Epoch [9/35], Loss: 0.8133\n",
      "[INFO   ] 2025-05-13@20:22:29: Epoch [41/41], Loss: 0.3133\n",
      "[INFO   ] 2025-05-13@20:22:29: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:22:32: Epoch [25/36], Loss: 0.8723\n",
      "[INFO   ] 2025-05-13@20:22:32: Epoch [9/19], Loss: 0.5984\n",
      "[INFO   ] 2025-05-13@20:22:32: Epoch [30/42], Loss: 0.5208\n",
      "[INFO   ] 2025-05-13@20:22:32: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[I 2025-05-13 17:22:32,557] Trial 3 finished with value: 0.23820299136290768 and parameters: {'window_size': 31, 'batch_size': 65, 'num_epochs': 41, 'lr': 0.002890175501005575}. Best is trial 6 with value: 0.5081318349474775.\n",
      "[INFO   ] 2025-05-13@20:22:32: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:22:35: Initializing model...\n",
      "[INFO   ] 2025-05-13@20:22:35: Epoch [10/35], Loss: 0.8350\n",
      "[INFO   ] 2025-05-13@20:22:35: Training model...\n",
      "[INFO   ] 2025-05-13@20:22:35: Epoch [28/38], Loss: 0.4707\n",
      "[INFO   ] 2025-05-13@20:22:35: Epoch [16/27], Loss: 0.6828\n",
      "[INFO   ] 2025-05-13@20:22:35: Epoch [10/19], Loss: 0.5574\n",
      "[INFO   ] 2025-05-13@20:22:36: Epoch [9/48], Loss: 0.6424\n",
      "[INFO   ] 2025-05-13@20:22:36: Epoch [31/42], Loss: 0.3780\n",
      "[INFO   ] 2025-05-13@20:22:36: Epoch [11/35], Loss: 0.9220\n",
      "[INFO   ] 2025-05-13@20:22:36: Epoch [1/41], Loss: 0.7172\n",
      "[INFO   ] 2025-05-13@20:22:36: Epoch [11/19], Loss: 0.5403\n",
      "[INFO   ] 2025-05-13@20:22:36: Epoch [26/36], Loss: 0.8496\n",
      "[INFO   ] 2025-05-13@20:22:36: Epoch [29/38], Loss: 0.6046\n",
      "[INFO   ] 2025-05-13@20:22:37: Epoch [32/42], Loss: 0.3596\n",
      "[INFO   ] 2025-05-13@20:22:37: Epoch [12/35], Loss: 0.9437\n",
      "[INFO   ] 2025-05-13@20:22:37: Epoch [17/27], Loss: 0.6839\n",
      "[INFO   ] 2025-05-13@20:22:37: Epoch [2/41], Loss: 0.6758\n",
      "[INFO   ] 2025-05-13@20:22:37: Epoch [12/19], Loss: 0.5770\n",
      "[INFO   ] 2025-05-13@20:22:37: Epoch [33/42], Loss: 0.3321\n",
      "[INFO   ] 2025-05-13@20:22:37: Epoch [13/35], Loss: 0.7480\n",
      "[INFO   ] 2025-05-13@20:22:38: Epoch [13/19], Loss: 0.6089\n",
      "[INFO   ] 2025-05-13@20:22:38: Epoch [3/41], Loss: 0.6439\n",
      "[INFO   ] 2025-05-13@20:22:38: Epoch [30/38], Loss: 0.8702\n",
      "[INFO   ] 2025-05-13@20:22:38: Epoch [27/36], Loss: 0.6173\n",
      "[INFO   ] 2025-05-13@20:22:38: Epoch [14/19], Loss: 0.6033\n",
      "[INFO   ] 2025-05-13@20:22:38: Epoch [34/42], Loss: 0.4127\n",
      "[INFO   ] 2025-05-13@20:22:38: Epoch [14/35], Loss: 0.9220\n",
      "[INFO   ] 2025-05-13@20:22:38: Epoch [18/27], Loss: 0.6961\n",
      "[INFO   ] 2025-05-13@20:22:38: Epoch [4/41], Loss: 0.6675\n",
      "[INFO   ] 2025-05-13@20:22:39: Epoch [15/19], Loss: 0.6184\n",
      "[INFO   ] 2025-05-13@20:22:39: Epoch [15/35], Loss: 0.8785\n",
      "[INFO   ] 2025-05-13@20:22:39: Epoch [31/38], Loss: 0.3449\n",
      "[INFO   ] 2025-05-13@20:22:39: Epoch [35/42], Loss: 0.4316\n",
      "[INFO   ] 2025-05-13@20:22:39: Epoch [5/41], Loss: 0.5947\n",
      "[INFO   ] 2025-05-13@20:22:40: Epoch [28/36], Loss: 0.8777\n",
      "[INFO   ] 2025-05-13@20:22:40: Epoch [16/19], Loss: 0.5875\n",
      "[INFO   ] 2025-05-13@20:22:40: Epoch [16/35], Loss: 0.8133\n",
      "[INFO   ] 2025-05-13@20:22:40: Epoch [10/48], Loss: 0.7614\n",
      "[INFO   ] 2025-05-13@20:22:40: Epoch [19/27], Loss: 0.6960\n",
      "[INFO   ] 2025-05-13@20:22:40: Epoch [36/42], Loss: 0.5333\n",
      "[INFO   ] 2025-05-13@20:22:40: Epoch [6/41], Loss: 0.7121\n",
      "[INFO   ] 2025-05-13@20:22:40: Epoch [17/19], Loss: 0.6332\n",
      "[INFO   ] 2025-05-13@20:22:41: Epoch [32/38], Loss: 0.5686\n",
      "[INFO   ] 2025-05-13@20:22:41: Epoch [17/35], Loss: 0.9220\n",
      "[INFO   ] 2025-05-13@20:22:41: Epoch [7/41], Loss: 0.5738\n",
      "[INFO   ] 2025-05-13@20:22:41: Epoch [29/36], Loss: 0.8907\n",
      "[INFO   ] 2025-05-13@20:22:41: Epoch [37/42], Loss: 0.3752\n",
      "[INFO   ] 2025-05-13@20:22:41: Epoch [18/19], Loss: 0.4806\n",
      "[INFO   ] 2025-05-13@20:22:42: Epoch [18/35], Loss: 0.8567\n",
      "[INFO   ] 2025-05-13@20:22:42: Epoch [19/19], Loss: 0.5786\n",
      "[INFO   ] 2025-05-13@20:22:42: Epoch [20/27], Loss: 0.6828\n",
      "[INFO   ] 2025-05-13@20:22:42: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:22:42: Epoch [8/41], Loss: 0.6926\n",
      "[INFO   ] 2025-05-13@20:22:42: Epoch [38/42], Loss: 0.3986\n",
      "[INFO   ] 2025-05-13@20:22:42: Epoch [33/38], Loss: 0.6100\n",
      "[INFO   ] 2025-05-13@20:22:43: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[I 2025-05-13 17:22:43,192] Trial 13 finished with value: 0.36633160267936804 and parameters: {'window_size': 6, 'batch_size': 74, 'num_epochs': 19, 'lr': 0.0028579352682015008}. Best is trial 6 with value: 0.5081318349474775.\n",
      "[INFO   ] 2025-05-13@20:22:43: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:22:44: Initializing model...\n",
      "[INFO   ] 2025-05-13@20:22:44: Training model...\n",
      "[INFO   ] 2025-05-13@20:22:44: Epoch [30/36], Loss: 0.5467\n",
      "[INFO   ] 2025-05-13@20:22:44: Epoch [19/35], Loss: 0.9437\n",
      "[INFO   ] 2025-05-13@20:22:44: Epoch [9/41], Loss: 0.6353\n",
      "[INFO   ] 2025-05-13@20:22:44: Epoch [1/34], Loss: 0.7072\n",
      "[INFO   ] 2025-05-13@20:22:44: Epoch [39/42], Loss: 0.3520\n",
      "[INFO   ] 2025-05-13@20:22:45: Epoch [20/35], Loss: 0.8133\n",
      "[INFO   ] 2025-05-13@20:22:45: Epoch [34/38], Loss: 0.6128\n",
      "[INFO   ] 2025-05-13@20:22:45: Epoch [2/34], Loss: 0.6934\n",
      "[INFO   ] 2025-05-13@20:22:45: Epoch [21/27], Loss: 0.7220\n",
      "[INFO   ] 2025-05-13@20:22:45: Epoch [40/42], Loss: 0.3964\n",
      "[INFO   ] 2025-05-13@20:22:45: Epoch [10/41], Loss: 0.6312\n",
      "[INFO   ] 2025-05-13@20:22:46: Epoch [31/36], Loss: 0.7155\n",
      "[INFO   ] 2025-05-13@20:22:46: Epoch [3/34], Loss: 0.7015\n",
      "[INFO   ] 2025-05-13@20:22:46: Epoch [21/35], Loss: 0.8133\n",
      "[INFO   ] 2025-05-13@20:22:46: Epoch [11/48], Loss: 0.7873\n",
      "[INFO   ] 2025-05-13@20:22:46: Epoch [11/41], Loss: 0.5871\n",
      "[INFO   ] 2025-05-13@20:22:46: Epoch [41/42], Loss: 0.3521\n",
      "[INFO   ] 2025-05-13@20:22:46: Epoch [4/34], Loss: 0.6857\n",
      "[INFO   ] 2025-05-13@20:22:47: Epoch [35/38], Loss: 0.6490\n",
      "[INFO   ] 2025-05-13@20:22:47: Epoch [22/35], Loss: 0.8567\n",
      "[INFO   ] 2025-05-13@20:22:47: Epoch [5/34], Loss: 0.6706\n",
      "[INFO   ] 2025-05-13@20:22:47: Epoch [22/27], Loss: 0.6841\n",
      "[INFO   ] 2025-05-13@20:22:47: Epoch [12/41], Loss: 0.5024\n",
      "[INFO   ] 2025-05-13@20:22:47: Epoch [42/42], Loss: 0.3156\n",
      "[INFO   ] 2025-05-13@20:22:47: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:22:51: Epoch [32/36], Loss: 0.7054\n",
      "[INFO   ] 2025-05-13@20:22:51: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[I 2025-05-13 17:22:51,303] Trial 10 finished with value: 0.18639440608714342 and parameters: {'window_size': 47, 'batch_size': 94, 'num_epochs': 42, 'lr': 0.0025869386099320224}. Best is trial 6 with value: 0.5081318349474775.\n",
      "[INFO   ] 2025-05-13@20:22:51: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:22:52: Initializing model...\n",
      "[INFO   ] 2025-05-13@20:22:52: Training model...\n",
      "[INFO   ] 2025-05-13@20:22:52: Epoch [6/34], Loss: 0.6401\n",
      "[INFO   ] 2025-05-13@20:22:52: Epoch [23/35], Loss: 0.7915\n",
      "[INFO   ] 2025-05-13@20:22:52: Epoch [13/41], Loss: 0.5881\n",
      "[INFO   ] 2025-05-13@20:22:52: Epoch [36/38], Loss: 0.3918\n",
      "[INFO   ] 2025-05-13@20:22:52: Epoch [7/34], Loss: 0.6317\n",
      "[INFO   ] 2025-05-13@20:22:53: Epoch [24/35], Loss: 0.7698\n",
      "[INFO   ] 2025-05-13@20:22:53: Epoch [1/40], Loss: 0.6936\n",
      "[INFO   ] 2025-05-13@20:22:53: Epoch [23/27], Loss: 0.6963\n",
      "[INFO   ] 2025-05-13@20:22:53: Epoch [33/36], Loss: 0.8420\n",
      "[INFO   ] 2025-05-13@20:22:53: Epoch [14/41], Loss: 0.4816\n",
      "[INFO   ] 2025-05-13@20:22:53: Epoch [8/34], Loss: 0.7152\n",
      "[INFO   ] 2025-05-13@20:22:54: Epoch [37/38], Loss: 0.4441\n",
      "[INFO   ] 2025-05-13@20:22:54: Epoch [25/35], Loss: 0.7915\n",
      "[INFO   ] 2025-05-13@20:22:54: Epoch [9/34], Loss: 0.7155\n",
      "[INFO   ] 2025-05-13@20:22:54: Epoch [15/41], Loss: 0.5926\n",
      "[INFO   ] 2025-05-13@20:22:54: Epoch [2/40], Loss: 0.6934\n",
      "[INFO   ] 2025-05-13@20:22:54: Epoch [26/35], Loss: 0.9437\n",
      "[INFO   ] 2025-05-13@20:22:54: Epoch [24/27], Loss: 0.7097\n",
      "[INFO   ] 2025-05-13@20:22:54: Epoch [34/36], Loss: 0.7163\n",
      "[INFO   ] 2025-05-13@20:22:55: Epoch [10/34], Loss: 0.7239\n",
      "[INFO   ] 2025-05-13@20:22:55: Epoch [12/48], Loss: 0.5072\n",
      "[INFO   ] 2025-05-13@20:22:55: Epoch [16/41], Loss: 0.4596\n",
      "[INFO   ] 2025-05-13@20:22:55: Epoch [38/38], Loss: 0.6520\n",
      "[INFO   ] 2025-05-13@20:22:55: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:22:56: Epoch [3/40], Loss: 0.6309\n",
      "[INFO   ] 2025-05-13@20:22:56: Epoch [27/35], Loss: 0.9220\n",
      "[INFO   ] 2025-05-13@20:22:56: Epoch [11/34], Loss: 0.6918\n",
      "[INFO   ] 2025-05-13@20:22:56: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[I 2025-05-13 17:22:56,834] Trial 4 finished with value: 0.0753417963962657 and parameters: {'window_size': 12, 'batch_size': 33, 'num_epochs': 38, 'lr': 0.0030388955643862936}. Best is trial 6 with value: 0.5081318349474775.\n",
      "[INFO   ] 2025-05-13@20:22:56: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:22:59: Initializing model...\n",
      "[INFO   ] 2025-05-13@20:22:59: Training model...\n",
      "[INFO   ] 2025-05-13@20:22:59: Epoch [17/41], Loss: 0.5502\n",
      "[INFO   ] 2025-05-13@20:22:59: Epoch [12/34], Loss: 0.5978\n",
      "[INFO   ] 2025-05-13@20:22:59: Epoch [28/35], Loss: 0.9002\n",
      "[INFO   ] 2025-05-13@20:23:00: Epoch [35/36], Loss: 0.7000\n",
      "[INFO   ] 2025-05-13@20:23:00: Epoch [25/27], Loss: 0.6684\n",
      "[INFO   ] 2025-05-13@20:23:00: Epoch [4/40], Loss: 0.6710\n",
      "[INFO   ] 2025-05-13@20:23:00: Epoch [18/41], Loss: 0.5912\n",
      "[INFO   ] 2025-05-13@20:23:00: Epoch [13/34], Loss: 0.7338\n",
      "[INFO   ] 2025-05-13@20:23:00: Epoch [29/35], Loss: 0.7915\n",
      "[INFO   ] 2025-05-13@20:23:01: Epoch [14/34], Loss: 0.5302\n",
      "[INFO   ] 2025-05-13@20:23:01: Epoch [5/40], Loss: 0.7122\n",
      "[INFO   ] 2025-05-13@20:23:01: Epoch [19/41], Loss: 0.5514\n",
      "[INFO   ] 2025-05-13@20:23:01: Epoch [36/36], Loss: 0.6318\n",
      "[INFO   ] 2025-05-13@20:23:01: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:23:03: Epoch [30/35], Loss: 0.7915\n",
      "[INFO   ] 2025-05-13@20:23:03: Epoch [26/27], Loss: 0.6961\n",
      "[INFO   ] 2025-05-13@20:23:03: Epoch [15/34], Loss: 0.6514\n",
      "[INFO   ] 2025-05-13@20:23:03: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:23:04: Epoch [20/41], Loss: 0.4878\n",
      "[I 2025-05-13 17:23:04,113] Trial 0 finished with value: 0.46070308409693883 and parameters: {'window_size': 27, 'batch_size': 34, 'num_epochs': 36, 'lr': 0.00451296225965972}. Best is trial 6 with value: 0.5081318349474775.\n",
      "[INFO   ] 2025-05-13@20:23:04: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:23:06: Initializing model...\n",
      "[INFO   ] 2025-05-13@20:23:06: Training model...\n",
      "[INFO   ] 2025-05-13@20:23:07: Epoch [31/35], Loss: 0.8785\n",
      "[INFO   ] 2025-05-13@20:23:07: Epoch [6/40], Loss: 0.6208\n",
      "[INFO   ] 2025-05-13@20:23:07: Epoch [16/34], Loss: 0.6933\n",
      "[INFO   ] 2025-05-13@20:23:07: Epoch [1/6], Loss: 0.8133\n",
      "[INFO   ] 2025-05-13@20:23:07: Epoch [13/48], Loss: 0.7877\n",
      "[INFO   ] 2025-05-13@20:23:07: Epoch [21/41], Loss: 0.5040\n",
      "[INFO   ] 2025-05-13@20:23:07: Epoch [32/35], Loss: 1.0089\n",
      "[INFO   ] 2025-05-13@20:23:07: Epoch [17/34], Loss: 0.6053\n",
      "[INFO   ] 2025-05-13@20:23:08: Epoch [27/27], Loss: 0.7092\n",
      "[INFO   ] 2025-05-13@20:23:08: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:23:09: Epoch [7/40], Loss: 0.6620\n",
      "[INFO   ] 2025-05-13@20:23:09: Epoch [18/34], Loss: 0.7423\n",
      "[INFO   ] 2025-05-13@20:23:09: Epoch [22/41], Loss: 0.5986\n",
      "[INFO   ] 2025-05-13@20:23:09: Epoch [33/35], Loss: 0.8567\n",
      "[INFO   ] 2025-05-13@20:23:09: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[I 2025-05-13 17:23:09,804] Trial 9 finished with value: 0.44100850832694405 and parameters: {'window_size': 12, 'batch_size': 27, 'num_epochs': 27, 'lr': 0.008556753040735922}. Best is trial 6 with value: 0.5081318349474775.\n",
      "[INFO   ] 2025-05-13@20:23:09: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:23:12: Initializing model...\n",
      "[INFO   ] 2025-05-13@20:23:12: Training model...\n",
      "[INFO   ] 2025-05-13@20:23:12: Epoch [19/34], Loss: 0.6435\n",
      "[INFO   ] 2025-05-13@20:23:13: Epoch [8/40], Loss: 0.7009\n",
      "[INFO   ] 2025-05-13@20:23:13: Epoch [34/35], Loss: 0.8785\n",
      "[INFO   ] 2025-05-13@20:23:13: Epoch [23/41], Loss: 0.5200\n",
      "[INFO   ] 2025-05-13@20:23:13: Epoch [20/34], Loss: 0.6689\n",
      "[INFO   ] 2025-05-13@20:23:13: Epoch [35/35], Loss: 0.7915\n",
      "[INFO   ] 2025-05-13@20:23:13: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:23:16: Epoch [2/6], Loss: 1.0133\n",
      "[INFO   ] 2025-05-13@20:23:16: Epoch [1/31], Loss: 0.5633\n",
      "[INFO   ] 2025-05-13@20:23:16: Epoch [24/41], Loss: 0.4776\n",
      "[INFO   ] 2025-05-13@20:23:16: Epoch [9/40], Loss: 0.6255\n",
      "[INFO   ] 2025-05-13@20:23:16: Epoch [21/34], Loss: 0.6628\n",
      "[INFO   ] 2025-05-13@20:23:16: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[I 2025-05-13 17:23:17,150] Trial 12 finished with value: 0.4966020290042636 and parameters: {'window_size': 41, 'batch_size': 99, 'num_epochs': 35, 'lr': 0.00900092830042101}. Best is trial 6 with value: 0.5081318349474775.\n",
      "[INFO   ] 2025-05-13@20:23:17: Epoch [22/34], Loss: 0.6178\n",
      "[INFO   ] 2025-05-13@20:23:17: Epoch [25/41], Loss: 0.4530\n",
      "[INFO   ] 2025-05-13@20:23:17: Epoch [10/40], Loss: 0.7015\n",
      "[INFO   ] 2025-05-13@20:23:17: Epoch [14/48], Loss: 0.6609\n",
      "[INFO   ] 2025-05-13@20:23:18: Epoch [23/34], Loss: 0.6163\n",
      "[INFO   ] 2025-05-13@20:23:18: Epoch [26/41], Loss: 0.4526\n",
      "[INFO   ] 2025-05-13@20:23:18: Epoch [24/34], Loss: 0.6180\n",
      "[INFO   ] 2025-05-13@20:23:18: Epoch [11/40], Loss: 0.6907\n",
      "[INFO   ] 2025-05-13@20:23:18: Epoch [27/41], Loss: 0.5209\n",
      "[INFO   ] 2025-05-13@20:23:19: Epoch [25/34], Loss: 0.6519\n",
      "[INFO   ] 2025-05-13@20:23:19: Epoch [3/6], Loss: 0.5133\n",
      "[INFO   ] 2025-05-13@20:23:19: Epoch [12/40], Loss: 0.7126\n",
      "[INFO   ] 2025-05-13@20:23:19: Epoch [2/31], Loss: 1.0633\n",
      "[INFO   ] 2025-05-13@20:23:19: Epoch [26/34], Loss: 0.6255\n",
      "[INFO   ] 2025-05-13@20:23:19: Epoch [28/41], Loss: 0.5003\n",
      "[INFO   ] 2025-05-13@20:23:20: Epoch [27/34], Loss: 0.5745\n",
      "[INFO   ] 2025-05-13@20:23:20: Epoch [29/41], Loss: 0.4647\n",
      "[INFO   ] 2025-05-13@20:23:20: Epoch [13/40], Loss: 0.7706\n",
      "[INFO   ] 2025-05-13@20:23:20: Epoch [28/34], Loss: 0.6248\n",
      "[INFO   ] 2025-05-13@20:23:21: Epoch [30/41], Loss: 0.6148\n",
      "[INFO   ] 2025-05-13@20:23:21: Epoch [29/34], Loss: 0.7036\n",
      "[INFO   ] 2025-05-13@20:23:21: Epoch [15/48], Loss: 0.5633\n",
      "[INFO   ] 2025-05-13@20:23:21: Epoch [14/40], Loss: 0.7061\n",
      "[INFO   ] 2025-05-13@20:23:21: Epoch [4/6], Loss: 0.9133\n",
      "[INFO   ] 2025-05-13@20:23:21: Epoch [30/34], Loss: 0.6621\n",
      "[INFO   ] 2025-05-13@20:23:21: Epoch [31/41], Loss: 0.5337\n",
      "[INFO   ] 2025-05-13@20:23:22: Epoch [15/40], Loss: 0.7057\n",
      "[INFO   ] 2025-05-13@20:23:22: Epoch [31/34], Loss: 0.6536\n",
      "[INFO   ] 2025-05-13@20:23:22: Epoch [1/49], Loss: 0.7133\n",
      "[INFO   ] 2025-05-13@20:23:22: Epoch [3/31], Loss: 0.8133\n",
      "[INFO   ] 2025-05-13@20:23:22: Epoch [32/41], Loss: 0.5130\n",
      "[INFO   ] 2025-05-13@20:23:22: Epoch [32/34], Loss: 0.6355\n",
      "[INFO   ] 2025-05-13@20:23:23: Epoch [16/40], Loss: 0.7629\n",
      "[INFO   ] 2025-05-13@20:23:23: Epoch [33/41], Loss: 0.4527\n",
      "[INFO   ] 2025-05-13@20:23:23: Epoch [33/34], Loss: 0.7488\n",
      "[INFO   ] 2025-05-13@20:23:23: Epoch [34/34], Loss: 0.7037\n",
      "[INFO   ] 2025-05-13@20:23:23: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:23:24: Epoch [34/41], Loss: 0.4699\n",
      "[INFO   ] 2025-05-13@20:23:25: Epoch [17/40], Loss: 0.5645\n",
      "[INFO   ] 2025-05-13@20:23:25: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[I 2025-05-13 17:23:25,189] Trial 15 finished with value: 0.5555453357478496 and parameters: {'window_size': 16, 'batch_size': 98, 'num_epochs': 34, 'lr': 0.007362634552544321}. Best is trial 15 with value: 0.5555453357478496.\n",
      "[INFO   ] 2025-05-13@20:23:25: Epoch [5/6], Loss: 0.8133\n",
      "[INFO   ] 2025-05-13@20:23:25: Epoch [16/48], Loss: 0.5483\n",
      "[INFO   ] 2025-05-13@20:23:25: Epoch [35/41], Loss: 0.4812\n",
      "[INFO   ] 2025-05-13@20:23:25: Epoch [18/40], Loss: 0.6706\n",
      "[INFO   ] 2025-05-13@20:23:26: Epoch [4/31], Loss: 1.3133\n",
      "[INFO   ] 2025-05-13@20:23:26: Epoch [36/41], Loss: 0.4584\n",
      "[INFO   ] 2025-05-13@20:23:26: Epoch [19/40], Loss: 0.7166\n",
      "[INFO   ] 2025-05-13@20:23:26: Epoch [37/41], Loss: 0.3993\n",
      "[INFO   ] 2025-05-13@20:23:27: Epoch [6/6], Loss: 0.8133\n",
      "[INFO   ] 2025-05-13@20:23:27: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:23:30: Epoch [38/41], Loss: 0.4183\n",
      "[INFO   ] 2025-05-13@20:23:30: Epoch [20/40], Loss: 0.7746\n",
      "[INFO   ] 2025-05-13@20:23:30: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[I 2025-05-13 17:23:30,806] Trial 17 finished with value: 0.49082397168989345 and parameters: {'window_size': 41, 'batch_size': 15, 'num_epochs': 6, 'lr': 0.00978421977853116}. Best is trial 15 with value: 0.5555453357478496.\n",
      "[INFO   ] 2025-05-13@20:23:30: Epoch [39/41], Loss: 0.3501\n",
      "[INFO   ] 2025-05-13@20:23:31: Epoch [21/40], Loss: 0.6854\n",
      "[INFO   ] 2025-05-13@20:23:31: Epoch [5/31], Loss: 0.8133\n",
      "[INFO   ] 2025-05-13@20:23:31: Epoch [17/48], Loss: 0.7682\n",
      "[INFO   ] 2025-05-13@20:23:31: Epoch [40/41], Loss: 0.4087\n",
      "[INFO   ] 2025-05-13@20:23:31: Epoch [22/40], Loss: 0.7827\n",
      "[INFO   ] 2025-05-13@20:23:31: Epoch [41/41], Loss: 0.4714\n",
      "[INFO   ] 2025-05-13@20:23:31: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:23:34: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[I 2025-05-13 17:23:34,945] Trial 14 finished with value: 0.055356699612984506 and parameters: {'window_size': 43, 'batch_size': 84, 'num_epochs': 41, 'lr': 0.003611639864989722}. Best is trial 15 with value: 0.5555453357478496.\n",
      "[INFO   ] 2025-05-13@20:23:34: Epoch [2/49], Loss: 1.1133\n",
      "[INFO   ] 2025-05-13@20:23:35: Epoch [23/40], Loss: 0.7693\n",
      "[INFO   ] 2025-05-13@20:23:35: Epoch [24/40], Loss: 0.8370\n",
      "[INFO   ] 2025-05-13@20:23:35: Epoch [6/31], Loss: 0.8133\n",
      "[INFO   ] 2025-05-13@20:23:36: Epoch [18/48], Loss: 0.3598\n",
      "[INFO   ] 2025-05-13@20:23:36: Epoch [25/40], Loss: 0.9412\n",
      "[INFO   ] 2025-05-13@20:23:36: Epoch [26/40], Loss: 0.7692\n",
      "[INFO   ] 2025-05-13@20:23:36: Epoch [27/40], Loss: 0.7816\n",
      "[INFO   ] 2025-05-13@20:23:37: Epoch [7/31], Loss: 0.8133\n",
      "[INFO   ] 2025-05-13@20:23:37: Epoch [28/40], Loss: 0.8028\n",
      "[INFO   ] 2025-05-13@20:23:37: Epoch [19/48], Loss: 0.4698\n",
      "[INFO   ] 2025-05-13@20:23:37: Epoch [29/40], Loss: 0.5943\n",
      "[INFO   ] 2025-05-13@20:23:38: Epoch [30/40], Loss: 0.6985\n",
      "[INFO   ] 2025-05-13@20:23:38: Epoch [8/31], Loss: 1.0633\n",
      "[INFO   ] 2025-05-13@20:23:38: Epoch [3/49], Loss: 1.3133\n",
      "[INFO   ] 2025-05-13@20:23:38: Epoch [31/40], Loss: 0.6983\n",
      "[INFO   ] 2025-05-13@20:23:39: Epoch [32/40], Loss: 0.8184\n",
      "[INFO   ] 2025-05-13@20:23:39: Epoch [20/48], Loss: 0.4271\n",
      "[INFO   ] 2025-05-13@20:23:39: Epoch [33/40], Loss: 0.7823\n",
      "[INFO   ] 2025-05-13@20:23:39: Epoch [9/31], Loss: 0.5633\n",
      "[INFO   ] 2025-05-13@20:23:40: Epoch [34/40], Loss: 0.7895\n",
      "[INFO   ] 2025-05-13@20:23:40: Epoch [35/40], Loss: 0.8847\n",
      "[INFO   ] 2025-05-13@20:23:41: Epoch [36/40], Loss: 0.7895\n",
      "[INFO   ] 2025-05-13@20:23:41: Epoch [21/48], Loss: 0.5332\n",
      "[INFO   ] 2025-05-13@20:23:41: Epoch [10/31], Loss: 0.8133\n",
      "[INFO   ] 2025-05-13@20:23:41: Epoch [37/40], Loss: 0.7895\n",
      "[INFO   ] 2025-05-13@20:23:42: Epoch [38/40], Loss: 0.7895\n",
      "[INFO   ] 2025-05-13@20:23:42: Epoch [4/49], Loss: 0.3133\n",
      "[INFO   ] 2025-05-13@20:23:42: Epoch [39/40], Loss: 0.8847\n",
      "[INFO   ] 2025-05-13@20:23:42: Epoch [11/31], Loss: 0.8133\n",
      "[INFO   ] 2025-05-13@20:23:42: Epoch [22/48], Loss: 0.6162\n",
      "[INFO   ] 2025-05-13@20:23:42: Epoch [40/40], Loss: 0.7418\n",
      "[INFO   ] 2025-05-13@20:23:42: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:23:44: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[I 2025-05-13 17:23:44,104] Trial 16 finished with value: 0.49017349352544876 and parameters: {'window_size': 12, 'batch_size': 42, 'num_epochs': 40, 'lr': 0.009742379059853331}. Best is trial 15 with value: 0.5555453357478496.\n",
      "[INFO   ] 2025-05-13@20:23:44: Epoch [12/31], Loss: 0.5633\n",
      "[INFO   ] 2025-05-13@20:23:45: Epoch [23/48], Loss: 0.3948\n",
      "[INFO   ] 2025-05-13@20:23:45: Epoch [13/31], Loss: 0.8133\n",
      "[INFO   ] 2025-05-13@20:23:46: Epoch [5/49], Loss: 0.9133\n",
      "[INFO   ] 2025-05-13@20:23:46: Epoch [24/48], Loss: 0.3482\n",
      "[INFO   ] 2025-05-13@20:23:47: Epoch [14/31], Loss: 0.8133\n",
      "[INFO   ] 2025-05-13@20:23:47: Epoch [25/48], Loss: 0.3562\n",
      "[INFO   ] 2025-05-13@20:23:48: Epoch [15/31], Loss: 0.8133\n",
      "[INFO   ] 2025-05-13@20:23:48: Epoch [6/49], Loss: 0.5133\n",
      "[INFO   ] 2025-05-13@20:23:49: Epoch [26/48], Loss: 0.4432\n",
      "[INFO   ] 2025-05-13@20:23:49: Epoch [16/31], Loss: 1.0633\n",
      "[INFO   ] 2025-05-13@20:23:50: Epoch [17/31], Loss: 0.8133\n",
      "[INFO   ] 2025-05-13@20:23:50: Epoch [27/48], Loss: 0.5370\n",
      "[INFO   ] 2025-05-13@20:23:51: Epoch [18/31], Loss: 1.0633\n",
      "[INFO   ] 2025-05-13@20:23:51: Epoch [28/48], Loss: 0.5188\n",
      "[INFO   ] 2025-05-13@20:23:51: Epoch [7/49], Loss: 0.5133\n",
      "[INFO   ] 2025-05-13@20:23:52: Epoch [19/31], Loss: 0.5633\n",
      "[INFO   ] 2025-05-13@20:23:52: Epoch [29/48], Loss: 0.5464\n",
      "[INFO   ] 2025-05-13@20:23:53: Epoch [20/31], Loss: 0.5633\n",
      "[INFO   ] 2025-05-13@20:23:54: Epoch [30/48], Loss: 0.4332\n",
      "[INFO   ] 2025-05-13@20:23:54: Epoch [8/49], Loss: 0.7133\n",
      "[INFO   ] 2025-05-13@20:23:54: Epoch [21/31], Loss: 0.8133\n",
      "[INFO   ] 2025-05-13@20:23:55: Epoch [31/48], Loss: 0.5229\n",
      "[INFO   ] 2025-05-13@20:23:55: Epoch [22/31], Loss: 1.0633\n",
      "[INFO   ] 2025-05-13@20:23:56: Epoch [23/31], Loss: 1.0633\n",
      "[INFO   ] 2025-05-13@20:23:56: Epoch [32/48], Loss: 0.5087\n",
      "[INFO   ] 2025-05-13@20:23:57: Epoch [9/49], Loss: 0.5133\n",
      "[INFO   ] 2025-05-13@20:23:57: Epoch [24/31], Loss: 0.5633\n",
      "[INFO   ] 2025-05-13@20:23:58: Epoch [33/48], Loss: 0.3493\n",
      "[INFO   ] 2025-05-13@20:23:58: Epoch [25/31], Loss: 1.0633\n",
      "[INFO   ] 2025-05-13@20:23:59: Epoch [34/48], Loss: 0.6129\n",
      "[INFO   ] 2025-05-13@20:23:59: Epoch [26/31], Loss: 0.8133\n",
      "[INFO   ] 2025-05-13@20:23:59: Epoch [10/49], Loss: 0.9133\n",
      "[INFO   ] 2025-05-13@20:24:00: Epoch [35/48], Loss: 0.6533\n",
      "[INFO   ] 2025-05-13@20:24:00: Epoch [27/31], Loss: 0.5633\n",
      "[INFO   ] 2025-05-13@20:24:02: Epoch [28/31], Loss: 0.8133\n",
      "[INFO   ] 2025-05-13@20:24:02: Epoch [36/48], Loss: 0.5190\n",
      "[INFO   ] 2025-05-13@20:24:02: Epoch [11/49], Loss: 1.1133\n",
      "[INFO   ] 2025-05-13@20:24:03: Epoch [29/31], Loss: 1.0633\n",
      "[INFO   ] 2025-05-13@20:24:03: Epoch [37/48], Loss: 0.4133\n",
      "[INFO   ] 2025-05-13@20:24:04: Epoch [30/31], Loss: 0.8133\n",
      "[INFO   ] 2025-05-13@20:24:04: Epoch [38/48], Loss: 0.3207\n",
      "[INFO   ] 2025-05-13@20:24:05: Epoch [31/31], Loss: 1.0633\n",
      "[INFO   ] 2025-05-13@20:24:05: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:24:08: Epoch [12/49], Loss: 1.1133\n",
      "[INFO   ] 2025-05-13@20:24:08: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[I 2025-05-13 17:24:08,486] Trial 18 finished with value: 0.4705492975744374 and parameters: {'window_size': 42, 'batch_size': 13, 'num_epochs': 31, 'lr': 0.009757331256764168}. Best is trial 15 with value: 0.5555453357478496.\n",
      "[INFO   ] 2025-05-13@20:24:08: Epoch [39/48], Loss: 0.4142\n",
      "[INFO   ] 2025-05-13@20:24:10: Epoch [40/48], Loss: 0.3512\n",
      "[INFO   ] 2025-05-13@20:24:10: Epoch [13/49], Loss: 1.1133\n",
      "[INFO   ] 2025-05-13@20:24:11: Epoch [41/48], Loss: 0.4344\n",
      "[INFO   ] 2025-05-13@20:24:12: Epoch [42/48], Loss: 0.5525\n",
      "[INFO   ] 2025-05-13@20:24:12: Epoch [14/49], Loss: 0.7133\n",
      "[INFO   ] 2025-05-13@20:24:13: Epoch [43/48], Loss: 0.3492\n",
      "[INFO   ] 2025-05-13@20:24:14: Epoch [44/48], Loss: 0.4133\n",
      "[INFO   ] 2025-05-13@20:24:14: Epoch [15/49], Loss: 0.9133\n",
      "[INFO   ] 2025-05-13@20:24:15: Epoch [45/48], Loss: 0.6369\n",
      "[INFO   ] 2025-05-13@20:24:16: Epoch [46/48], Loss: 0.5765\n",
      "[INFO   ] 2025-05-13@20:24:16: Epoch [16/49], Loss: 0.7133\n",
      "[INFO   ] 2025-05-13@20:24:17: Epoch [47/48], Loss: 0.3138\n",
      "[INFO   ] 2025-05-13@20:24:18: Epoch [17/49], Loss: 0.5133\n",
      "[INFO   ] 2025-05-13@20:24:18: Epoch [48/48], Loss: 0.4273\n",
      "[INFO   ] 2025-05-13@20:24:18: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:24:22: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[I 2025-05-13 17:24:22,950] Trial 1 finished with value: 0.20004956981213962 and parameters: {'window_size': 55, 'batch_size': 11, 'num_epochs': 48, 'lr': 0.0017332904994354736}. Best is trial 15 with value: 0.5555453357478496.\n",
      "[INFO   ] 2025-05-13@20:24:24: Epoch [18/49], Loss: 0.3133\n",
      "[INFO   ] 2025-05-13@20:24:25: Epoch [19/49], Loss: 0.9133\n",
      "[INFO   ] 2025-05-13@20:24:26: Epoch [20/49], Loss: 1.1133\n",
      "[INFO   ] 2025-05-13@20:24:28: Epoch [21/49], Loss: 0.7133\n",
      "[INFO   ] 2025-05-13@20:24:29: Epoch [22/49], Loss: 1.1133\n",
      "[INFO   ] 2025-05-13@20:24:30: Epoch [23/49], Loss: 0.9133\n",
      "[INFO   ] 2025-05-13@20:24:32: Epoch [24/49], Loss: 1.1133\n",
      "[INFO   ] 2025-05-13@20:24:33: Epoch [25/49], Loss: 1.1133\n",
      "[INFO   ] 2025-05-13@20:24:34: Epoch [26/49], Loss: 0.9133\n",
      "[INFO   ] 2025-05-13@20:24:36: Epoch [27/49], Loss: 0.9133\n",
      "[INFO   ] 2025-05-13@20:24:37: Epoch [28/49], Loss: 0.9133\n",
      "[INFO   ] 2025-05-13@20:24:38: Epoch [29/49], Loss: 0.9133\n",
      "[INFO   ] 2025-05-13@20:24:40: Epoch [30/49], Loss: 1.1133\n",
      "[INFO   ] 2025-05-13@20:24:41: Epoch [31/49], Loss: 0.5133\n",
      "[INFO   ] 2025-05-13@20:24:43: Epoch [32/49], Loss: 0.9133\n",
      "[INFO   ] 2025-05-13@20:24:44: Epoch [33/49], Loss: 0.5133\n",
      "[INFO   ] 2025-05-13@20:24:45: Epoch [34/49], Loss: 1.1133\n",
      "[INFO   ] 2025-05-13@20:24:47: Epoch [35/49], Loss: 1.1133\n",
      "[INFO   ] 2025-05-13@20:24:48: Epoch [36/49], Loss: 0.9133\n",
      "[INFO   ] 2025-05-13@20:24:49: Epoch [37/49], Loss: 0.5133\n",
      "[INFO   ] 2025-05-13@20:24:51: Epoch [38/49], Loss: 1.1133\n",
      "[INFO   ] 2025-05-13@20:24:52: Epoch [39/49], Loss: 0.9133\n",
      "[INFO   ] 2025-05-13@20:24:53: Epoch [40/49], Loss: 1.3133\n",
      "[INFO   ] 2025-05-13@20:24:55: Epoch [41/49], Loss: 0.9133\n",
      "[INFO   ] 2025-05-13@20:24:56: Epoch [42/49], Loss: 0.7133\n",
      "[INFO   ] 2025-05-13@20:24:57: Epoch [43/49], Loss: 0.7133\n",
      "[INFO   ] 2025-05-13@20:24:59: Epoch [44/49], Loss: 1.1133\n",
      "[INFO   ] 2025-05-13@20:25:00: Epoch [45/49], Loss: 0.7133\n",
      "[INFO   ] 2025-05-13@20:25:02: Epoch [46/49], Loss: 1.1133\n",
      "[INFO   ] 2025-05-13@20:25:03: Epoch [47/49], Loss: 0.9133\n",
      "[INFO   ] 2025-05-13@20:25:04: Epoch [48/49], Loss: 0.7133\n",
      "[INFO   ] 2025-05-13@20:25:06: Epoch [49/49], Loss: 1.1133\n",
      "[INFO   ] 2025-05-13@20:25:06: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:25:09: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[I 2025-05-13 17:25:09,239] Trial 19 finished with value: 0.4946118504777723 and parameters: {'window_size': 41, 'batch_size': 5, 'num_epochs': 49, 'lr': 0.009081316007230295}. Best is trial 15 with value: 0.5555453357478496.\n",
      "[INFO   ] 2025-05-13@20:25:09: Лучшие параметры: {'window_size': 16, 'batch_size': 98, 'num_epochs': 34, 'lr': 0.007362634552544321}\n",
      "[INFO   ] 2025-05-13@20:25:09: Лучший скорректированный ROC AUC Score: 0.5555453357478496\n",
      "[INFO   ] 2025-05-13@20:25:09: Fitting model with best parameters...\n",
      "[INFO   ] 2025-05-13@20:25:09: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:25:10: Initializing model...\n",
      "[INFO   ] 2025-05-13@20:25:10: Training model...\n",
      "[INFO   ] 2025-05-13@20:25:10: Epoch [1/34], Loss: 0.7113\n",
      "[INFO   ] 2025-05-13@20:25:10: Epoch [2/34], Loss: 0.6717\n",
      "[INFO   ] 2025-05-13@20:25:10: Epoch [3/34], Loss: 0.6941\n",
      "[INFO   ] 2025-05-13@20:25:10: Epoch [4/34], Loss: 0.6582\n",
      "[INFO   ] 2025-05-13@20:25:10: Epoch [5/34], Loss: 0.6787\n",
      "[INFO   ] 2025-05-13@20:25:11: Epoch [6/34], Loss: 0.7734\n",
      "[INFO   ] 2025-05-13@20:25:11: Epoch [7/34], Loss: 0.6611\n",
      "[INFO   ] 2025-05-13@20:25:11: Epoch [8/34], Loss: 0.6754\n",
      "[INFO   ] 2025-05-13@20:25:11: Epoch [9/34], Loss: 0.6793\n",
      "[INFO   ] 2025-05-13@20:25:11: Epoch [10/34], Loss: 0.6970\n",
      "[INFO   ] 2025-05-13@20:25:11: Epoch [11/34], Loss: 0.6936\n",
      "[INFO   ] 2025-05-13@20:25:12: Epoch [12/34], Loss: 0.6498\n",
      "[INFO   ] 2025-05-13@20:25:12: Epoch [13/34], Loss: 0.6284\n",
      "[INFO   ] 2025-05-13@20:25:12: Epoch [14/34], Loss: 0.7187\n",
      "[INFO   ] 2025-05-13@20:25:12: Epoch [15/34], Loss: 0.5895\n",
      "[INFO   ] 2025-05-13@20:25:12: Epoch [16/34], Loss: 0.6439\n",
      "[INFO   ] 2025-05-13@20:25:12: Epoch [17/34], Loss: 0.6515\n",
      "[INFO   ] 2025-05-13@20:25:12: Epoch [18/34], Loss: 0.6718\n",
      "[INFO   ] 2025-05-13@20:25:12: Epoch [19/34], Loss: 0.5781\n",
      "[INFO   ] 2025-05-13@20:25:13: Epoch [20/34], Loss: 0.6760\n",
      "[INFO   ] 2025-05-13@20:25:13: Epoch [21/34], Loss: 0.6193\n",
      "[INFO   ] 2025-05-13@20:25:13: Epoch [22/34], Loss: 0.5804\n",
      "[INFO   ] 2025-05-13@20:25:13: Epoch [23/34], Loss: 0.6761\n",
      "[INFO   ] 2025-05-13@20:25:13: Epoch [24/34], Loss: 0.6014\n",
      "[INFO   ] 2025-05-13@20:25:13: Epoch [25/34], Loss: 0.5978\n",
      "[INFO   ] 2025-05-13@20:25:13: Epoch [26/34], Loss: 0.6417\n",
      "[INFO   ] 2025-05-13@20:25:14: Epoch [27/34], Loss: 0.6582\n",
      "[INFO   ] 2025-05-13@20:25:14: Epoch [28/34], Loss: 0.6326\n",
      "[INFO   ] 2025-05-13@20:25:14: Epoch [29/34], Loss: 0.6506\n",
      "[INFO   ] 2025-05-13@20:25:14: Epoch [30/34], Loss: 0.6313\n",
      "[INFO   ] 2025-05-13@20:25:14: Epoch [31/34], Loss: 0.6390\n",
      "[INFO   ] 2025-05-13@20:25:14: Epoch [32/34], Loss: 0.5833\n",
      "[INFO   ] 2025-05-13@20:25:14: Epoch [33/34], Loss: 0.6333\n",
      "[INFO   ] 2025-05-13@20:25:15: Epoch [34/34], Loss: 0.6543\n",
      "[INFO   ] 2025-05-13@20:25:15: Generating predictions...\n",
      "[INFO   ] 2025-05-13@20:25:15: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:25:16: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:25:16: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:25:16: Evaluating ROC AUC...\n",
      "[INFO   ] 2025-05-13@20:25:16: Evaluating metrics by threshold...\n",
      "[INFO   ] 2025-05-13@20:25:16: Running backtesting and collecting all metrics..\n",
      "[INFO   ] 2025-05-13@20:25:16: Converting numpy arrays to TensorDataset and DataLoader...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Метрики для TRAIN выборки ===\n",
      "ROC AUC: 0.7388\n",
      "   Cutoff  Precision     Recall   Accuracy   F1-Score\n",
      "0    50.0  65.763324  81.340782  67.344498  72.727273\n",
      "1    60.0  67.707317  77.541899  68.181818  72.291667\n",
      "2    70.0  69.296375  72.625698  68.122010  70.921986\n",
      "3    80.0  70.616114  66.592179  67.284689  68.545141\n",
      "\n",
      "=== Метрики для TEST выборки ===\n",
      "ROC AUC: 0.3840\n",
      "   Cutoff  Precision     Recall   Accuracy   F1-Score\n",
      "0    50.0  50.000000  87.500000  47.826087  63.636364\n",
      "1    60.0  48.333333  60.416667  45.652174  53.703704\n",
      "2    70.0  47.916667  47.916667  45.652174  47.916667\n",
      "3    80.0  41.379310  25.000000  42.391304  31.168831\n",
      "\n",
      "=== Метрики для VAL выборки ===\n",
      "ROC AUC: 0.5012\n",
      "   Cutoff  Precision     Recall   Accuracy   F1-Score\n",
      "0    50.0  47.826087  76.744186  48.888889  58.928571\n",
      "1    60.0  41.666667  46.511628  43.333333  43.956044\n",
      "2    70.0  50.000000  27.906977  52.222222  35.820896\n",
      "3    80.0  66.666667   9.302326  54.444444  16.326531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO   ] 2025-05-13@20:25:17: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:25:19: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:25:20: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:25:21: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:25:21: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:25:22: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:25:22: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:25:22: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:25:22: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:25:22: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:25:22: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-13@20:25:22: Saving results to DB...\n",
      "[INFO   ] 2025-05-13@20:25:23: ML Model Strategy training loop complete!\n"
     ]
    }
   ],
   "source": [
    "result = ml_model_strategy_training_loop(\n",
    "    tickers=tickers,\n",
    "    interval=interval,\n",
    "    train_start=train_start,\n",
    "    train_end=train_end,\n",
    "    test_end=test_end,\n",
    "    valid_end=valid_end,\n",
    "    model_options=model_options,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Experiment_ID</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Interval</th>\n",
       "      <th>Type</th>\n",
       "      <th>START_DT</th>\n",
       "      <th>END_DT</th>\n",
       "      <th>Model</th>\n",
       "      <th>Model_params</th>\n",
       "      <th>Cutoff</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1_Score</th>\n",
       "      <th>ROC_AUC</th>\n",
       "      <th>Return_pct</th>\n",
       "      <th>Win_Rate_pct</th>\n",
       "      <th>Num_Trades</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17307833-3006-11f0-959c-240a64112db6</td>\n",
       "      <td>^GSPC</td>\n",
       "      <td>1d</td>\n",
       "      <td>TRAIN</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>2024-10-01</td>\n",
       "      <td>CNNModel</td>\n",
       "      <td>{\"window_size\": 16, \"batch_size\": 98, \"num_epo...</td>\n",
       "      <td>50.0</td>\n",
       "      <td>65.763324</td>\n",
       "      <td>81.340782</td>\n",
       "      <td>67.344498</td>\n",
       "      <td>72.727273</td>\n",
       "      <td>0.738796</td>\n",
       "      <td>325.101135</td>\n",
       "      <td>70.38835</td>\n",
       "      <td>206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17307833-3006-11f0-959c-240a64112db6</td>\n",
       "      <td>^GSPC</td>\n",
       "      <td>1d</td>\n",
       "      <td>TRAIN</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>2024-10-01</td>\n",
       "      <td>CNNModel</td>\n",
       "      <td>{\"window_size\": 16, \"batch_size\": 98, \"num_epo...</td>\n",
       "      <td>60.0</td>\n",
       "      <td>67.707317</td>\n",
       "      <td>77.541899</td>\n",
       "      <td>68.181818</td>\n",
       "      <td>72.291667</td>\n",
       "      <td>0.738796</td>\n",
       "      <td>486.080849</td>\n",
       "      <td>66.028708</td>\n",
       "      <td>209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17307833-3006-11f0-959c-240a64112db6</td>\n",
       "      <td>^GSPC</td>\n",
       "      <td>1d</td>\n",
       "      <td>TRAIN</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>2024-10-01</td>\n",
       "      <td>CNNModel</td>\n",
       "      <td>{\"window_size\": 16, \"batch_size\": 98, \"num_epo...</td>\n",
       "      <td>70.0</td>\n",
       "      <td>69.296375</td>\n",
       "      <td>72.625698</td>\n",
       "      <td>68.122010</td>\n",
       "      <td>70.921986</td>\n",
       "      <td>0.738796</td>\n",
       "      <td>467.10368</td>\n",
       "      <td>66.509434</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17307833-3006-11f0-959c-240a64112db6</td>\n",
       "      <td>^GSPC</td>\n",
       "      <td>1d</td>\n",
       "      <td>TRAIN</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>2024-10-01</td>\n",
       "      <td>CNNModel</td>\n",
       "      <td>{\"window_size\": 16, \"batch_size\": 98, \"num_epo...</td>\n",
       "      <td>80.0</td>\n",
       "      <td>70.616114</td>\n",
       "      <td>66.592179</td>\n",
       "      <td>67.284689</td>\n",
       "      <td>68.545141</td>\n",
       "      <td>0.738796</td>\n",
       "      <td>498.668633</td>\n",
       "      <td>69.387755</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17307833-3006-11f0-959c-240a64112db6</td>\n",
       "      <td>^GSPC</td>\n",
       "      <td>1d</td>\n",
       "      <td>TEST</td>\n",
       "      <td>2024-10-01</td>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>CNNModel</td>\n",
       "      <td>{\"window_size\": 16, \"batch_size\": 98, \"num_epo...</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>87.500000</td>\n",
       "      <td>47.826087</td>\n",
       "      <td>63.636364</td>\n",
       "      <td>0.383996</td>\n",
       "      <td>1.777532</td>\n",
       "      <td>71.428571</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>17307833-3006-11f0-959c-240a64112db6</td>\n",
       "      <td>^GSPC</td>\n",
       "      <td>1d</td>\n",
       "      <td>TEST</td>\n",
       "      <td>2024-10-01</td>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>CNNModel</td>\n",
       "      <td>{\"window_size\": 16, \"batch_size\": 98, \"num_epo...</td>\n",
       "      <td>60.0</td>\n",
       "      <td>48.333333</td>\n",
       "      <td>60.416667</td>\n",
       "      <td>45.652174</td>\n",
       "      <td>53.703704</td>\n",
       "      <td>0.383996</td>\n",
       "      <td>-4.156214</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>17307833-3006-11f0-959c-240a64112db6</td>\n",
       "      <td>^GSPC</td>\n",
       "      <td>1d</td>\n",
       "      <td>TEST</td>\n",
       "      <td>2024-10-01</td>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>CNNModel</td>\n",
       "      <td>{\"window_size\": 16, \"batch_size\": 98, \"num_epo...</td>\n",
       "      <td>70.0</td>\n",
       "      <td>47.916667</td>\n",
       "      <td>47.916667</td>\n",
       "      <td>45.652174</td>\n",
       "      <td>47.916667</td>\n",
       "      <td>0.383996</td>\n",
       "      <td>-4.256129</td>\n",
       "      <td>43.75</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>17307833-3006-11f0-959c-240a64112db6</td>\n",
       "      <td>^GSPC</td>\n",
       "      <td>1d</td>\n",
       "      <td>TEST</td>\n",
       "      <td>2024-10-01</td>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>CNNModel</td>\n",
       "      <td>{\"window_size\": 16, \"batch_size\": 98, \"num_epo...</td>\n",
       "      <td>80.0</td>\n",
       "      <td>41.379310</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>42.391304</td>\n",
       "      <td>31.168831</td>\n",
       "      <td>0.383996</td>\n",
       "      <td>-11.165178</td>\n",
       "      <td>50.0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>17307833-3006-11f0-959c-240a64112db6</td>\n",
       "      <td>^GSPC</td>\n",
       "      <td>1d</td>\n",
       "      <td>VALID</td>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>CNNModel</td>\n",
       "      <td>{\"window_size\": 16, \"batch_size\": 98, \"num_epo...</td>\n",
       "      <td>50.0</td>\n",
       "      <td>47.826087</td>\n",
       "      <td>76.744186</td>\n",
       "      <td>48.888889</td>\n",
       "      <td>58.928571</td>\n",
       "      <td>0.501237</td>\n",
       "      <td>-6.225919</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>17307833-3006-11f0-959c-240a64112db6</td>\n",
       "      <td>^GSPC</td>\n",
       "      <td>1d</td>\n",
       "      <td>VALID</td>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>CNNModel</td>\n",
       "      <td>{\"window_size\": 16, \"batch_size\": 98, \"num_epo...</td>\n",
       "      <td>60.0</td>\n",
       "      <td>41.666667</td>\n",
       "      <td>46.511628</td>\n",
       "      <td>43.333333</td>\n",
       "      <td>43.956044</td>\n",
       "      <td>0.501237</td>\n",
       "      <td>-8.535883</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>17307833-3006-11f0-959c-240a64112db6</td>\n",
       "      <td>^GSPC</td>\n",
       "      <td>1d</td>\n",
       "      <td>VALID</td>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>CNNModel</td>\n",
       "      <td>{\"window_size\": 16, \"batch_size\": 98, \"num_epo...</td>\n",
       "      <td>70.0</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>27.906977</td>\n",
       "      <td>52.222222</td>\n",
       "      <td>35.820896</td>\n",
       "      <td>0.501237</td>\n",
       "      <td>-2.556942</td>\n",
       "      <td>62.5</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>17307833-3006-11f0-959c-240a64112db6</td>\n",
       "      <td>^GSPC</td>\n",
       "      <td>1d</td>\n",
       "      <td>VALID</td>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>CNNModel</td>\n",
       "      <td>{\"window_size\": 16, \"batch_size\": 98, \"num_epo...</td>\n",
       "      <td>80.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>9.302326</td>\n",
       "      <td>54.444444</td>\n",
       "      <td>16.326531</td>\n",
       "      <td>0.501237</td>\n",
       "      <td>5.695134</td>\n",
       "      <td>75.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Experiment_ID Ticker Interval   Type    START_DT  \\\n",
       "0   17307833-3006-11f0-959c-240a64112db6  ^GSPC       1d  TRAIN  2020-01-01   \n",
       "1   17307833-3006-11f0-959c-240a64112db6  ^GSPC       1d  TRAIN  2020-01-01   \n",
       "2   17307833-3006-11f0-959c-240a64112db6  ^GSPC       1d  TRAIN  2020-01-01   \n",
       "3   17307833-3006-11f0-959c-240a64112db6  ^GSPC       1d  TRAIN  2020-01-01   \n",
       "4   17307833-3006-11f0-959c-240a64112db6  ^GSPC       1d   TEST  2024-10-01   \n",
       "5   17307833-3006-11f0-959c-240a64112db6  ^GSPC       1d   TEST  2024-10-01   \n",
       "6   17307833-3006-11f0-959c-240a64112db6  ^GSPC       1d   TEST  2024-10-01   \n",
       "7   17307833-3006-11f0-959c-240a64112db6  ^GSPC       1d   TEST  2024-10-01   \n",
       "8   17307833-3006-11f0-959c-240a64112db6  ^GSPC       1d  VALID  2025-01-01   \n",
       "9   17307833-3006-11f0-959c-240a64112db6  ^GSPC       1d  VALID  2025-01-01   \n",
       "10  17307833-3006-11f0-959c-240a64112db6  ^GSPC       1d  VALID  2025-01-01   \n",
       "11  17307833-3006-11f0-959c-240a64112db6  ^GSPC       1d  VALID  2025-01-01   \n",
       "\n",
       "        END_DT     Model                                       Model_params  \\\n",
       "0   2024-10-01  CNNModel  {\"window_size\": 16, \"batch_size\": 98, \"num_epo...   \n",
       "1   2024-10-01  CNNModel  {\"window_size\": 16, \"batch_size\": 98, \"num_epo...   \n",
       "2   2024-10-01  CNNModel  {\"window_size\": 16, \"batch_size\": 98, \"num_epo...   \n",
       "3   2024-10-01  CNNModel  {\"window_size\": 16, \"batch_size\": 98, \"num_epo...   \n",
       "4   2025-01-01  CNNModel  {\"window_size\": 16, \"batch_size\": 98, \"num_epo...   \n",
       "5   2025-01-01  CNNModel  {\"window_size\": 16, \"batch_size\": 98, \"num_epo...   \n",
       "6   2025-01-01  CNNModel  {\"window_size\": 16, \"batch_size\": 98, \"num_epo...   \n",
       "7   2025-01-01  CNNModel  {\"window_size\": 16, \"batch_size\": 98, \"num_epo...   \n",
       "8   2025-04-01  CNNModel  {\"window_size\": 16, \"batch_size\": 98, \"num_epo...   \n",
       "9   2025-04-01  CNNModel  {\"window_size\": 16, \"batch_size\": 98, \"num_epo...   \n",
       "10  2025-04-01  CNNModel  {\"window_size\": 16, \"batch_size\": 98, \"num_epo...   \n",
       "11  2025-04-01  CNNModel  {\"window_size\": 16, \"batch_size\": 98, \"num_epo...   \n",
       "\n",
       "    Cutoff  Precision     Recall   Accuracy   F1_Score   ROC_AUC  Return_pct  \\\n",
       "0     50.0  65.763324  81.340782  67.344498  72.727273  0.738796  325.101135   \n",
       "1     60.0  67.707317  77.541899  68.181818  72.291667  0.738796  486.080849   \n",
       "2     70.0  69.296375  72.625698  68.122010  70.921986  0.738796   467.10368   \n",
       "3     80.0  70.616114  66.592179  67.284689  68.545141  0.738796  498.668633   \n",
       "4     50.0  50.000000  87.500000  47.826087  63.636364  0.383996    1.777532   \n",
       "5     60.0  48.333333  60.416667  45.652174  53.703704  0.383996   -4.156214   \n",
       "6     70.0  47.916667  47.916667  45.652174  47.916667  0.383996   -4.256129   \n",
       "7     80.0  41.379310  25.000000  42.391304  31.168831  0.383996  -11.165178   \n",
       "8     50.0  47.826087  76.744186  48.888889  58.928571  0.501237   -6.225919   \n",
       "9     60.0  41.666667  46.511628  43.333333  43.956044  0.501237   -8.535883   \n",
       "10    70.0  50.000000  27.906977  52.222222  35.820896  0.501237   -2.556942   \n",
       "11    80.0  66.666667   9.302326  54.444444  16.326531  0.501237    5.695134   \n",
       "\n",
       "   Win_Rate_pct Num_Trades  \n",
       "0      70.38835        206  \n",
       "1     66.028708        209  \n",
       "2     66.509434        212  \n",
       "3     69.387755        196  \n",
       "4     71.428571          7  \n",
       "5     33.333333         12  \n",
       "6         43.75         16  \n",
       "7          50.0         18  \n",
       "8     66.666667          3  \n",
       "9     66.666667          6  \n",
       "10         62.5         16  \n",
       "11         75.0          8  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим эту модель также в список возможностей в UI и протестируем полный функционал, с проведением экспериментов и просмотром результатов в Streamlit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализуем также модели использующие LSTM и GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Архитектура моделей ниже, добавлены в соответствующие модули в /app/src/models и в UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Потенциальные параметры для тюнинга\n",
    "window_size = 30\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "batch_size = 32\n",
    "lr = 0.001\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural Network with LSTM core\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_features, hidden_size=64, num_layers=2):\n",
    "        super(StockLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(n_features, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, 2)\n",
    "        self.sm = nn.Softmax(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (h_n, _) = self.lstm(x)\n",
    "        x = self.linear(h_n[-1])\n",
    "        probabilities = self.sm(x)\n",
    "        return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### данные на выходе из training_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\miniconda3\\envs\\hw_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\ProgramData\\miniconda3\\envs\\hw_env\\lib\\site-packages\\backtesting\\_plotting.py:55: UserWarning: Jupyter Notebook detected. Setting Bokeh output to notebook. This may not work in Jupyter clients without JavaScript support, such as old IDEs. Reset with `backtesting.set_bokeh_output(notebook=False)`.\n",
      "  warnings.warn('Jupyter Notebook detected. '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "    <style>\n",
       "        .bk-notebook-logo {\n",
       "            display: block;\n",
       "            width: 20px;\n",
       "            height: 20px;\n",
       "            background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAABx0RVh0U29mdHdhcmUAQWRvYmUgRmlyZXdvcmtzIENTNui8sowAAAOkSURBVDiNjZRtaJVlGMd/1/08zzln5zjP1LWcU9N0NkN8m2CYjpgQYQXqSs0I84OLIC0hkEKoPtiH3gmKoiJDU7QpLgoLjLIQCpEsNJ1vqUOdO7ppbuec5+V+rj4ctwzd8IIbbi6u+8f1539dt3A78eXC7QizUF7gyV1fD1Yqg4JWz84yffhm0qkFqBogB9rM8tZdtwVsPUhWhGcFJngGeWrPzHm5oaMmkfEg1usvLFyc8jLRqDOMru7AyC8saQr7GG7f5fvDeH7Ej8CM66nIF+8yngt6HWaKh7k49Soy9nXurCi1o3qUbS3zWfrYeQDTB/Qj6kX6Ybhw4B+bOYoLKCC9H3Nu/leUTZ1JdRWkkn2ldcCamzrcf47KKXdAJllSlxAOkRgyHsGC/zRday5Qld9DyoM4/q/rUoy/CXh3jzOu3bHUVZeU+DEn8FInkPBFlu3+nW3Nw0mk6vCDiWg8CeJaxEwuHS3+z5RgY+YBR6V1Z1nxSOfoaPa4LASWxxdNp+VWTk7+4vzaou8v8PN+xo+KY2xsw6une2frhw05CTYOmQvsEhjhWjn0bmXPjpE1+kplmmkP3suftwTubK9Vq22qKmrBhpY4jvd5afdRA3wGjFAgcnTK2s4hY0/GPNIb0nErGMCRxWOOX64Z8RAC4oCXdklmEvcL8o0BfkNK4lUg9HTl+oPlQxdNo3Mg4Nv175e/1LDGzZen30MEjRUtmXSfiTVu1kK8W4txyV6BMKlbgk3lMwYCiusNy9fVfvvwMxv8Ynl6vxoByANLTWplvuj/nF9m2+PDtt1eiHPBr1oIfhCChQMBw6Aw0UulqTKZdfVvfG7VcfIqLG9bcldL/+pdWTLxLUy8Qq38heUIjh4XlzZxzQm19lLFlr8vdQ97rjZVOLf8nclzckbcD4wxXMidpX30sFd37Fv/GtwwhzhxGVAprjbg0gCAEeIgwCZyTV2Z1REEW8O4py0wsjeloKoMr6iCY6dP92H6Vw/oTyICIthibxjm/DfN9lVz8IqtqKYLUXfoKVMVQVVJOElGjrnnUt9T9wbgp8AyYKaGlqingHZU/uG2NTZSVqwHQTWkx9hxjkpWDaCg6Ckj5qebgBVbT3V3NNXMSiWSDdGV3hrtzla7J+duwPOToIg42ChPQOQjspnSlp1V+Gjdged7+8UN5CRAV7a5EdFNwCjEaBR27b3W890TE7g24NAP/mMDXRWrGoFPQI9ls/MWO2dWFAar/xcOIImbbpA3zgAAAABJRU5ErkJggg==);\n",
       "        }\n",
       "    </style>\n",
       "    <div>\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-notebook-logo\"></a>\n",
       "        <span id=\"b34b5b20-1234-43bf-a730-7bef0b9c324a\">Loading BokehJS ...</span>\n",
       "    </div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "'use strict';\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\nconst JS_MIME_TYPE = 'application/javascript';\n  const HTML_MIME_TYPE = 'text/html';\n  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n  const CLASS_NAME = 'output_bokeh rendered_html';\n\n  /**\n   * Render data to the DOM node\n   */\n  function render(props, node) {\n    const script = document.createElement(\"script\");\n    node.appendChild(script);\n  }\n\n  /**\n   * Handle when an output is cleared or removed\n   */\n  function handleClearOutput(event, handle) {\n    function drop(id) {\n      const view = Bokeh.index.get_by_id(id)\n      if (view != null) {\n        view.model.document.clear()\n        Bokeh.index.delete(view)\n      }\n    }\n\n    const cell = handle.cell;\n\n    const id = cell.output_area._bokeh_element_id;\n    const server_id = cell.output_area._bokeh_server_id;\n\n    // Clean up Bokeh references\n    if (id != null) {\n      drop(id)\n    }\n\n    if (server_id !== undefined) {\n      // Clean up Bokeh references\n      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n      cell.notebook.kernel.execute(cmd_clean, {\n        iopub: {\n          output: function(msg) {\n            const id = msg.content.text.trim()\n            drop(id)\n          }\n        }\n      });\n      // Destroy server and session\n      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n      cell.notebook.kernel.execute(cmd_destroy);\n    }\n  }\n\n  /**\n   * Handle when a new output is added\n   */\n  function handleAddOutput(event, handle) {\n    const output_area = handle.output_area;\n    const output = handle.output;\n\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n      return\n    }\n\n    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n      // store reference to embed id on output_area\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n    }\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n      const bk_div = document.createElement(\"div\");\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n      const script_attrs = bk_div.children[0].attributes;\n      for (let i = 0; i < script_attrs.length; i++) {\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n      }\n      // store reference to server id on output_area\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n    }\n  }\n\n  function register_renderer(events, OutputArea) {\n\n    function append_mime(data, metadata, element) {\n      // create a DOM node to render to\n      const toinsert = this.create_output_subarea(\n        metadata,\n        CLASS_NAME,\n        EXEC_MIME_TYPE\n      );\n      this.keyboard_manager.register_events(toinsert);\n      // Render to node\n      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n      render(props, toinsert[toinsert.length - 1]);\n      element.append(toinsert);\n      return toinsert\n    }\n\n    /* Handle when an output is cleared or removed */\n    events.on('clear_output.CodeCell', handleClearOutput);\n    events.on('delete.Cell', handleClearOutput);\n\n    /* Handle when a new output is added */\n    events.on('output_added.OutputArea', handleAddOutput);\n\n    /**\n     * Register the mime type and append_mime function with output_area\n     */\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n      /* Is output safe? */\n      safe: true,\n      /* Index of renderer in `output_area.display_order` */\n      index: 0\n    });\n  }\n\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\n  if (root.Jupyter !== undefined) {\n    const events = require('base/js/events');\n    const OutputArea = require('notebook/js/outputarea').OutputArea;\n\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  }\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded(error = null) {\n    const el = document.getElementById(\"b34b5b20-1234-43bf-a730-7bef0b9c324a\");\n    if (el != null) {\n      const html = (() => {\n        if (typeof root.Bokeh === \"undefined\") {\n          if (error == null) {\n            return \"BokehJS is loading ...\";\n          } else {\n            return \"BokehJS failed to load.\";\n          }\n        } else {\n          const prefix = `BokehJS ${root.Bokeh.version}`;\n          if (error == null) {\n            return `${prefix} successfully loaded.`;\n          } else {\n            return `${prefix} <b>encountered errors</b> while loading and may not function as expected.`;\n          }\n        }\n      })();\n      el.innerHTML = html;\n\n      if (error != null) {\n        const wrapper = document.createElement(\"div\");\n        wrapper.style.overflow = \"auto\";\n        wrapper.style.height = \"5em\";\n        wrapper.style.resize = \"vertical\";\n        const content = document.createElement(\"div\");\n        content.style.fontFamily = \"monospace\";\n        content.style.whiteSpace = \"pre-wrap\";\n        content.style.backgroundColor = \"rgb(255, 221, 221)\";\n        content.textContent = error.stack ?? error.toString();\n        wrapper.append(content);\n        el.append(wrapper);\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(() => display_loaded(error), 100);\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.7.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.7.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.7.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.7.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.7.2.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n      try {\n            for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n\n      } catch (error) {display_loaded(error);throw error;\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"b34b5b20-1234-43bf-a730-7bef0b9c324a\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));",
      "application/vnd.bokehjs_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Относительные ссылки, включая импорты, относительно корневой папки проекта\n",
    "import os\n",
    "\n",
    "os.chdir(os.path.dirname(os.getcwd()))\n",
    "\n",
    "import main\n",
    "import logging\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from src.core import utils\n",
    "\n",
    "# initialize\n",
    "logger = logging.getLogger()\n",
    "# initialize config dict\n",
    "config = main.main_launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params for testing\n",
    "tickers = [\"^GSPC\"]\n",
    "interval = config.INTERVAL\n",
    "\n",
    "# Params for train-test-valid split\n",
    "# TEST - Q4'24 | VAL - Q1'25\n",
    "train_start = \"2020-01-01\"\n",
    "train_end = \"2024-10-01\"\n",
    "test_end = \"2025-01-01\"\n",
    "valid_end = \"2025-04-01\"  # захватим первый квартал 2025, тестовая выборка по длине такая же как валидационная - 3мес\n",
    "\n",
    "# Params inside Optimizer\n",
    "early_stopping_rounds = 50\n",
    "n_trials = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Список моделей и вариантов их гиперпараметров для тестирования\n",
    "# TODO: ML модели появятся здесь\n",
    "model_options = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO   ] 2025-05-14@16:47:40: Getting preprocessed history from local cache DB...\n",
      "[INFO   ] 2025-05-14@16:47:42: Got history of shape (1854, 8), 0 NaNs\n",
      "[INFO   ] 2025-05-14@16:47:43: Parsed features from JSON to separate columns: (1854, 302), 0 NaNs\n",
      "[INFO   ] 2025-05-14@16:47:43: Adding binary target...\n",
      "[INFO   ] 2025-05-14@16:47:43: Target added: (1854, 303), 0 NaNs\n",
      "[INFO   ] 2025-05-14@16:47:43: ~ ~ ~ Modelling for ^GSPC ~ ~ ~\n",
      "[INFO   ] 2025-05-14@16:47:43: Splitting ticker data to train/test/validation parts\n",
      "[INFO   ] 2025-05-14@16:47:43: X_train.shape=(1672, 296) | y_train.shape=(1672,) || X_test.shape=(92, 296) | y_test.shape=(92,) || X_val.shape=(90, 296) | y_val.shape=(90,)\n",
      "[INFO   ] 2025-05-14@16:47:43: Scaling features...\n"
     ]
    }
   ],
   "source": [
    "# Часть логики training_loop - получим в блокноте те же переменные что и при инициализации optimizer.ModelOptimizer\n",
    "# Get preprocessed data\n",
    "data, features = utils.get_preprocessed_history(\n",
    "    tickers=tickers, start=train_start, end=valid_end, interval=interval\n",
    ")\n",
    "\n",
    "# Add feature column\n",
    "data = utils.add_target(data)\n",
    "\n",
    "# Loop over tickers in dataset\n",
    "grand_result = []\n",
    "for ticker in data[\"Ticker\"].unique():\n",
    "    logger.info(f\"~ ~ ~ Modelling for {ticker} ~ ~ ~\")\n",
    "    ticker_data = data[data[\"Ticker\"] == ticker].reset_index(drop=True)\n",
    "\n",
    "    # Split dataset into parts\n",
    "    X_train, y_train, X_test, y_test, X_val, y_val = utils.train_test_valid_split(\n",
    "        ticker_data,\n",
    "        train_start=train_start,\n",
    "        train_end=train_end,\n",
    "        test_end=test_end,\n",
    "        valid_end=valid_end,\n",
    "        drop_leaky=True,\n",
    "        target_col=\"target\",\n",
    "    )\n",
    "    logger.info(\n",
    "        f\"{X_train.shape=} | {y_train.shape=} || {X_test.shape=} | {y_test.shape=} || {X_val.shape=} | {y_val.shape=}\"\n",
    "    )\n",
    "\n",
    "    # Scale train / test / validation datasets - fit on train\n",
    "    logger.info(\"Scaling features...\")\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train[features])\n",
    "    X_test_scaled = scaler.transform(X_test[features])\n",
    "    X_val_scaled = scaler.transform(X_val[features])\n",
    "\n",
    "    break\n",
    "\n",
    "    # # Для каждого потенциального типа модели:\n",
    "    # for model_type, param_dict in model_options.items():\n",
    "    #     logger.info(f\"~ ~ Iteration for {model_type.__name__} ~ ~\")\n",
    "    #     # Тюним гиперпараметры на train / test датасетах, выбираем лучшее\n",
    "    #     model_optimizer = optimizer.ModelOptimizer(\n",
    "    #         model_type,\n",
    "    #         param_dict,\n",
    "    #         X_train_scaled,\n",
    "    #         y_train,\n",
    "    #         X_test_scaled,\n",
    "    #         y_test,\n",
    "    #         X_val_scaled,\n",
    "    #         y_val,\n",
    "    #     )\n",
    "    #     (\n",
    "    #         model,\n",
    "    #         best_params,\n",
    "    #         (train_roc_auc, test_roc_auc, val_roc_auc),\n",
    "    #         (train_metrics_table, test_metrics_table, val_metrics_table),\n",
    "    #     ) = model_optimizer.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.lstm_model import LSTMModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO   ] 2025-05-14@16:49:41: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "d:\\Otus\\ML-Finance-Bot\\app\\src\\models\\lstm_model.py:51: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:257.)\n",
      "  X = torch.tensor(X, dtype=torch.float32)\n",
      "[INFO   ] 2025-05-14@16:49:43: Initializing model...\n",
      "[INFO   ] 2025-05-14@16:49:43: Training model...\n",
      "[INFO   ] 2025-05-14@16:49:46: Epoch [1/10], Loss: 0.6919\n",
      "[INFO   ] 2025-05-14@16:49:47: Epoch [2/10], Loss: 0.6638\n",
      "[INFO   ] 2025-05-14@16:49:47: Epoch [3/10], Loss: 0.5824\n",
      "[INFO   ] 2025-05-14@16:49:48: Epoch [4/10], Loss: 0.6729\n",
      "[INFO   ] 2025-05-14@16:49:49: Epoch [5/10], Loss: 0.7782\n",
      "[INFO   ] 2025-05-14@16:49:49: Epoch [6/10], Loss: 0.5621\n",
      "[INFO   ] 2025-05-14@16:49:50: Epoch [7/10], Loss: 0.5408\n",
      "[INFO   ] 2025-05-14@16:49:51: Epoch [8/10], Loss: 0.6467\n",
      "[INFO   ] 2025-05-14@16:49:52: Epoch [9/10], Loss: 0.5997\n",
      "[INFO   ] 2025-05-14@16:49:53: Epoch [10/10], Loss: 0.5819\n"
     ]
    }
   ],
   "source": [
    "# Instantiate and train model\n",
    "model = LSTMModel()\n",
    "_ = model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO   ] 2025-05-14@16:51:28: Converting numpy arrays to TensorDataset and DataLoader...\n"
     ]
    }
   ],
   "source": [
    "# predict_proba\n",
    "X_raw = X_train_scaled\n",
    "\n",
    "dataloader = convert_to_dataloader(\n",
    "    X_raw, shuffle=False\n",
    ")  # !!! turn off shuffle to keep prediction indices correct\n",
    "_ = model.model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1672, 295)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for inputs, _ in dataloader:\n",
    "        logits = model.model(inputs)\n",
    "        predictions.extend(logits.numpy())\n",
    "\n",
    "predictions = np.stack(predictions, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1671"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape[0] + window_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0212, 0.9788],\n",
       "        [0.0175, 0.9825],\n",
       "        [0.0293, 0.9707],\n",
       "        [0.0735, 0.9265],\n",
       "        [0.2012, 0.7988],\n",
       "        [0.4405, 0.5595],\n",
       "        [0.6335, 0.3665],\n",
       "        [0.7371, 0.2629],\n",
       "        [0.7108, 0.2892],\n",
       "        [0.6209, 0.3791],\n",
       "        [0.3972, 0.6028],\n",
       "        [0.2251, 0.7749],\n",
       "        [0.1547, 0.8453],\n",
       "        [0.1684, 0.8316],\n",
       "        [0.2640, 0.7360],\n",
       "        [0.5010, 0.4990],\n",
       "        [0.7168, 0.2832],\n",
       "        [0.7940, 0.2060],\n",
       "        [0.7546, 0.2454],\n",
       "        [0.5976, 0.4024],\n",
       "        [0.3599, 0.6401],\n",
       "        [0.2062, 0.7938],\n",
       "        [0.1290, 0.8710],\n",
       "        [0.1020, 0.8980],\n",
       "        [0.1026, 0.8974],\n",
       "        [0.1209, 0.8791],\n",
       "        [0.1730, 0.8270],\n",
       "        [0.3945, 0.6055],\n",
       "        [0.6243, 0.3757],\n",
       "        [0.6526, 0.3474],\n",
       "        [0.4549, 0.5451],\n",
       "        [0.1939, 0.8061]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### что внутри"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Потенциальные параметры для тюнинга\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "window_size = 30\n",
    "\n",
    "batch_size = 32\n",
    "lr = 0.001\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_dataloader(X_raw, y_raw=None, shuffle: bool = True):\n",
    "    \"\"\"\n",
    "    Adapter to be used inside NN models\n",
    "    \"\"\"\n",
    "    logger.info(\"Converting numpy arrays to TensorDataset and DataLoader...\")\n",
    "    # Convert to PyTorch Tensors\n",
    "    X, y = [], []\n",
    "    for i in range(len(X_raw) - window_size - 1):\n",
    "        X.append(X_raw[i : i + window_size])\n",
    "        if y_raw is not None:\n",
    "            y.append(y_raw[i + window_size])\n",
    "        else:\n",
    "            y.append(0)\n",
    "\n",
    "    # Now convert lists to tensors\n",
    "    X = torch.tensor(X, dtype=torch.float32)\n",
    "    y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    # DataLoader\n",
    "    dataset = TensorDataset(X, y)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(dataloader) -> None:\n",
    "    \"\"\"\n",
    "    NN model training loop\n",
    "    \"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Switch model to train mode\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for inputs, labels in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        logger.info(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural Network with LSTM core\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_features, hidden_size=64, num_layers=2):\n",
    "        super(StockLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(n_features, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, 2)\n",
    "        self.sm = nn.Softmax(0)\n",
    "\n",
    "        # self.lstm_stack = nn.Sequential(\n",
    "        #     nn.LSTM(n_features, hidden_size, num_layers, batch_first=True),\n",
    "        #     nn.Linear(hidden_size, 2),\n",
    "        #     nn.Softmax(1),\n",
    "        # )\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (h_n, _) = self.lstm(x)\n",
    "        x = self.linear(h_n[-1])\n",
    "        probabilities = self.sm(x)\n",
    "        return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO   ] 2025-05-14@16:36:21: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "C:\\Users\\Pavel\\AppData\\Local\\Temp\\ipykernel_5952\\492921307.py:16: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:257.)\n",
      "  X = torch.tensor(X, dtype=torch.float32)\n",
      "[INFO   ] 2025-05-14@16:36:23: Initializing model...\n",
      "[INFO   ] 2025-05-14@16:36:23: Training model...\n",
      "[INFO   ] 2025-05-14@16:36:26: Epoch [1/10], Loss: 0.6771\n",
      "[INFO   ] 2025-05-14@16:36:26: Epoch [2/10], Loss: 0.7022\n",
      "[INFO   ] 2025-05-14@16:36:27: Epoch [3/10], Loss: 0.6825\n",
      "[INFO   ] 2025-05-14@16:36:28: Epoch [4/10], Loss: 0.6957\n",
      "[INFO   ] 2025-05-14@16:36:28: Epoch [5/10], Loss: 0.6150\n",
      "[INFO   ] 2025-05-14@16:36:29: Epoch [6/10], Loss: 0.8118\n",
      "[INFO   ] 2025-05-14@16:36:30: Epoch [7/10], Loss: 0.6225\n",
      "[INFO   ] 2025-05-14@16:36:30: Epoch [8/10], Loss: 0.6198\n",
      "[INFO   ] 2025-05-14@16:36:31: Epoch [9/10], Loss: 0.6750\n",
      "[INFO   ] 2025-05-14@16:36:32: Epoch [10/10], Loss: 0.7148\n"
     ]
    }
   ],
   "source": [
    "# ~ ~ ~ Inside fit() ~ ~ ~\n",
    "# Convert data\n",
    "dataloader = convert_to_dataloader(X_raw=X_train_scaled, y_raw=y_train)\n",
    "# Initialize model\n",
    "logger.info(\"Initializing model...\")\n",
    "model = StockLSTM(\n",
    "    n_features=X_train_scaled.shape[1],\n",
    "    hidden_size=hidden_size,\n",
    "    num_layers=num_layers,\n",
    ")\n",
    "# Train it\n",
    "logger.info(\"Training model...\")\n",
    "train_model(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Switch model to train mode\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels in dataloader:\n",
    "        break\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # logger.info(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = X_train_scaled.shape[1]\n",
    "hidden_size = 64\n",
    "num_layers = 1 # 2\n",
    "\n",
    "x = nn.LSTM(n_features, hidden_size, num_layers, batch_first=True)(inputs)[1][0][-1]\n",
    "x = nn.Linear(hidden_size, 2)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4619, 0.5381],\n",
       "        [0.4585, 0.5415],\n",
       "        [0.4264, 0.5736],\n",
       "        [0.5144, 0.4856],\n",
       "        [0.4870, 0.5130],\n",
       "        [0.3743, 0.6257],\n",
       "        [0.5103, 0.4897],\n",
       "        [0.4445, 0.5555],\n",
       "        [0.4741, 0.5259],\n",
       "        [0.5572, 0.4428],\n",
       "        [0.4127, 0.5873],\n",
       "        [0.4987, 0.5013],\n",
       "        [0.3913, 0.6087],\n",
       "        [0.4416, 0.5584],\n",
       "        [0.3837, 0.6163],\n",
       "        [0.5425, 0.4575],\n",
       "        [0.4540, 0.5460],\n",
       "        [0.4937, 0.5063],\n",
       "        [0.3943, 0.6057],\n",
       "        [0.5033, 0.4967],\n",
       "        [0.4250, 0.5750],\n",
       "        [0.4592, 0.5408],\n",
       "        [0.4407, 0.5593],\n",
       "        [0.4398, 0.5602],\n",
       "        [0.3590, 0.6410],\n",
       "        [0.4161, 0.5839],\n",
       "        [0.4121, 0.5879],\n",
       "        [0.3904, 0.6096],\n",
       "        [0.4787, 0.5213],\n",
       "        [0.3983, 0.6017],\n",
       "        [0.3792, 0.6208],\n",
       "        [0.3777, 0.6223]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Softmax(1)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch (got input: [2], target: [32])",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# optimizer.zero_grad()\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# outputs = model(inputs)\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\hw_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\hw_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\hw_env\\lib\\site-packages\\torch\\nn\\modules\\loss.py:1297\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1296\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1297\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1298\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1299\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1300\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1301\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1302\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1303\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1304\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\hw_env\\lib\\site-packages\\torch\\nn\\functional.py:3494\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3492\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3493\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3494\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3495\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3496\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3499\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3500\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3501\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: size mismatch (got input: [2], target: [32])"
     ]
    }
   ],
   "source": [
    "# optimizer.zero_grad()\n",
    "# outputs = model(inputs)\n",
    "criterion(outputs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1672, 295)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 295])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = X_train_scaled.shape[1]\n",
    "hidden_size=64\n",
    "num_layers=2\n",
    "\n",
    "lstm = nn.LSTM(X_train_scaled.shape[1], hidden_size, num_layers)\n",
    "linear = nn.Linear(hidden_size, 2)\n",
    "sm = nn.Softmax(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for inputs, labels in dataloader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = inputs\n",
    "\n",
    "_, (h_n, _) = lstm(x)\n",
    "x = linear(h_n[-1])  # last LSTM output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4792, 0.5208], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Softmax(0)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0059,  0.0773], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43msm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\hw_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\hw_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\hw_env\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1672\u001b[0m, in \u001b[0;36mSoftmax.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m   1671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1672\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacklevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\hw_env\\lib\\site-packages\\torch\\nn\\functional.py:2140\u001b[0m, in \u001b[0;36msoftmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   2138\u001b[0m     dim \u001b[38;5;241m=\u001b[39m _get_softmax_dim(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim(), _stacklevel)\n\u001b[0;32m   2139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 2140\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2141\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2142\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(dim, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[1;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "sm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0059,  0.0773], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\miniconda3\\envs\\hw_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\ProgramData\\miniconda3\\envs\\hw_env\\lib\\site-packages\\backtesting\\_plotting.py:55: UserWarning: Jupyter Notebook detected. Setting Bokeh output to notebook. This may not work in Jupyter clients without JavaScript support, such as old IDEs. Reset with `backtesting.set_bokeh_output(notebook=False)`.\n",
      "  warnings.warn('Jupyter Notebook detected. '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "    <style>\n",
       "        .bk-notebook-logo {\n",
       "            display: block;\n",
       "            width: 20px;\n",
       "            height: 20px;\n",
       "            background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAABx0RVh0U29mdHdhcmUAQWRvYmUgRmlyZXdvcmtzIENTNui8sowAAAOkSURBVDiNjZRtaJVlGMd/1/08zzln5zjP1LWcU9N0NkN8m2CYjpgQYQXqSs0I84OLIC0hkEKoPtiH3gmKoiJDU7QpLgoLjLIQCpEsNJ1vqUOdO7ppbuec5+V+rj4ctwzd8IIbbi6u+8f1539dt3A78eXC7QizUF7gyV1fD1Yqg4JWz84yffhm0qkFqBogB9rM8tZdtwVsPUhWhGcFJngGeWrPzHm5oaMmkfEg1usvLFyc8jLRqDOMru7AyC8saQr7GG7f5fvDeH7Ej8CM66nIF+8yngt6HWaKh7k49Soy9nXurCi1o3qUbS3zWfrYeQDTB/Qj6kX6Ybhw4B+bOYoLKCC9H3Nu/leUTZ1JdRWkkn2ldcCamzrcf47KKXdAJllSlxAOkRgyHsGC/zRday5Qld9DyoM4/q/rUoy/CXh3jzOu3bHUVZeU+DEn8FInkPBFlu3+nW3Nw0mk6vCDiWg8CeJaxEwuHS3+z5RgY+YBR6V1Z1nxSOfoaPa4LASWxxdNp+VWTk7+4vzaou8v8PN+xo+KY2xsw6une2frhw05CTYOmQvsEhjhWjn0bmXPjpE1+kplmmkP3suftwTubK9Vq22qKmrBhpY4jvd5afdRA3wGjFAgcnTK2s4hY0/GPNIb0nErGMCRxWOOX64Z8RAC4oCXdklmEvcL8o0BfkNK4lUg9HTl+oPlQxdNo3Mg4Nv175e/1LDGzZen30MEjRUtmXSfiTVu1kK8W4txyV6BMKlbgk3lMwYCiusNy9fVfvvwMxv8Ynl6vxoByANLTWplvuj/nF9m2+PDtt1eiHPBr1oIfhCChQMBw6Aw0UulqTKZdfVvfG7VcfIqLG9bcldL/+pdWTLxLUy8Qq38heUIjh4XlzZxzQm19lLFlr8vdQ97rjZVOLf8nclzckbcD4wxXMidpX30sFd37Fv/GtwwhzhxGVAprjbg0gCAEeIgwCZyTV2Z1REEW8O4py0wsjeloKoMr6iCY6dP92H6Vw/oTyICIthibxjm/DfN9lVz8IqtqKYLUXfoKVMVQVVJOElGjrnnUt9T9wbgp8AyYKaGlqingHZU/uG2NTZSVqwHQTWkx9hxjkpWDaCg6Ckj5qebgBVbT3V3NNXMSiWSDdGV3hrtzla7J+duwPOToIg42ChPQOQjspnSlp1V+Gjdged7+8UN5CRAV7a5EdFNwCjEaBR27b3W890TE7g24NAP/mMDXRWrGoFPQI9ls/MWO2dWFAar/xcOIImbbpA3zgAAAABJRU5ErkJggg==);\n",
       "        }\n",
       "    </style>\n",
       "    <div>\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-notebook-logo\"></a>\n",
       "        <span id=\"d58b005a-680c-4a9c-acf3-8712b9697605\">Loading BokehJS ...</span>\n",
       "    </div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "'use strict';\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\nconst JS_MIME_TYPE = 'application/javascript';\n  const HTML_MIME_TYPE = 'text/html';\n  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n  const CLASS_NAME = 'output_bokeh rendered_html';\n\n  /**\n   * Render data to the DOM node\n   */\n  function render(props, node) {\n    const script = document.createElement(\"script\");\n    node.appendChild(script);\n  }\n\n  /**\n   * Handle when an output is cleared or removed\n   */\n  function handleClearOutput(event, handle) {\n    function drop(id) {\n      const view = Bokeh.index.get_by_id(id)\n      if (view != null) {\n        view.model.document.clear()\n        Bokeh.index.delete(view)\n      }\n    }\n\n    const cell = handle.cell;\n\n    const id = cell.output_area._bokeh_element_id;\n    const server_id = cell.output_area._bokeh_server_id;\n\n    // Clean up Bokeh references\n    if (id != null) {\n      drop(id)\n    }\n\n    if (server_id !== undefined) {\n      // Clean up Bokeh references\n      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n      cell.notebook.kernel.execute(cmd_clean, {\n        iopub: {\n          output: function(msg) {\n            const id = msg.content.text.trim()\n            drop(id)\n          }\n        }\n      });\n      // Destroy server and session\n      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n      cell.notebook.kernel.execute(cmd_destroy);\n    }\n  }\n\n  /**\n   * Handle when a new output is added\n   */\n  function handleAddOutput(event, handle) {\n    const output_area = handle.output_area;\n    const output = handle.output;\n\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n      return\n    }\n\n    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n      // store reference to embed id on output_area\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n    }\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n      const bk_div = document.createElement(\"div\");\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n      const script_attrs = bk_div.children[0].attributes;\n      for (let i = 0; i < script_attrs.length; i++) {\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n      }\n      // store reference to server id on output_area\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n    }\n  }\n\n  function register_renderer(events, OutputArea) {\n\n    function append_mime(data, metadata, element) {\n      // create a DOM node to render to\n      const toinsert = this.create_output_subarea(\n        metadata,\n        CLASS_NAME,\n        EXEC_MIME_TYPE\n      );\n      this.keyboard_manager.register_events(toinsert);\n      // Render to node\n      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n      render(props, toinsert[toinsert.length - 1]);\n      element.append(toinsert);\n      return toinsert\n    }\n\n    /* Handle when an output is cleared or removed */\n    events.on('clear_output.CodeCell', handleClearOutput);\n    events.on('delete.Cell', handleClearOutput);\n\n    /* Handle when a new output is added */\n    events.on('output_added.OutputArea', handleAddOutput);\n\n    /**\n     * Register the mime type and append_mime function with output_area\n     */\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n      /* Is output safe? */\n      safe: true,\n      /* Index of renderer in `output_area.display_order` */\n      index: 0\n    });\n  }\n\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\n  if (root.Jupyter !== undefined) {\n    const events = require('base/js/events');\n    const OutputArea = require('notebook/js/outputarea').OutputArea;\n\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  }\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded(error = null) {\n    const el = document.getElementById(\"d58b005a-680c-4a9c-acf3-8712b9697605\");\n    if (el != null) {\n      const html = (() => {\n        if (typeof root.Bokeh === \"undefined\") {\n          if (error == null) {\n            return \"BokehJS is loading ...\";\n          } else {\n            return \"BokehJS failed to load.\";\n          }\n        } else {\n          const prefix = `BokehJS ${root.Bokeh.version}`;\n          if (error == null) {\n            return `${prefix} successfully loaded.`;\n          } else {\n            return `${prefix} <b>encountered errors</b> while loading and may not function as expected.`;\n          }\n        }\n      })();\n      el.innerHTML = html;\n\n      if (error != null) {\n        const wrapper = document.createElement(\"div\");\n        wrapper.style.overflow = \"auto\";\n        wrapper.style.height = \"5em\";\n        wrapper.style.resize = \"vertical\";\n        const content = document.createElement(\"div\");\n        content.style.fontFamily = \"monospace\";\n        content.style.whiteSpace = \"pre-wrap\";\n        content.style.backgroundColor = \"rgb(255, 221, 221)\";\n        content.textContent = error.stack ?? error.toString();\n        wrapper.append(content);\n        el.append(wrapper);\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(() => display_loaded(error), 100);\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.7.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.7.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.7.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.7.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.7.2.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n      try {\n            for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n\n      } catch (error) {display_loaded(error);throw error;\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"d58b005a-680c-4a9c-acf3-8712b9697605\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));",
      "application/vnd.bokehjs_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Относительные ссылки, включая импорты, относительно корневой папки проекта\n",
    "import os\n",
    "\n",
    "os.chdir(os.path.dirname(os.getcwd()))\n",
    "\n",
    "import main\n",
    "import logging\n",
    "\n",
    "from src.models.training_loop import ml_model_strategy_training_loop\n",
    "from src.models.lstm_model import LSTMModel\n",
    "\n",
    "# initialize\n",
    "logger = logging.getLogger()\n",
    "# initialize config dict\n",
    "config = main.main_launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params for testing\n",
    "tickers = [\"^GSPC\"]\n",
    "interval = config.INTERVAL\n",
    "\n",
    "# Params for train-test-valid split\n",
    "# TEST - Q4'24 | VAL - Q1'25\n",
    "train_start = \"2020-01-01\"\n",
    "train_end = \"2024-10-01\"\n",
    "test_end = \"2025-01-01\"\n",
    "valid_end = \"2025-04-01\"  # захватим первый квартал 2025, тестовая выборка по длине такая же как валидационная - 3мес\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Потенциальные параметры для тюнинга\n",
    "model_options = {\n",
    "    LSTMModel: {\n",
    "        \"int\": {\n",
    "            \"window_size\": {\"low\": 5, \"high\": 60},\n",
    "            \"hidden_size\": {\"low\": 8, \"high\": 128},\n",
    "            \"num_layers\": {\"low\": 1, \"high\": 4},\n",
    "            \"batch_size\": {\"low\": 4, \"high\": 64},\n",
    "        },\n",
    "        \"float\": {\"lr\": {\"low\": 0.0005, \"high\": 0.01}},\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO   ] 2025-05-14@16:56:04: Getting preprocessed history from local cache DB...\n",
      "[INFO   ] 2025-05-14@16:56:06: Got history of shape (1854, 8), 0 NaNs\n",
      "[INFO   ] 2025-05-14@16:56:07: Parsed features from JSON to separate columns: (1854, 302), 0 NaNs\n",
      "[INFO   ] 2025-05-14@16:56:07: Adding binary target...\n",
      "[INFO   ] 2025-05-14@16:56:07: Target added: (1854, 303), 0 NaNs\n",
      "[INFO   ] 2025-05-14@16:56:07: ~ ~ ~ Modelling for ^GSPC ~ ~ ~\n",
      "[INFO   ] 2025-05-14@16:56:07: Splitting ticker data to train/test/validation parts\n",
      "[INFO   ] 2025-05-14@16:56:07: X_train.shape=(1672, 296) | y_train.shape=(1672,) || X_test.shape=(92, 296) | y_test.shape=(92,) || X_val.shape=(90, 296) | y_val.shape=(90,)\n",
      "[INFO   ] 2025-05-14@16:56:07: Scaling features...\n",
      "[INFO   ] 2025-05-14@16:56:07: ~ ~ Iteration for LSTMModel ~ ~\n",
      "[INFO   ] 2025-05-14@16:56:07: Searching for best hyperparameters using Optuna...\n",
      "[I 2025-05-14 13:56:07,577] A new study created in memory with name: no-name-e62cbe4e-1109-43e7-9115-57f9acf33ac0\n",
      "[INFO   ] 2025-05-14@16:56:07: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "d:\\Otus\\ML-Finance-Bot\\app\\src\\models\\lstm_model.py:51: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:257.)\n",
      "  X = torch.tensor(X, dtype=torch.float32)\n",
      "[INFO   ] 2025-05-14@16:56:07: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-14@16:56:10: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-14@16:56:10: Initializing model...\n",
      "[INFO   ] 2025-05-14@16:56:10: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-14@16:56:11: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-14@16:56:11: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-14@16:56:11: Initializing model...\n",
      "[INFO   ] 2025-05-14@16:56:14: Initializing model...\n",
      "[INFO   ] 2025-05-14@16:56:14: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-14@16:56:14: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-14@16:56:20: Initializing model...\n",
      "[INFO   ] 2025-05-14@16:56:20: Initializing model...\n",
      "[INFO   ] 2025-05-14@16:56:21: Initializing model...\n",
      "[INFO   ] 2025-05-14@16:56:21: Training model...\n",
      "[INFO   ] 2025-05-14@16:56:24: Initializing model...\n",
      "[INFO   ] 2025-05-14@16:56:25: Initializing model...\n",
      "[INFO   ] 2025-05-14@16:56:25: Training model...\n",
      "[INFO   ] 2025-05-14@16:56:25: Training model...\n",
      "[INFO   ] 2025-05-14@16:56:25: Training model...\n",
      "[INFO   ] 2025-05-14@16:56:25: Training model...\n",
      "[INFO   ] 2025-05-14@16:56:25: Training model...\n",
      "[INFO   ] 2025-05-14@16:56:25: Training model...\n",
      "[INFO   ] 2025-05-14@16:56:25: Training model...\n",
      "[INFO   ] 2025-05-14@16:56:31: Epoch [1/10], Loss: 0.6453\n",
      "[INFO   ] 2025-05-14@16:56:32: Epoch [1/10], Loss: 0.6306\n",
      "[INFO   ] 2025-05-14@16:56:32: Epoch [1/10], Loss: 0.6992\n",
      "[INFO   ] 2025-05-14@16:56:32: Epoch [1/10], Loss: 0.6758\n",
      "[INFO   ] 2025-05-14@16:56:33: Epoch [2/10], Loss: 0.6620\n",
      "[INFO   ] 2025-05-14@16:56:33: Epoch [1/10], Loss: 0.7331\n",
      "[INFO   ] 2025-05-14@16:56:33: Epoch [1/10], Loss: 0.5795\n",
      "[INFO   ] 2025-05-14@16:56:34: Epoch [2/10], Loss: 0.7245\n",
      "[INFO   ] 2025-05-14@16:56:35: Epoch [3/10], Loss: 0.6664\n",
      "[INFO   ] 2025-05-14@16:56:35: Epoch [2/10], Loss: 0.6842\n",
      "[INFO   ] 2025-05-14@16:56:36: Epoch [2/10], Loss: 0.7544\n",
      "[INFO   ] 2025-05-14@16:56:37: Epoch [3/10], Loss: 0.7065\n",
      "[INFO   ] 2025-05-14@16:56:37: Epoch [4/10], Loss: 0.5827\n",
      "[INFO   ] 2025-05-14@16:56:38: Epoch [2/10], Loss: 0.7186\n",
      "[INFO   ] 2025-05-14@16:56:38: Epoch [2/10], Loss: 0.6602\n",
      "[INFO   ] 2025-05-14@16:56:39: Epoch [3/10], Loss: 0.6739\n",
      "[INFO   ] 2025-05-14@16:56:40: Epoch [5/10], Loss: 0.7126\n",
      "[INFO   ] 2025-05-14@16:56:40: Epoch [3/10], Loss: 0.6652\n",
      "[INFO   ] 2025-05-14@16:56:40: Epoch [4/10], Loss: 0.6520\n",
      "[INFO   ] 2025-05-14@16:56:41: Epoch [1/10], Loss: 0.6622\n",
      "[INFO   ] 2025-05-14@16:56:42: Epoch [6/10], Loss: 0.7501\n",
      "[INFO   ] 2025-05-14@16:56:42: Epoch [3/10], Loss: 0.5521\n",
      "[INFO   ] 2025-05-14@16:56:42: Epoch [3/10], Loss: 0.7468\n",
      "[INFO   ] 2025-05-14@16:56:43: Epoch [4/10], Loss: 0.6438\n",
      "[INFO   ] 2025-05-14@16:56:43: Epoch [5/10], Loss: 0.7294\n",
      "[INFO   ] 2025-05-14@16:56:44: Epoch [4/10], Loss: 0.6724\n",
      "[INFO   ] 2025-05-14@16:56:44: Epoch [7/10], Loss: 0.6107\n",
      "[INFO   ] 2025-05-14@16:56:47: Epoch [8/10], Loss: 0.5592\n",
      "[INFO   ] 2025-05-14@16:56:47: Epoch [6/10], Loss: 0.6825\n",
      "[INFO   ] 2025-05-14@16:56:47: Epoch [5/10], Loss: 0.7807\n",
      "[INFO   ] 2025-05-14@16:56:47: Epoch [4/10], Loss: 0.5972\n",
      "[INFO   ] 2025-05-14@16:56:48: Epoch [4/10], Loss: 0.7179\n",
      "[INFO   ] 2025-05-14@16:56:48: Epoch [5/10], Loss: 0.6558\n",
      "[INFO   ] 2025-05-14@16:56:48: Epoch [1/10], Loss: 0.7343\n",
      "[INFO   ] 2025-05-14@16:56:49: Epoch [9/10], Loss: 0.6932\n",
      "[INFO   ] 2025-05-14@16:56:50: Epoch [7/10], Loss: 0.6788\n",
      "[INFO   ] 2025-05-14@16:56:52: Epoch [6/10], Loss: 0.6816\n",
      "[INFO   ] 2025-05-14@16:56:52: Epoch [10/10], Loss: 0.5901\n",
      "[INFO   ] 2025-05-14@16:56:52: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-14@16:56:56: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[I 2025-05-14 13:56:56,845] Trial 4 finished with value: 0.12957035587971333 and parameters: {'window_size': 55, 'hidden_size': 49, 'num_layers': 1, 'batch_size': 42, 'lr': 0.0025420104958346246}. Best is trial 4 with value: 0.12957035587971333.\n",
      "[INFO   ] 2025-05-14@16:56:56: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-14@16:56:58: Initializing model...\n",
      "[INFO   ] 2025-05-14@16:56:58: Training model...\n",
      "[INFO   ] 2025-05-14@16:56:58: Epoch [6/10], Loss: 0.6066\n",
      "[INFO   ] 2025-05-14@16:56:58: Epoch [5/10], Loss: 0.5953\n",
      "[INFO   ] 2025-05-14@16:56:58: Epoch [5/10], Loss: 0.5210\n",
      "[INFO   ] 2025-05-14@16:56:59: Epoch [8/10], Loss: 0.7013\n",
      "[INFO   ] 2025-05-14@16:57:00: Epoch [2/10], Loss: 0.5268\n",
      "[INFO   ] 2025-05-14@16:57:01: Epoch [7/10], Loss: 0.7014\n",
      "[INFO   ] 2025-05-14@16:57:02: Epoch [7/10], Loss: 0.6098\n",
      "[INFO   ] 2025-05-14@16:57:02: Epoch [9/10], Loss: 0.6885\n",
      "[INFO   ] 2025-05-14@16:57:03: Epoch [6/10], Loss: 0.8347\n",
      "[INFO   ] 2025-05-14@16:57:04: Epoch [1/10], Loss: 0.6744\n",
      "[INFO   ] 2025-05-14@16:57:04: Epoch [6/10], Loss: 0.6080\n",
      "[INFO   ] 2025-05-14@16:57:06: Epoch [10/10], Loss: 0.6648\n",
      "[INFO   ] 2025-05-14@16:57:06: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-14@16:57:07: Epoch [8/10], Loss: 0.4058\n",
      "[INFO   ] 2025-05-14@16:57:08: Epoch [8/10], Loss: 0.6762\n",
      "[INFO   ] 2025-05-14@16:57:08: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[I 2025-05-14 13:57:08,554] Trial 1 finished with value: 0.44956314305127165 and parameters: {'window_size': 11, 'hidden_size': 110, 'num_layers': 3, 'batch_size': 38, 'lr': 0.007607057669435164}. Best is trial 1 with value: 0.44956314305127165.\n",
      "[INFO   ] 2025-05-14@16:57:08: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-14@16:57:10: Initializing model...\n",
      "[INFO   ] 2025-05-14@16:57:10: Training model...\n",
      "[INFO   ] 2025-05-14@16:57:11: Epoch [7/10], Loss: 0.7662\n",
      "[INFO   ] 2025-05-14@16:57:12: Epoch [7/10], Loss: 0.5679\n",
      "[INFO   ] 2025-05-14@16:57:12: Epoch [2/10], Loss: 0.7189\n",
      "[INFO   ] 2025-05-14@16:57:13: Epoch [9/10], Loss: 0.7447\n",
      "[INFO   ] 2025-05-14@16:57:14: Epoch [9/10], Loss: 0.6767\n",
      "[INFO   ] 2025-05-14@16:57:15: Epoch [8/10], Loss: 0.5708\n",
      "[INFO   ] 2025-05-14@16:57:16: Epoch [3/10], Loss: 0.6384\n",
      "[INFO   ] 2025-05-14@16:57:17: Epoch [8/10], Loss: 0.5541\n",
      "[INFO   ] 2025-05-14@16:57:17: Epoch [2/10], Loss: 0.6958\n",
      "[INFO   ] 2025-05-14@16:57:17: Epoch [10/10], Loss: 0.5860\n",
      "[INFO   ] 2025-05-14@16:57:17: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-14@16:57:20: Epoch [1/10], Loss: 0.7216\n",
      "[INFO   ] 2025-05-14@16:57:20: Epoch [3/10], Loss: 0.6764\n",
      "[INFO   ] 2025-05-14@16:57:20: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[I 2025-05-14 13:57:21,186] Trial 6 finished with value: 0.12555271116723632 and parameters: {'window_size': 40, 'hidden_size': 33, 'num_layers': 3, 'batch_size': 52, 'lr': 0.0038431592890492603}. Best is trial 1 with value: 0.44956314305127165.\n",
      "[INFO   ] 2025-05-14@16:57:21: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-14@16:57:23: Initializing model...\n",
      "[INFO   ] 2025-05-14@16:57:23: Training model...\n",
      "[INFO   ] 2025-05-14@16:57:24: Epoch [10/10], Loss: 0.7218\n",
      "[INFO   ] 2025-05-14@16:57:24: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-14@16:57:28: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-14@16:57:28: Epoch [9/10], Loss: 0.6230\n",
      "[I 2025-05-14 13:57:28,253] Trial 3 finished with value: 0.2482972765444833 and parameters: {'window_size': 38, 'hidden_size': 93, 'num_layers': 1, 'batch_size': 22, 'lr': 0.005305225054526672}. Best is trial 1 with value: 0.44956314305127165.\n",
      "[INFO   ] 2025-05-14@16:57:28: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-14@16:57:30: Initializing model...\n",
      "[INFO   ] 2025-05-14@16:57:30: Training model...\n",
      "[INFO   ] 2025-05-14@16:57:32: Epoch [1/10], Loss: 0.6899\n",
      "[INFO   ] 2025-05-14@16:57:32: Epoch [9/10], Loss: 0.7796\n",
      "[INFO   ] 2025-05-14@16:57:33: Epoch [4/10], Loss: 0.5542\n",
      "[INFO   ] 2025-05-14@16:57:33: Epoch [2/10], Loss: 0.6590\n",
      "[INFO   ] 2025-05-14@16:57:34: Epoch [1/10], Loss: 0.5987\n",
      "[INFO   ] 2025-05-14@16:57:35: Epoch [10/10], Loss: 0.8604\n",
      "[INFO   ] 2025-05-14@16:57:35: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-14@16:57:38: Epoch [2/10], Loss: 0.7480\n",
      "[INFO   ] 2025-05-14@16:57:38: Epoch [3/10], Loss: 0.6666\n",
      "[INFO   ] 2025-05-14@16:57:39: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[I 2025-05-14 13:57:39,991] Trial 0 finished with value: 0.06603861624811347 and parameters: {'window_size': 44, 'hidden_size': 43, 'num_layers': 2, 'batch_size': 30, 'lr': 0.002243960814988655}. Best is trial 1 with value: 0.44956314305127165.\n",
      "[INFO   ] 2025-05-14@16:57:39: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-14@16:57:41: Initializing model...\n",
      "[INFO   ] 2025-05-14@16:57:41: Training model...\n",
      "[INFO   ] 2025-05-14@16:57:41: Epoch [4/10], Loss: 0.6403\n",
      "[INFO   ] 2025-05-14@16:57:42: Epoch [10/10], Loss: 0.5761\n",
      "[INFO   ] 2025-05-14@16:57:42: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-14@16:57:43: Epoch [4/10], Loss: 0.7355\n",
      "[INFO   ] 2025-05-14@16:57:44: Epoch [5/10], Loss: 0.5865\n",
      "[INFO   ] 2025-05-14@16:57:44: Epoch [5/10], Loss: 0.7057\n",
      "[INFO   ] 2025-05-14@16:57:44: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[I 2025-05-14 13:57:44,774] Trial 5 finished with value: 0.2934529120283311 and parameters: {'window_size': 17, 'hidden_size': 87, 'num_layers': 2, 'batch_size': 17, 'lr': 0.00595815093900863}. Best is trial 1 with value: 0.44956314305127165.\n",
      "[INFO   ] 2025-05-14@16:57:44: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-14@16:57:45: Initializing model...\n",
      "[INFO   ] 2025-05-14@16:57:45: Training model...\n",
      "[INFO   ] 2025-05-14@16:57:46: Epoch [1/10], Loss: 0.6892\n",
      "[INFO   ] 2025-05-14@16:57:46: Epoch [6/10], Loss: 0.6016\n",
      "[INFO   ] 2025-05-14@16:57:47: Epoch [2/10], Loss: 0.7895\n",
      "[INFO   ] 2025-05-14@16:57:47: Epoch [2/10], Loss: 0.6768\n",
      "[INFO   ] 2025-05-14@16:57:48: Epoch [7/10], Loss: 0.5925\n",
      "[INFO   ] 2025-05-14@16:57:48: Epoch [3/10], Loss: 0.5971\n",
      "[INFO   ] 2025-05-14@16:57:49: Epoch [3/10], Loss: 0.6700\n",
      "[INFO   ] 2025-05-14@16:57:49: Epoch [4/10], Loss: 0.5955\n",
      "[INFO   ] 2025-05-14@16:57:50: Epoch [8/10], Loss: 0.6543\n",
      "[INFO   ] 2025-05-14@16:57:50: Epoch [6/10], Loss: 0.6070\n",
      "[INFO   ] 2025-05-14@16:57:50: Epoch [5/10], Loss: 0.6363\n",
      "[INFO   ] 2025-05-14@16:57:51: Epoch [6/10], Loss: 0.6346\n",
      "[INFO   ] 2025-05-14@16:57:51: Epoch [9/10], Loss: 0.5572\n",
      "[INFO   ] 2025-05-14@16:57:52: Epoch [3/10], Loss: 0.7187\n",
      "[INFO   ] 2025-05-14@16:57:52: Epoch [7/10], Loss: 0.6004\n",
      "[INFO   ] 2025-05-14@16:57:53: Epoch [10/10], Loss: 0.6664\n",
      "[INFO   ] 2025-05-14@16:57:53: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-14@16:57:56: Epoch [8/10], Loss: 0.6217\n",
      "[INFO   ] 2025-05-14@16:57:56: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[I 2025-05-14 13:57:56,662] Trial 11 finished with value: 0.3227785358546531 and parameters: {'window_size': 35, 'hidden_size': 102, 'num_layers': 1, 'batch_size': 50, 'lr': 0.004071515987641827}. Best is trial 1 with value: 0.44956314305127165.\n",
      "[INFO   ] 2025-05-14@16:57:56: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-14@16:57:59: Initializing model...\n",
      "[INFO   ] 2025-05-14@16:57:59: Training model...\n",
      "[INFO   ] 2025-05-14@16:58:00: Epoch [9/10], Loss: 0.5898\n",
      "[INFO   ] 2025-05-14@16:58:00: Epoch [3/10], Loss: 0.6041\n",
      "[INFO   ] 2025-05-14@16:58:01: Epoch [10/10], Loss: 0.6044\n",
      "[INFO   ] 2025-05-14@16:58:01: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-14@16:58:01: Epoch [7/10], Loss: 0.7960\n",
      "[INFO   ] 2025-05-14@16:58:02: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[I 2025-05-14 13:58:02,300] Trial 13 finished with value: 0.23792896230047056 and parameters: {'window_size': 10, 'hidden_size': 56, 'num_layers': 2, 'batch_size': 60, 'lr': 0.0037622122153733208}. Best is trial 1 with value: 0.44956314305127165.\n",
      "[INFO   ] 2025-05-14@16:58:02: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-14@16:58:06: Initializing model...\n",
      "[INFO   ] 2025-05-14@16:58:06: Training model...\n",
      "[INFO   ] 2025-05-14@16:58:06: Epoch [1/10], Loss: 0.6941\n",
      "[INFO   ] 2025-05-14@16:58:06: Epoch [4/10], Loss: 0.6544\n",
      "[INFO   ] 2025-05-14@16:58:07: Epoch [5/10], Loss: 0.7271\n",
      "[INFO   ] 2025-05-14@16:58:07: Epoch [1/10], Loss: 0.6658\n",
      "[INFO   ] 2025-05-14@16:58:08: Epoch [2/10], Loss: 0.6607\n",
      "[INFO   ] 2025-05-14@16:58:09: Epoch [3/10], Loss: 0.7029\n",
      "[INFO   ] 2025-05-14@16:58:10: Epoch [4/10], Loss: 0.6813\n",
      "[INFO   ] 2025-05-14@16:58:10: Epoch [8/10], Loss: 0.6906\n",
      "[INFO   ] 2025-05-14@16:58:11: Epoch [5/10], Loss: 0.6427\n",
      "[INFO   ] 2025-05-14@16:58:12: Epoch [6/10], Loss: 0.6538\n",
      "[INFO   ] 2025-05-14@16:58:12: Epoch [4/10], Loss: 0.6849\n",
      "[INFO   ] 2025-05-14@16:58:13: Epoch [7/10], Loss: 0.6240\n",
      "[INFO   ] 2025-05-14@16:58:13: Epoch [5/10], Loss: 0.7336\n",
      "[INFO   ] 2025-05-14@16:58:14: Epoch [8/10], Loss: 0.6447\n",
      "[INFO   ] 2025-05-14@16:58:15: Epoch [9/10], Loss: 0.6483\n",
      "[INFO   ] 2025-05-14@16:58:15: Epoch [9/10], Loss: 0.6550\n",
      "[INFO   ] 2025-05-14@16:58:15: Epoch [1/10], Loss: 0.6062\n",
      "[INFO   ] 2025-05-14@16:58:16: Epoch [10/10], Loss: 0.6094\n",
      "[INFO   ] 2025-05-14@16:58:16: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-14@16:58:21: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[I 2025-05-14 13:58:21,382] Trial 15 finished with value: 0.39233440479948867 and parameters: {'window_size': 60, 'hidden_size': 8, 'num_layers': 1, 'batch_size': 58, 'lr': 0.005499559638895356}. Best is trial 1 with value: 0.44956314305127165.\n",
      "[INFO   ] 2025-05-14@16:58:21: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-14@16:58:23: Initializing model...\n",
      "[INFO   ] 2025-05-14@16:58:23: Training model...\n",
      "[INFO   ] 2025-05-14@16:58:24: Epoch [2/10], Loss: 0.6384\n",
      "[INFO   ] 2025-05-14@16:58:24: Epoch [6/10], Loss: 0.6815\n",
      "[INFO   ] 2025-05-14@16:58:25: Epoch [4/10], Loss: 0.6998\n",
      "[INFO   ] 2025-05-14@16:58:26: Epoch [10/10], Loss: 0.5484\n",
      "[INFO   ] 2025-05-14@16:58:26: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-14@16:58:28: Epoch [6/10], Loss: 0.6365\n",
      "[INFO   ] 2025-05-14@16:58:28: Epoch [5/10], Loss: 0.7330\n",
      "[INFO   ] 2025-05-14@16:58:29: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[I 2025-05-14 13:58:29,855] Trial 8 finished with value: 0.27189876156356596 and parameters: {'window_size': 20, 'hidden_size': 55, 'num_layers': 2, 'batch_size': 14, 'lr': 0.001704371450989466}. Best is trial 1 with value: 0.44956314305127165.\n",
      "[INFO   ] 2025-05-14@16:58:29: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-14@16:58:30: Initializing model...\n",
      "[INFO   ] 2025-05-14@16:58:30: Training model...\n",
      "[INFO   ] 2025-05-14@16:58:34: Epoch [1/10], Loss: 0.7059\n",
      "[INFO   ] 2025-05-14@16:58:35: Epoch [7/10], Loss: 0.7509\n",
      "[INFO   ] 2025-05-14@16:58:35: Epoch [2/10], Loss: 0.8382\n",
      "[INFO   ] 2025-05-14@16:58:36: Epoch [6/10], Loss: 0.5971\n",
      "[INFO   ] 2025-05-14@16:58:37: Epoch [7/10], Loss: 0.6866\n",
      "[INFO   ] 2025-05-14@16:58:37: Epoch [3/10], Loss: 0.7738\n",
      "[INFO   ] 2025-05-14@16:58:41: Epoch [8/10], Loss: 0.6244\n",
      "[INFO   ] 2025-05-14@16:58:43: Epoch [5/10], Loss: 0.6406\n",
      "[INFO   ] 2025-05-14@16:58:43: Epoch [2/10], Loss: 0.6678\n",
      "[INFO   ] 2025-05-14@16:58:44: Epoch [7/10], Loss: 0.6674\n",
      "[INFO   ] 2025-05-14@16:58:44: Epoch [1/10], Loss: 0.7426\n",
      "[INFO   ] 2025-05-14@16:58:46: Epoch [3/10], Loss: 0.7722\n",
      "[INFO   ] 2025-05-14@16:58:47: Epoch [8/10], Loss: 0.7244\n",
      "[INFO   ] 2025-05-14@16:58:47: Epoch [9/10], Loss: 0.7440\n",
      "[INFO   ] 2025-05-14@16:58:49: Epoch [4/10], Loss: 0.7607\n",
      "[INFO   ] 2025-05-14@16:58:52: Epoch [3/10], Loss: 0.6462\n",
      "[INFO   ] 2025-05-14@16:58:52: Epoch [8/10], Loss: 0.5446\n",
      "[INFO   ] 2025-05-14@16:58:54: Epoch [10/10], Loss: 0.6532\n",
      "[INFO   ] 2025-05-14@16:58:54: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-14@16:58:58: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[I 2025-05-14 13:58:58,917] Trial 9 finished with value: 0.3612664801037707 and parameters: {'window_size': 27, 'hidden_size': 40, 'num_layers': 4, 'batch_size': 23, 'lr': 0.000896216869449941}. Best is trial 1 with value: 0.44956314305127165.\n",
      "[INFO   ] 2025-05-14@16:58:58: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-14@16:59:02: Initializing model...\n",
      "[INFO   ] 2025-05-14@16:59:02: Training model...\n",
      "[INFO   ] 2025-05-14@16:59:03: Epoch [4/10], Loss: 0.4071\n",
      "[INFO   ] 2025-05-14@16:59:04: Epoch [9/10], Loss: 0.7595\n",
      "[INFO   ] 2025-05-14@16:59:05: Epoch [6/10], Loss: 0.7128\n",
      "[INFO   ] 2025-05-14@16:59:05: Epoch [2/10], Loss: 0.6410\n",
      "[INFO   ] 2025-05-14@16:59:06: Epoch [5/10], Loss: 0.5789\n",
      "[INFO   ] 2025-05-14@16:59:07: Epoch [9/10], Loss: 0.7494\n",
      "[INFO   ] 2025-05-14@16:59:08: Epoch [4/10], Loss: 0.6612\n",
      "[INFO   ] 2025-05-14@16:59:15: Epoch [10/10], Loss: 0.6889\n",
      "[INFO   ] 2025-05-14@16:59:15: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-14@16:59:16: Epoch [10/10], Loss: 0.6970\n",
      "[INFO   ] 2025-05-14@16:59:16: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-14@16:59:19: Epoch [5/10], Loss: 0.7323\n",
      "[INFO   ] 2025-05-14@16:59:20: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[I 2025-05-14 13:59:20,471] Trial 10 finished with value: 0.4454155971614071 and parameters: {'window_size': 36, 'hidden_size': 124, 'num_layers': 1, 'batch_size': 14, 'lr': 0.009438729447717073}. Best is trial 1 with value: 0.44956314305127165.\n",
      "[INFO   ] 2025-05-14@16:59:20: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-14@16:59:21: Initializing model...\n",
      "[INFO   ] 2025-05-14@16:59:21: Training model...\n",
      "[INFO   ] 2025-05-14@16:59:21: Epoch [5/10], Loss: 0.6660\n",
      "[INFO   ] 2025-05-14@16:59:22: Epoch [6/10], Loss: 0.8512\n",
      "[INFO   ] 2025-05-14@16:59:23: Epoch [1/10], Loss: 0.6585\n",
      "[INFO   ] 2025-05-14@16:59:23: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[I 2025-05-14 13:59:24,047] Trial 7 finished with value: 0.3480570300402702 and parameters: {'window_size': 16, 'hidden_size': 16, 'num_layers': 3, 'batch_size': 5, 'lr': 0.007712814113535204}. Best is trial 1 with value: 0.44956314305127165.\n",
      "[INFO   ] 2025-05-14@16:59:24: Epoch [3/10], Loss: 0.7515\n",
      "[INFO   ] 2025-05-14@16:59:25: Epoch [7/10], Loss: 0.6889\n",
      "[INFO   ] 2025-05-14@16:59:25: Epoch [2/10], Loss: 0.7040\n",
      "[INFO   ] 2025-05-14@16:59:27: Epoch [3/10], Loss: 0.7424\n",
      "[INFO   ] 2025-05-14@16:59:29: Epoch [6/10], Loss: 0.6423\n",
      "[INFO   ] 2025-05-14@16:59:29: Epoch [4/10], Loss: 0.6651\n",
      "[INFO   ] 2025-05-14@16:59:30: Epoch [6/10], Loss: 0.5661\n",
      "[INFO   ] 2025-05-14@16:59:32: Epoch [5/10], Loss: 0.6700\n",
      "[INFO   ] 2025-05-14@16:59:32: Epoch [7/10], Loss: 0.6537\n",
      "[INFO   ] 2025-05-14@16:59:33: Epoch [1/10], Loss: 0.7931\n",
      "[INFO   ] 2025-05-14@16:59:34: Epoch [6/10], Loss: 0.6522\n",
      "[INFO   ] 2025-05-14@16:59:37: Epoch [7/10], Loss: 0.6369\n",
      "[INFO   ] 2025-05-14@16:59:37: Epoch [7/10], Loss: 0.7688\n",
      "[INFO   ] 2025-05-14@16:59:38: Epoch [4/10], Loss: 0.7839\n",
      "[INFO   ] 2025-05-14@16:59:38: Epoch [8/10], Loss: 0.7195\n",
      "[INFO   ] 2025-05-14@16:59:40: Epoch [8/10], Loss: 0.6209\n",
      "[INFO   ] 2025-05-14@16:59:41: Epoch [7/10], Loss: 0.6765\n",
      "[INFO   ] 2025-05-14@16:59:42: Epoch [8/10], Loss: 0.5509\n",
      "[INFO   ] 2025-05-14@16:59:42: Epoch [9/10], Loss: 0.6325\n",
      "[INFO   ] 2025-05-14@16:59:45: Epoch [8/10], Loss: 0.6987\n",
      "[INFO   ] 2025-05-14@16:59:45: Epoch [10/10], Loss: 0.6474\n",
      "[INFO   ] 2025-05-14@16:59:45: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-14@16:59:46: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[I 2025-05-14 13:59:46,685] Trial 19 finished with value: 0.35387839047056935 and parameters: {'window_size': 8, 'hidden_size': 128, 'num_layers': 3, 'batch_size': 36, 'lr': 0.009185462754431521}. Best is trial 1 with value: 0.44956314305127165.\n",
      "[INFO   ] 2025-05-14@16:59:51: Epoch [5/10], Loss: 0.6199\n",
      "[INFO   ] 2025-05-14@16:59:51: Epoch [9/10], Loss: 0.6496\n",
      "[INFO   ] 2025-05-14@16:59:51: Epoch [9/10], Loss: 0.6540\n",
      "[INFO   ] 2025-05-14@16:59:51: Epoch [9/10], Loss: 0.7268\n",
      "[INFO   ] 2025-05-14@16:59:51: Epoch [8/10], Loss: 0.6786\n",
      "[INFO   ] 2025-05-14@16:59:54: Epoch [2/10], Loss: 0.5684\n",
      "[INFO   ] 2025-05-14@16:59:57: Epoch [10/10], Loss: 0.7121\n",
      "[INFO   ] 2025-05-14@16:59:57: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-14@17:00:01: Epoch [10/10], Loss: 0.6707\n",
      "[INFO   ] 2025-05-14@17:00:01: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-14@17:00:03: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[I 2025-05-14 14:00:03,283] Trial 16 finished with value: 0.4556131643596728 and parameters: {'window_size': 35, 'hidden_size': 21, 'num_layers': 3, 'batch_size': 10, 'lr': 0.007227799189983123}. Best is trial 16 with value: 0.4556131643596728.\n",
      "[INFO   ] 2025-05-14@17:00:03: Epoch [9/10], Loss: 0.8050\n",
      "[INFO   ] 2025-05-14@17:00:04: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[I 2025-05-14 14:00:04,648] Trial 12 finished with value: 0.4867553865808055 and parameters: {'window_size': 16, 'hidden_size': 68, 'num_layers': 2, 'batch_size': 5, 'lr': 0.004426620716894683}. Best is trial 12 with value: 0.4867553865808055.\n",
      "[INFO   ] 2025-05-14@17:00:04: Epoch [10/10], Loss: 0.7432\n",
      "[INFO   ] 2025-05-14@17:00:04: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-14@17:00:08: Epoch [6/10], Loss: 0.7730\n",
      "[INFO   ] 2025-05-14@17:00:09: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[I 2025-05-14 14:00:10,137] Trial 2 finished with value: 0.3411379621749735 and parameters: {'window_size': 45, 'hidden_size': 90, 'num_layers': 3, 'batch_size': 11, 'lr': 0.003845669344374106}. Best is trial 12 with value: 0.4867553865808055.\n",
      "[INFO   ] 2025-05-14@17:00:12: Epoch [3/10], Loss: 0.7110\n",
      "[INFO   ] 2025-05-14@17:00:13: Epoch [10/10], Loss: 0.7975\n",
      "[INFO   ] 2025-05-14@17:00:13: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-14@17:00:17: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[I 2025-05-14 14:00:17,777] Trial 14 finished with value: 0.4377446809883959 and parameters: {'window_size': 47, 'hidden_size': 111, 'num_layers': 1, 'batch_size': 6, 'lr': 0.003522592337518306}. Best is trial 12 with value: 0.4867553865808055.\n",
      "[INFO   ] 2025-05-14@17:00:18: Epoch [7/10], Loss: 0.3133\n",
      "[INFO   ] 2025-05-14@17:00:20: Epoch [4/10], Loss: 0.6933\n",
      "[INFO   ] 2025-05-14@17:00:24: Epoch [5/10], Loss: 0.6208\n",
      "[INFO   ] 2025-05-14@17:00:24: Epoch [8/10], Loss: 0.5202\n",
      "[INFO   ] 2025-05-14@17:00:28: Epoch [6/10], Loss: 0.7901\n",
      "[INFO   ] 2025-05-14@17:00:29: Epoch [9/10], Loss: 1.3133\n",
      "[INFO   ] 2025-05-14@17:00:32: Epoch [7/10], Loss: 0.6120\n",
      "[INFO   ] 2025-05-14@17:00:35: Epoch [10/10], Loss: 0.7820\n",
      "[INFO   ] 2025-05-14@17:00:35: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-14@17:00:36: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[I 2025-05-14 14:00:36,745] Trial 17 finished with value: 0.4679651718757864 and parameters: {'window_size': 5, 'hidden_size': 123, 'num_layers': 4, 'batch_size': 5, 'lr': 0.008862536783171861}. Best is trial 12 with value: 0.4867553865808055.\n",
      "[INFO   ] 2025-05-14@17:00:37: Epoch [8/10], Loss: 0.6695\n",
      "[INFO   ] 2025-05-14@17:00:39: Epoch [9/10], Loss: 0.7239\n",
      "[INFO   ] 2025-05-14@17:00:42: Epoch [10/10], Loss: 0.6947\n",
      "[INFO   ] 2025-05-14@17:00:42: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-14@17:00:47: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[I 2025-05-14 14:00:47,626] Trial 18 finished with value: 0.3589217164636157 and parameters: {'window_size': 57, 'hidden_size': 8, 'num_layers': 3, 'batch_size': 4, 'lr': 0.00919799130505454}. Best is trial 12 with value: 0.4867553865808055.\n",
      "[INFO   ] 2025-05-14@17:00:47: Лучшие параметры: {'window_size': 16, 'hidden_size': 68, 'num_layers': 2, 'batch_size': 5, 'lr': 0.004426620716894683}\n",
      "[INFO   ] 2025-05-14@17:00:47: Лучший скорректированный ROC AUC Score: 0.4867553865808055\n",
      "[INFO   ] 2025-05-14@17:00:47: Fitting model with best parameters...\n",
      "[INFO   ] 2025-05-14@17:00:47: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-14@17:00:48: Initializing model...\n",
      "[INFO   ] 2025-05-14@17:00:48: Training model...\n",
      "[INFO   ] 2025-05-14@17:00:50: Epoch [1/10], Loss: 0.7468\n",
      "[INFO   ] 2025-05-14@17:00:52: Epoch [2/10], Loss: 0.7343\n",
      "[INFO   ] 2025-05-14@17:00:54: Epoch [3/10], Loss: 0.7022\n",
      "[INFO   ] 2025-05-14@17:00:56: Epoch [4/10], Loss: 0.7996\n",
      "[INFO   ] 2025-05-14@17:00:59: Epoch [5/10], Loss: 0.5517\n",
      "[INFO   ] 2025-05-14@17:01:01: Epoch [6/10], Loss: 0.4397\n",
      "[INFO   ] 2025-05-14@17:01:04: Epoch [7/10], Loss: 0.6785\n",
      "[INFO   ] 2025-05-14@17:01:06: Epoch [8/10], Loss: 0.4888\n",
      "[INFO   ] 2025-05-14@17:01:09: Epoch [9/10], Loss: 0.4563\n",
      "[INFO   ] 2025-05-14@17:01:11: Epoch [10/10], Loss: 0.5686\n",
      "[INFO   ] 2025-05-14@17:01:11: Generating predictions...\n",
      "[INFO   ] 2025-05-14@17:01:11: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-14@17:01:13: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-14@17:01:13: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-14@17:01:13: Evaluating ROC AUC...\n",
      "[INFO   ] 2025-05-14@17:01:13: Evaluating metrics by threshold...\n",
      "c:\\ProgramData\\miniconda3\\envs\\hw_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\ProgramData\\miniconda3\\envs\\hw_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "[INFO   ] 2025-05-14@17:01:13: Running backtesting and collecting all metrics..\n",
      "[INFO   ] 2025-05-14@17:01:13: Converting numpy arrays to TensorDataset and DataLoader...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Метрики для TRAIN выборки ===\n",
      "ROC AUC: 0.6883\n",
      "   Cutoff  Precision     Recall   Accuracy   F1-Score\n",
      "0    50.0  59.706960  91.061453  62.320574  72.123894\n",
      "1    60.0  61.454261  87.821229  63.995215  72.309108\n",
      "2    70.0  63.252471  78.659218  64.114833  70.119522\n",
      "3    80.0  72.832370  42.234637  60.645933  53.465347\n",
      "\n",
      "=== Метрики для TEST выборки ===\n",
      "ROC AUC: 0.4995\n",
      "   Cutoff  Precision      Recall   Accuracy   F1-Score\n",
      "0    50.0  52.173913  100.000000  52.173913  68.571429\n",
      "1    60.0  50.666667   79.166667  48.913043  61.788618\n",
      "2    70.0  50.666667   79.166667  48.913043  61.788618\n",
      "3    80.0   0.000000    0.000000  47.826087   0.000000\n",
      "\n",
      "=== Метрики для VAL выборки ===\n",
      "ROC AUC: 0.3845\n",
      "   Cutoff  Precision      Recall   Accuracy   F1-Score\n",
      "0    50.0  47.777778  100.000000  47.777778  64.661654\n",
      "1    60.0  42.857143   69.767442  41.111111  53.097345\n",
      "2    70.0  42.857143   48.837209  44.444444  45.652174\n",
      "3    80.0   0.000000    0.000000  52.222222   0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO   ] 2025-05-14@17:01:15: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-14@17:01:16: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-14@17:01:18: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-14@17:01:20: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-14@17:01:20: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-14@17:01:20: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-14@17:01:20: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-14@17:01:20: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-14@17:01:20: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-14@17:01:21: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-14@17:01:21: Converting numpy arrays to TensorDataset and DataLoader...\n",
      "[INFO   ] 2025-05-14@17:01:21: Saving results to DB...\n",
      "[INFO   ] 2025-05-14@17:01:21: ML Model Strategy training loop complete!\n"
     ]
    }
   ],
   "source": [
    "result = ml_model_strategy_training_loop(\n",
    "    tickers=tickers,\n",
    "    interval=interval,\n",
    "    train_start=train_start,\n",
    "    train_end=train_end,\n",
    "    test_end=test_end,\n",
    "    valid_end=valid_end,\n",
    "    model_options=model_options,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Experiment_ID</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Interval</th>\n",
       "      <th>Type</th>\n",
       "      <th>START_DT</th>\n",
       "      <th>END_DT</th>\n",
       "      <th>Model</th>\n",
       "      <th>Model_params</th>\n",
       "      <th>Cutoff</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1_Score</th>\n",
       "      <th>ROC_AUC</th>\n",
       "      <th>Return_pct</th>\n",
       "      <th>Win_Rate_pct</th>\n",
       "      <th>Num_Trades</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c02c1016-30b2-11f0-beeb-240a64112db6</td>\n",
       "      <td>^GSPC</td>\n",
       "      <td>1d</td>\n",
       "      <td>TRAIN</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>2024-10-01</td>\n",
       "      <td>LSTMModel</td>\n",
       "      <td>{\"window_size\": 16, \"hidden_size\": 68, \"num_la...</td>\n",
       "      <td>50.0</td>\n",
       "      <td>59.706960</td>\n",
       "      <td>91.061453</td>\n",
       "      <td>62.320574</td>\n",
       "      <td>72.123894</td>\n",
       "      <td>0.688321</td>\n",
       "      <td>223.109658</td>\n",
       "      <td>63.333333</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c02c1016-30b2-11f0-beeb-240a64112db6</td>\n",
       "      <td>^GSPC</td>\n",
       "      <td>1d</td>\n",
       "      <td>TRAIN</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>2024-10-01</td>\n",
       "      <td>LSTMModel</td>\n",
       "      <td>{\"window_size\": 16, \"hidden_size\": 68, \"num_la...</td>\n",
       "      <td>60.0</td>\n",
       "      <td>61.454261</td>\n",
       "      <td>87.821229</td>\n",
       "      <td>63.995215</td>\n",
       "      <td>72.309108</td>\n",
       "      <td>0.688321</td>\n",
       "      <td>353.301084</td>\n",
       "      <td>62.091503</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c02c1016-30b2-11f0-beeb-240a64112db6</td>\n",
       "      <td>^GSPC</td>\n",
       "      <td>1d</td>\n",
       "      <td>TRAIN</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>2024-10-01</td>\n",
       "      <td>LSTMModel</td>\n",
       "      <td>{\"window_size\": 16, \"hidden_size\": 68, \"num_la...</td>\n",
       "      <td>70.0</td>\n",
       "      <td>63.252471</td>\n",
       "      <td>78.659218</td>\n",
       "      <td>64.114833</td>\n",
       "      <td>70.119522</td>\n",
       "      <td>0.688321</td>\n",
       "      <td>108.836299</td>\n",
       "      <td>54.883721</td>\n",
       "      <td>215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c02c1016-30b2-11f0-beeb-240a64112db6</td>\n",
       "      <td>^GSPC</td>\n",
       "      <td>1d</td>\n",
       "      <td>TRAIN</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>2024-10-01</td>\n",
       "      <td>LSTMModel</td>\n",
       "      <td>{\"window_size\": 16, \"hidden_size\": 68, \"num_la...</td>\n",
       "      <td>80.0</td>\n",
       "      <td>72.832370</td>\n",
       "      <td>42.234637</td>\n",
       "      <td>60.645933</td>\n",
       "      <td>53.465347</td>\n",
       "      <td>0.688321</td>\n",
       "      <td>32.144892</td>\n",
       "      <td>55.769231</td>\n",
       "      <td>208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c02c1016-30b2-11f0-beeb-240a64112db6</td>\n",
       "      <td>^GSPC</td>\n",
       "      <td>1d</td>\n",
       "      <td>TEST</td>\n",
       "      <td>2024-10-01</td>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>LSTMModel</td>\n",
       "      <td>{\"window_size\": 16, \"hidden_size\": 68, \"num_la...</td>\n",
       "      <td>50.0</td>\n",
       "      <td>52.173913</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>52.173913</td>\n",
       "      <td>68.571429</td>\n",
       "      <td>0.499527</td>\n",
       "      <td>2.924741</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>c02c1016-30b2-11f0-beeb-240a64112db6</td>\n",
       "      <td>^GSPC</td>\n",
       "      <td>1d</td>\n",
       "      <td>TEST</td>\n",
       "      <td>2024-10-01</td>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>LSTMModel</td>\n",
       "      <td>{\"window_size\": 16, \"hidden_size\": 68, \"num_la...</td>\n",
       "      <td>60.0</td>\n",
       "      <td>50.666667</td>\n",
       "      <td>79.166667</td>\n",
       "      <td>48.913043</td>\n",
       "      <td>61.788618</td>\n",
       "      <td>0.499527</td>\n",
       "      <td>-2.9486</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>c02c1016-30b2-11f0-beeb-240a64112db6</td>\n",
       "      <td>^GSPC</td>\n",
       "      <td>1d</td>\n",
       "      <td>TEST</td>\n",
       "      <td>2024-10-01</td>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>LSTMModel</td>\n",
       "      <td>{\"window_size\": 16, \"hidden_size\": 68, \"num_la...</td>\n",
       "      <td>70.0</td>\n",
       "      <td>50.666667</td>\n",
       "      <td>79.166667</td>\n",
       "      <td>48.913043</td>\n",
       "      <td>61.788618</td>\n",
       "      <td>0.499527</td>\n",
       "      <td>-2.9486</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>c02c1016-30b2-11f0-beeb-240a64112db6</td>\n",
       "      <td>^GSPC</td>\n",
       "      <td>1d</td>\n",
       "      <td>TEST</td>\n",
       "      <td>2024-10-01</td>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>LSTMModel</td>\n",
       "      <td>{\"window_size\": 16, \"hidden_size\": 68, \"num_la...</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>47.826087</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.499527</td>\n",
       "      <td>-3.312217</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>c02c1016-30b2-11f0-beeb-240a64112db6</td>\n",
       "      <td>^GSPC</td>\n",
       "      <td>1d</td>\n",
       "      <td>VALID</td>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>LSTMModel</td>\n",
       "      <td>{\"window_size\": 16, \"hidden_size\": 68, \"num_la...</td>\n",
       "      <td>50.0</td>\n",
       "      <td>47.777778</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>47.777778</td>\n",
       "      <td>64.661654</td>\n",
       "      <td>0.384463</td>\n",
       "      <td>-4.65603</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>c02c1016-30b2-11f0-beeb-240a64112db6</td>\n",
       "      <td>^GSPC</td>\n",
       "      <td>1d</td>\n",
       "      <td>VALID</td>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>LSTMModel</td>\n",
       "      <td>{\"window_size\": 16, \"hidden_size\": 68, \"num_la...</td>\n",
       "      <td>60.0</td>\n",
       "      <td>42.857143</td>\n",
       "      <td>69.767442</td>\n",
       "      <td>41.111111</td>\n",
       "      <td>53.097345</td>\n",
       "      <td>0.384463</td>\n",
       "      <td>-11.656164</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>c02c1016-30b2-11f0-beeb-240a64112db6</td>\n",
       "      <td>^GSPC</td>\n",
       "      <td>1d</td>\n",
       "      <td>VALID</td>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>LSTMModel</td>\n",
       "      <td>{\"window_size\": 16, \"hidden_size\": 68, \"num_la...</td>\n",
       "      <td>70.0</td>\n",
       "      <td>42.857143</td>\n",
       "      <td>48.837209</td>\n",
       "      <td>44.444444</td>\n",
       "      <td>45.652174</td>\n",
       "      <td>0.384463</td>\n",
       "      <td>-6.34354</td>\n",
       "      <td>25.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>c02c1016-30b2-11f0-beeb-240a64112db6</td>\n",
       "      <td>^GSPC</td>\n",
       "      <td>1d</td>\n",
       "      <td>VALID</td>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>LSTMModel</td>\n",
       "      <td>{\"window_size\": 16, \"hidden_size\": 68, \"num_la...</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>52.222222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.384463</td>\n",
       "      <td>4.279001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Experiment_ID Ticker Interval   Type    START_DT  \\\n",
       "0   c02c1016-30b2-11f0-beeb-240a64112db6  ^GSPC       1d  TRAIN  2020-01-01   \n",
       "1   c02c1016-30b2-11f0-beeb-240a64112db6  ^GSPC       1d  TRAIN  2020-01-01   \n",
       "2   c02c1016-30b2-11f0-beeb-240a64112db6  ^GSPC       1d  TRAIN  2020-01-01   \n",
       "3   c02c1016-30b2-11f0-beeb-240a64112db6  ^GSPC       1d  TRAIN  2020-01-01   \n",
       "4   c02c1016-30b2-11f0-beeb-240a64112db6  ^GSPC       1d   TEST  2024-10-01   \n",
       "5   c02c1016-30b2-11f0-beeb-240a64112db6  ^GSPC       1d   TEST  2024-10-01   \n",
       "6   c02c1016-30b2-11f0-beeb-240a64112db6  ^GSPC       1d   TEST  2024-10-01   \n",
       "7   c02c1016-30b2-11f0-beeb-240a64112db6  ^GSPC       1d   TEST  2024-10-01   \n",
       "8   c02c1016-30b2-11f0-beeb-240a64112db6  ^GSPC       1d  VALID  2025-01-01   \n",
       "9   c02c1016-30b2-11f0-beeb-240a64112db6  ^GSPC       1d  VALID  2025-01-01   \n",
       "10  c02c1016-30b2-11f0-beeb-240a64112db6  ^GSPC       1d  VALID  2025-01-01   \n",
       "11  c02c1016-30b2-11f0-beeb-240a64112db6  ^GSPC       1d  VALID  2025-01-01   \n",
       "\n",
       "        END_DT      Model                                       Model_params  \\\n",
       "0   2024-10-01  LSTMModel  {\"window_size\": 16, \"hidden_size\": 68, \"num_la...   \n",
       "1   2024-10-01  LSTMModel  {\"window_size\": 16, \"hidden_size\": 68, \"num_la...   \n",
       "2   2024-10-01  LSTMModel  {\"window_size\": 16, \"hidden_size\": 68, \"num_la...   \n",
       "3   2024-10-01  LSTMModel  {\"window_size\": 16, \"hidden_size\": 68, \"num_la...   \n",
       "4   2025-01-01  LSTMModel  {\"window_size\": 16, \"hidden_size\": 68, \"num_la...   \n",
       "5   2025-01-01  LSTMModel  {\"window_size\": 16, \"hidden_size\": 68, \"num_la...   \n",
       "6   2025-01-01  LSTMModel  {\"window_size\": 16, \"hidden_size\": 68, \"num_la...   \n",
       "7   2025-01-01  LSTMModel  {\"window_size\": 16, \"hidden_size\": 68, \"num_la...   \n",
       "8   2025-04-01  LSTMModel  {\"window_size\": 16, \"hidden_size\": 68, \"num_la...   \n",
       "9   2025-04-01  LSTMModel  {\"window_size\": 16, \"hidden_size\": 68, \"num_la...   \n",
       "10  2025-04-01  LSTMModel  {\"window_size\": 16, \"hidden_size\": 68, \"num_la...   \n",
       "11  2025-04-01  LSTMModel  {\"window_size\": 16, \"hidden_size\": 68, \"num_la...   \n",
       "\n",
       "    Cutoff  Precision      Recall   Accuracy   F1_Score   ROC_AUC  Return_pct  \\\n",
       "0     50.0  59.706960   91.061453  62.320574  72.123894  0.688321  223.109658   \n",
       "1     60.0  61.454261   87.821229  63.995215  72.309108  0.688321  353.301084   \n",
       "2     70.0  63.252471   78.659218  64.114833  70.119522  0.688321  108.836299   \n",
       "3     80.0  72.832370   42.234637  60.645933  53.465347  0.688321   32.144892   \n",
       "4     50.0  52.173913  100.000000  52.173913  68.571429  0.499527    2.924741   \n",
       "5     60.0  50.666667   79.166667  48.913043  61.788618  0.499527     -2.9486   \n",
       "6     70.0  50.666667   79.166667  48.913043  61.788618  0.499527     -2.9486   \n",
       "7     80.0   0.000000    0.000000  47.826087   0.000000  0.499527   -3.312217   \n",
       "8     50.0  47.777778  100.000000  47.777778  64.661654  0.384463    -4.65603   \n",
       "9     60.0  42.857143   69.767442  41.111111  53.097345  0.384463  -11.656164   \n",
       "10    70.0  42.857143   48.837209  44.444444  45.652174  0.384463    -6.34354   \n",
       "11    80.0   0.000000    0.000000  52.222222   0.000000  0.384463    4.279001   \n",
       "\n",
       "   Win_Rate_pct Num_Trades  \n",
       "0     63.333333        120  \n",
       "1     62.091503        153  \n",
       "2     54.883721        215  \n",
       "3     55.769231        208  \n",
       "4           NaN          0  \n",
       "5           0.0          1  \n",
       "6           0.0          1  \n",
       "7           NaN          0  \n",
       "8           NaN          0  \n",
       "9           0.0          3  \n",
       "10         25.0          4  \n",
       "11          NaN          0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO:\n",
    "# - LSTM\n",
    "# - GRU\n",
    "# - Transformer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# см блокноты \"CNN LSTM GRU\" и практику из 24го урока\n",
    "# возможно добавить Transformer из 25го урока?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hw_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
